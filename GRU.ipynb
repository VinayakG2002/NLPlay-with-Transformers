{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GRU_model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNuY7RknHPzcJQywqknQFNS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VinayakG2002/README.md/blob/main/GRU_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfWbSzFsnsLy"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "import random"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ki2caeNin1VC"
      },
      "source": [
        "url='https://raw.githubusercontent.com/VinayakG2002/NLPlay-with-Transformers/main/IMDB%20Dataset.csv?token=ATKA2LIXPHWLGQT64HBC2KLA6XEPW'\n",
        "df = pd.read_csv(url)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 189
        },
        "id": "zfxF_KxTn1SG",
        "outputId": "7a8b7b80-7d14-4b49-bc1b-8e4c7e59aaf2"
      },
      "source": [
        "print(df.columns)\n",
        "df.describe()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Index(['review', 'sentiment'], dtype='object')\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>50000</td>\n",
              "      <td>50000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>unique</th>\n",
              "      <td>49582</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>top</th>\n",
              "      <td>Loved today's show!!! It was a variety and not...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>freq</th>\n",
              "      <td>5</td>\n",
              "      <td>25000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   review sentiment\n",
              "count                                               50000     50000\n",
              "unique                                              49582         2\n",
              "top     Loved today's show!!! It was a variety and not...  negative\n",
              "freq                                                    5     25000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PsslgRbNn1PA"
      },
      "source": [
        "def splitDF(r):\n",
        "  dataLen = len(df)\n",
        "  temp = ['test']*int((1-r)*dataLen) + ['train']*int((r)*dataLen)\n",
        "  random.shuffle(temp)\n",
        "  df['split'] = temp\n",
        "\n",
        "#splitDF(0.3)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OVVjAGf2n1MM",
        "outputId": "eec962d7-3d34-468c-e670-d95f3e8524dc"
      },
      "source": [
        "nltk.download('stopwords')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2uDldRWzn1JG"
      },
      "source": [
        "\n",
        "df['review'] = df['review'].apply(lambda x:x.lower())"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uiCyV4ctn1GD"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "stop = stopwords.words('english')\n",
        "df['review'] = df['review'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "id": "iYmca_Ppn1DC",
        "outputId": "b0fe2dc9-b97b-4681-d60b-21b8139cd14f"
      },
      "source": [
        "df['review'][0]"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"one reviewers mentioned watching 1 oz episode hooked. right, exactly happened me.<br /><br />the first thing struck oz brutality unflinching scenes violence, set right word go. trust me, show faint hearted timid. show pulls punches regards drugs, sex violence. hardcore, classic use word.<br /><br />it called oz nickname given oswald maximum security state penitentary. focuses mainly emerald city, experimental section prison cells glass fronts face inwards, privacy high agenda. em city home many..aryans, muslims, gangstas, latinos, christians, italians, irish more....so scuffles, death stares, dodgy dealings shady agreements never far away.<br /><br />i would say main appeal show due fact goes shows dare. forget pretty pictures painted mainstream audiences, forget charm, forget romance...oz mess around. first episode ever saw struck nasty surreal, say ready it, watched more, developed taste oz, got accustomed high levels graphic violence. violence, injustice (crooked guards who'll sold nickel, inmates who'll kill order get away it, well mannered, middle class inmates turned prison bitches due lack street skills prison experience) watching oz, may become comfortable uncomfortable viewing....thats get touch darker side.\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZmUPBNXBn0_u"
      },
      "source": [
        "def remove_punctuation(text):\n",
        "    final = \"\".join(u for u in text if u not in (\"?\", \".\", \";\", \":\", \"!\", '\"', ',','#','$','@','%','^','&','*'))\n",
        "    return final\n",
        "\n",
        "df['review'] = df['review'].apply(remove_punctuation)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_SdTVnon085"
      },
      "source": [
        "\n",
        "def remove_tags(text):\n",
        "    final = \"\"\n",
        "    stt = True\n",
        "    for char in text:\n",
        "        if char == '<':\n",
        "            stt = False\n",
        "        if(stt):\n",
        "            final = final + char\n",
        "        if char == '>':\n",
        "            stt = True\n",
        "            final = final + ' '\n",
        "    return final\n",
        "    \n",
        "df['review'] = df['review'].apply(remove_tags)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "id": "TKQYm28Mn058",
        "outputId": "4a126a77-33b3-4905-a14d-b56730e972a6"
      },
      "source": [
        "\n",
        "df['review'][0]"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"one reviewers mentioned watching 1 oz episode hooked right exactly happened me  the first thing struck oz brutality unflinching scenes violence set right word go trust me show faint hearted timid show pulls punches regards drugs sex violence hardcore classic use word  it called oz nickname given oswald maximum security state penitentary focuses mainly emerald city experimental section prison cells glass fronts face inwards privacy high agenda em city home manyaryans muslims gangstas latinos christians italians irish moreso scuffles death stares dodgy dealings shady agreements never far away  i would say main appeal show due fact goes shows dare forget pretty pictures painted mainstream audiences forget charm forget romanceoz mess around first episode ever saw struck nasty surreal say ready it watched more developed taste oz got accustomed high levels graphic violence violence injustice (crooked guards who'll sold nickel inmates who'll kill order get away it well mannered middle class inmates turned prison bitches due lack street skills prison experience) watching oz may become comfortable uncomfortable viewingthats get touch darker side\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ewn3Sughn02v",
        "outputId": "7ff48035-df7c-4e1d-cda3-89f70043bfec"
      },
      "source": [
        "nltk.download('punkt')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jeke1cwnn0zq"
      },
      "source": [
        "df['review'] = df['review'].apply(nltk.word_tokenize)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pleiFyOon0wq"
      },
      "source": [
        "\n",
        "def stem_tokens(tokens):\n",
        "    final = [nltk.stem.PorterStemmer().stem(word) for word in tokens]\n",
        "    return final\n",
        "    \n",
        "df['review'] = df['review'].apply(stem_tokens)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "optwOWs1n0t5"
      },
      "source": [
        "df['len_review'] = df['review'].apply(lambda x:len(x))"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bp_vSKTon0q4"
      },
      "source": [
        "df['sentiment'] = [1*(sent=='positive') for sent in df['sentiment']]"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "id": "aR-sUKyIn0oE",
        "outputId": "40ef8cb6-adab-47fc-c339-5660d845cb9a"
      },
      "source": [
        "df.head(10)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>len_review</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[one, review, mention, watch, 1, oz, episod, h...</td>\n",
              "      <td>1</td>\n",
              "      <td>175</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[wonder, littl, product, the, film, techniqu, ...</td>\n",
              "      <td>1</td>\n",
              "      <td>96</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[thought, wonder, way, spend, time, hot, summe...</td>\n",
              "      <td>1</td>\n",
              "      <td>95</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[basic, there, 's, famili, littl, boy, (, jake...</td>\n",
              "      <td>0</td>\n",
              "      <td>74</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[petter, mattei, 's, love, time, money, visual...</td>\n",
              "      <td>1</td>\n",
              "      <td>132</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>[probabl, all-tim, favorit, movi, stori, selfl...</td>\n",
              "      <td>1</td>\n",
              "      <td>64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>[sure, would, like, see, resurrect, date, seah...</td>\n",
              "      <td>1</td>\n",
              "      <td>79</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>[show, amaz, fresh, innov, idea, 70, 's, first...</td>\n",
              "      <td>0</td>\n",
              "      <td>89</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>[encourag, posit, comment, film, look, forward...</td>\n",
              "      <td>0</td>\n",
              "      <td>76</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>[like, origin, gut, wrench, laughter, like, mo...</td>\n",
              "      <td>1</td>\n",
              "      <td>18</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              review  sentiment  len_review\n",
              "0  [one, review, mention, watch, 1, oz, episod, h...          1         175\n",
              "1  [wonder, littl, product, the, film, techniqu, ...          1          96\n",
              "2  [thought, wonder, way, spend, time, hot, summe...          1          95\n",
              "3  [basic, there, 's, famili, littl, boy, (, jake...          0          74\n",
              "4  [petter, mattei, 's, love, time, money, visual...          1         132\n",
              "5  [probabl, all-tim, favorit, movi, stori, selfl...          1          64\n",
              "6  [sure, would, like, see, resurrect, date, seah...          1          79\n",
              "7  [show, amaz, fresh, innov, idea, 70, 's, first...          0          89\n",
              "8  [encourag, posit, comment, film, look, forward...          0          76\n",
              "9  [like, origin, gut, wrench, laughter, like, mo...          1          18"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UR6EQrMSn0lD"
      },
      "source": [
        "from gensim import corpora\n",
        "\n",
        "review_dict = corpora.Dictionary(df['review'])\n",
        "VOCAB_SIZE = len(review_dict)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NCVaCRh4n0iC",
        "outputId": "a94e15b1-de78-4a73-f25c-f381af38041e"
      },
      "source": [
        "\n",
        "print(len(review_dict))\n",
        "review_dict"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "132056\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<gensim.corpora.dictionary.Dictionary at 0x7feaf4f8e910>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvO-8zksn0fE"
      },
      "source": [
        "review_int = []\n",
        "for review in df['review']:\n",
        "  temp = [review_dict.token2id[x] for x in review]\n",
        "  review_int.append(temp)\n",
        "df['review_int'] = review_int"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "id": "eHraO2edn0cZ",
        "outputId": "fb7f3020-5ae3-4c9d-fb6e-a87c3722b82c"
      },
      "source": [
        "\n",
        "df.head(10)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>len_review</th>\n",
              "      <th>review_int</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[one, review, mention, watch, 1, oz, episod, h...</td>\n",
              "      <td>1</td>\n",
              "      <td>175</td>\n",
              "      <td>[90, 104, 80, 137, 3, 93, 33, 60, 105, 35, 55,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[wonder, littl, product, the, film, techniqu, ...</td>\n",
              "      <td>1</td>\n",
              "      <td>96</td>\n",
              "      <td>[206, 171, 183, 126, 162, 196, 202, 175, 161, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[thought, wonder, way, spend, time, hot, summe...</td>\n",
              "      <td>1</td>\n",
              "      <td>95</td>\n",
              "      <td>[265, 206, 269, 256, 266, 230, 260, 271, 255, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[basic, there, 's, famili, littl, boy, (, jake...</td>\n",
              "      <td>0</td>\n",
              "      <td>74</td>\n",
              "      <td>[281, 314, 144, 291, 171, 283, 1, 295, 2, 315,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[petter, mattei, 's, love, time, money, visual...</td>\n",
              "      <td>1</td>\n",
              "      <td>132</td>\n",
              "      <td>[366, 357, 144, 241, 266, 359, 392, 383, 162, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>[probabl, all-tim, favorit, movi, stori, selfl...</td>\n",
              "      <td>1</td>\n",
              "      <td>64</td>\n",
              "      <td>[429, 401, 417, 299, 437, 434, 432, 411, 425, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>[sure, would, like, see, resurrect, date, seah...</td>\n",
              "      <td>1</td>\n",
              "      <td>79</td>\n",
              "      <td>[478, 141, 296, 190, 471, 449, 475, 476, 479, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>[show, amaz, fresh, innov, idea, 70, 's, first...</td>\n",
              "      <td>0</td>\n",
              "      <td>89</td>\n",
              "      <td>[116, 497, 517, 525, 524, 493, 144, 42, 213, 4...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>[encourag, posit, comment, film, look, forward...</td>\n",
              "      <td>0</td>\n",
              "      <td>76</td>\n",
              "      <td>[550, 563, 546, 162, 354, 552, 137, 162, 500, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>[like, origin, gut, wrench, laughter, like, mo...</td>\n",
              "      <td>1</td>\n",
              "      <td>18</td>\n",
              "      <td>[296, 531, 572, 576, 574, 296, 299, 277, 426, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              review  ...                                         review_int\n",
              "0  [one, review, mention, watch, 1, oz, episod, h...  ...  [90, 104, 80, 137, 3, 93, 33, 60, 105, 35, 55,...\n",
              "1  [wonder, littl, product, the, film, techniqu, ...  ...  [206, 171, 183, 126, 162, 196, 202, 175, 161, ...\n",
              "2  [thought, wonder, way, spend, time, hot, summe...  ...  [265, 206, 269, 256, 266, 230, 260, 271, 255, ...\n",
              "3  [basic, there, 's, famili, littl, boy, (, jake...  ...  [281, 314, 144, 291, 171, 283, 1, 295, 2, 315,...\n",
              "4  [petter, mattei, 's, love, time, money, visual...  ...  [366, 357, 144, 241, 266, 359, 392, 383, 162, ...\n",
              "5  [probabl, all-tim, favorit, movi, stori, selfl...  ...  [429, 401, 417, 299, 437, 434, 432, 411, 425, ...\n",
              "6  [sure, would, like, see, resurrect, date, seah...  ...  [478, 141, 296, 190, 471, 449, 475, 476, 479, ...\n",
              "7  [show, amaz, fresh, innov, idea, 70, 's, first...  ...  [116, 497, 517, 525, 524, 493, 144, 42, 213, 4...\n",
              "8  [encourag, posit, comment, film, look, forward...  ...  [550, 563, 546, 162, 354, 552, 137, 162, 500, ...\n",
              "9  [like, origin, gut, wrench, laughter, like, mo...  ...  [296, 531, 572, 576, 574, 296, 299, 277, 426, ...\n",
              "\n",
              "[10 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416
        },
        "id": "JQbbUJuDn0Zq",
        "outputId": "caf860d3-4ec1-45a4-fb70-dc0027cc0aa1"
      },
      "source": [
        "%matplotlib inline\n",
        "pd.Series(df['len_review']).hist()\n",
        "plt.show()\n",
        "pd.Series(df['len_review']).describe()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    50000.000000\n",
              "mean       130.815000\n",
              "std         99.359444\n",
              "min          3.000000\n",
              "25%         69.000000\n",
              "50%         97.000000\n",
              "75%        160.000000\n",
              "max       1486.000000\n",
              "Name: len_review, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rh_rMxazn0W0"
      },
      "source": [
        "review_int = df['review_int']\n",
        "review_len = df['len_review']\n",
        "review_label = df['sentiment']\n",
        "review_int = [review_int[i] for i,x in enumerate(review_int) if review_len[i]>0]\n",
        "review_label = [review_label[i] for i,x in enumerate(review_label) if review_len[i]>0]\n",
        "review_len = [review_len[i] for i,x in enumerate(review_len) if review_len[i]>0 ]"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d1VJ4V6Un0UI",
        "outputId": "e1c6c7d1-f6b0-47f4-a604-71facf848a7a"
      },
      "source": [
        "len(review_int)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3U1yVAdRn0Rl"
      },
      "source": [
        "\n",
        "def pad_features(review_int, L):\n",
        "  features = np.zeros((len(review_int),L), dtype = int)\n",
        "  for i,review in enumerate(review_int):\n",
        "    l = len(review)\n",
        "    if l <= L:\n",
        "      patch = [0]*(L-l)\n",
        "      new = patch + review\n",
        "      features[i,:] = np.array(new)\n",
        "    else :\n",
        "      features[i,:] = np.array(review[:L])\n",
        "  return features"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pBSpjb8vn0PM"
      },
      "source": [
        "features = pad_features(review_int, 200)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bAw-wAppn0Mv",
        "outputId": "714e2fed-c755-45ca-976a-58400e912754"
      },
      "source": [
        "print(features[:10,:])"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[  0   0   0 ... 129  24 117]\n",
            " [  0   0   0 ... 197 138 154]\n",
            " [  0   0   0 ...  50 190 227]\n",
            " ...\n",
            " [  0   0   0 ... 404 258 213]\n",
            " [  0   0   0 ... 556 561 562]\n",
            " [  0   0   0 ...  66 165 571]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azsgZFU_n0Kb"
      },
      "source": [
        "split_frac = 0.8\n",
        "\n",
        "train_x = features[0:int(len(features)*split_frac)]\n",
        "train_y = np.array(review_label[0:int(len(features)*split_frac)])\n",
        "\n",
        "test_x = features[len(features)-int(len(features)*split_frac):len(features)-int(len(features)*split_frac/2)]\n",
        "test_y = np.array(review_label[len(features)-int(len(features)*split_frac):len(features)-int(len(features)*split_frac/2)])\n",
        "\n",
        "valid_x = features[len(features)-int(len(features)*split_frac/2):]\n",
        "valid_y = np.array(review_label[len(features)-int(len(features)*split_frac/2):])"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EDgC2B7hn0H8"
      },
      "source": [
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
        "test_data = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n",
        "valid_data = TensorDataset(torch.from_numpy(valid_x), torch.from_numpy(valid_y))\n",
        "\n",
        "batch_size = 50\n",
        "\n",
        "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
        "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)\n",
        "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZcfHnfgYn0Fu",
        "outputId": "c7d86f47-3f99-4429-e377-32f93a78963c"
      },
      "source": [
        "# obtain one batch of training data\n",
        "dataiter = iter(train_loader)\n",
        "sample_x, sample_y = dataiter.next()\n",
        "print('Sample input size: ', sample_x.size()) # batch_size, seq_length\n",
        "print('Sample input: \\n', sample_x)\n",
        "print()\n",
        "print('Sample label size: ', sample_y.size()) # batch_size\n",
        "print('Sample label: \\n', sample_y)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sample input size:  torch.Size([50, 200])\n",
            "Sample input: \n",
            " tensor([[    0,     0,     0,  ...,   788,   857,  4709],\n",
            "        [    0,     0,     0,  ..., 39925,    40,    57],\n",
            "        [    0,     0,     0,  ...,   263,  1252,  6143],\n",
            "        ...,\n",
            "        [    0,     0,     0,  ...,  1972,  2692,  4372],\n",
            "        [    0,     0,     0,  ...,   825,    66,  2695],\n",
            "        [    0,     0,     0,  ...,  5098,   144,  1830]])\n",
            "\n",
            "Sample label size:  torch.Size([50])\n",
            "Sample label: \n",
            " tensor([0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1,\n",
            "        0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1,\n",
            "        0, 1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IxDxSO5fn0DA",
        "outputId": "fd6725eb-8bff-428c-ce77-ecc44db54d87"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device available for running: \")\n",
        "print(device)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device available for running: \n",
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_VKqDVfn0Ac"
      },
      "source": [
        "\n",
        "class SentimentGRU(nn.Module):\n",
        "    \"\"\"\n",
        "    The GRU model that will be used to perform Sentiment analysis.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n",
        "        \"\"\"\n",
        "        Initialize the model by setting up the layers.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_dim = hidden_dim\n",
        "        \n",
        "        # embedding and GRU layers\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.gru = nn.GRU(embedding_dim, hidden_dim, n_layers, \n",
        "                            dropout=drop_prob, batch_first=True)\n",
        "        \n",
        "        # dropout layer\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        \n",
        "        # linear and sigmoid layers\n",
        "        self.fc = nn.Linear(hidden_dim, output_size)\n",
        "        self.sig = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        \"\"\"\n",
        "        Perform a forward pass of our model on some input and hidden state.\n",
        "        \"\"\"\n",
        "        batch_size = x.size(0)\n",
        "  \n",
        "        # embeddings and gru_out\n",
        "        embeds = self.embedding(x)\n",
        "        gru_out, hidden = self.gru(embeds, hidden)\n",
        "    \n",
        "        # stack up gru outputs\n",
        "        gru_out = gru_out.contiguous().view(-1, self.hidden_dim)\n",
        "        \n",
        "        # dropout and fully-connected layer\n",
        "        out = self.dropout(gru_out)\n",
        "        out = self.fc(out)\n",
        "        # sigmoid function\n",
        "        sig_out = self.sig(out)\n",
        "        \n",
        "        # reshape to be batch_size first\n",
        "        sig_out = sig_out.view(batch_size, -1)\n",
        "        sig_out = sig_out[:, -1] # get last batch of labels\n",
        "        \n",
        "        # return last sigmoid output and hidden state\n",
        "        return sig_out, hidden\n",
        "    \n",
        "    \n",
        "    def init_hidden(self, batch_size):\n",
        "        weight = next(self.parameters()).data\n",
        "        hidden = weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device)\n",
        "        return hidden        "
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wKoEhc0inz9v",
        "outputId": "30e2919b-4d3d-4c8d-b4c9-172823a83ca1"
      },
      "source": [
        "# Instantiate the model w/ hyperparams\n",
        "vocab_size = len(review_dict)+1 # +1 for the 0 padding\n",
        "output_size = 1\n",
        "embedding_dim = 400\n",
        "hidden_dim = 256\n",
        "n_layers = 2\n",
        "net = SentimentGRU(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
        "\n",
        "print(net)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SentimentGRU(\n",
            "  (embedding): Embedding(132057, 400)\n",
            "  (gru): GRU(400, 256, num_layers=2, batch_first=True, dropout=0.5)\n",
            "  (dropout): Dropout(p=0.3, inplace=False)\n",
            "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
            "  (sig): Sigmoid()\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OLSObcy_nz7I",
        "outputId": "7a4ea0f6-ddd0-4068-983f-e4f2621ee849"
      },
      "source": [
        "train_on_gpu=torch.cuda.is_available()\n",
        "\n",
        "if(train_on_gpu):\n",
        "    print('Training on GPU.')\n",
        "else:\n",
        "    print('No GPU available, training on CPU.')"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training on GPU.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3hgaDghDnz4d"
      },
      "source": [
        "lr=0.001\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=lr)"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5lFEPEb9nz1o",
        "outputId": "dac82304-336c-4f19-faff-0ee8bef67c9a"
      },
      "source": [
        "# training params\n",
        "\n",
        "epochs = 10\n",
        "\n",
        "counter = 0\n",
        "print_every = 100\n",
        "clip=5 # gradient clipping\n",
        "\n",
        "# move model to GPU, if available\n",
        "if(train_on_gpu):\n",
        "    net.cuda()\n",
        "\n",
        "net.train()\n",
        "# train for some number of epochs\n",
        "for e in range(epochs):\n",
        "    # initialize hidden state\n",
        "    h = net.init_hidden(batch_size)\n",
        "\n",
        "    # batch loop\n",
        "    for inputs, labels in train_loader:\n",
        "        counter += 1\n",
        "        #print(counter)\n",
        "       \n",
        "        if(train_on_gpu):\n",
        "            inputs, labels = inputs.cuda(), labels.cuda()\n",
        "\n",
        "        # Creating new variables for the hidden state, otherwise\n",
        "        # we'd backprop through the entire training history\n",
        "        h = h.data\n",
        "        # zero accumulated gradients\n",
        "        net.zero_grad()\n",
        "\n",
        "        # get the output from the model\n",
        "        output, h = net(inputs, h)\n",
        "\n",
        "        # calculate the loss and perform backprop\n",
        "        loss = criterion(output.squeeze(), labels.float())\n",
        "        loss.backward()\n",
        "        # `clip_grad_norm` helps prevent the exploding gradient problem in GRUs / LSTMs.\n",
        "        nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
        "        optimizer.step()\n",
        "\n",
        "        # loss stats\n",
        "        if counter % print_every == 0:\n",
        "            # Get validation loss\n",
        "            val_h = net.init_hidden(batch_size)\n",
        "            val_losses = []\n",
        "            net.eval()\n",
        "            for inputs, labels in valid_loader:\n",
        "\n",
        "                # Creating new variables for the hidden state, otherwise\n",
        "                # we'd backprop through the entire training history\n",
        "                val_h = h.data\n",
        "\n",
        "                if(train_on_gpu):\n",
        "                    inputs, labels = inputs.cuda(), labels.cuda()\n",
        "\n",
        "                output, val_h = net(inputs, val_h)\n",
        "                val_loss = criterion(output.squeeze(), labels.float())\n",
        "\n",
        "                val_losses.append(val_loss.item())\n",
        "\n",
        "            net.train()\n",
        "            print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
        "                  \"Step: {}...\".format(counter),\n",
        "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
        "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1/10... Step: 100... Loss: 0.582795... Val Loss: 0.601676\n",
            "Epoch: 1/10... Step: 200... Loss: 0.454726... Val Loss: 0.503012\n",
            "Epoch: 1/10... Step: 300... Loss: 0.598925... Val Loss: 0.374123\n",
            "Epoch: 1/10... Step: 400... Loss: 0.381327... Val Loss: 0.330491\n",
            "Epoch: 1/10... Step: 500... Loss: 0.419899... Val Loss: 0.317296\n",
            "Epoch: 1/10... Step: 600... Loss: 0.398828... Val Loss: 0.293837\n",
            "Epoch: 1/10... Step: 700... Loss: 0.348291... Val Loss: 0.270757\n",
            "Epoch: 1/10... Step: 800... Loss: 0.239603... Val Loss: 0.283495\n",
            "Epoch: 2/10... Step: 900... Loss: 0.192921... Val Loss: 0.250172\n",
            "Epoch: 2/10... Step: 1000... Loss: 0.166741... Val Loss: 0.248965\n",
            "Epoch: 2/10... Step: 1100... Loss: 0.219345... Val Loss: 0.230233\n",
            "Epoch: 2/10... Step: 1200... Loss: 0.200919... Val Loss: 0.231631\n",
            "Epoch: 2/10... Step: 1300... Loss: 0.305226... Val Loss: 0.218576\n",
            "Epoch: 2/10... Step: 1400... Loss: 0.336621... Val Loss: 0.208411\n",
            "Epoch: 2/10... Step: 1500... Loss: 0.265770... Val Loss: 0.240016\n",
            "Epoch: 2/10... Step: 1600... Loss: 0.262311... Val Loss: 0.205796\n",
            "Epoch: 3/10... Step: 1700... Loss: 0.075124... Val Loss: 0.203521\n",
            "Epoch: 3/10... Step: 1800... Loss: 0.068190... Val Loss: 0.220744\n",
            "Epoch: 3/10... Step: 1900... Loss: 0.045363... Val Loss: 0.213966\n",
            "Epoch: 3/10... Step: 2000... Loss: 0.035920... Val Loss: 0.220067\n",
            "Epoch: 3/10... Step: 2100... Loss: 0.045800... Val Loss: 0.226156\n",
            "Epoch: 3/10... Step: 2200... Loss: 0.188079... Val Loss: 0.207094\n",
            "Epoch: 3/10... Step: 2300... Loss: 0.157744... Val Loss: 0.209810\n",
            "Epoch: 3/10... Step: 2400... Loss: 0.117756... Val Loss: 0.214528\n",
            "Epoch: 4/10... Step: 2500... Loss: 0.107070... Val Loss: 0.248628\n",
            "Epoch: 4/10... Step: 2600... Loss: 0.104624... Val Loss: 0.217705\n",
            "Epoch: 4/10... Step: 2700... Loss: 0.024822... Val Loss: 0.245069\n",
            "Epoch: 4/10... Step: 2800... Loss: 0.006557... Val Loss: 0.220104\n",
            "Epoch: 4/10... Step: 2900... Loss: 0.139125... Val Loss: 0.223345\n",
            "Epoch: 4/10... Step: 3000... Loss: 0.089392... Val Loss: 0.224986\n",
            "Epoch: 4/10... Step: 3100... Loss: 0.110574... Val Loss: 0.212560\n",
            "Epoch: 4/10... Step: 3200... Loss: 0.032900... Val Loss: 0.204223\n",
            "Epoch: 5/10... Step: 3300... Loss: 0.006287... Val Loss: 0.239742\n",
            "Epoch: 5/10... Step: 3400... Loss: 0.005483... Val Loss: 0.256393\n",
            "Epoch: 5/10... Step: 3500... Loss: 0.020619... Val Loss: 0.270005\n",
            "Epoch: 5/10... Step: 3600... Loss: 0.004030... Val Loss: 0.273100\n",
            "Epoch: 5/10... Step: 3700... Loss: 0.024443... Val Loss: 0.273381\n",
            "Epoch: 5/10... Step: 3800... Loss: 0.009667... Val Loss: 0.265070\n",
            "Epoch: 5/10... Step: 3900... Loss: 0.037498... Val Loss: 0.252045\n",
            "Epoch: 5/10... Step: 4000... Loss: 0.013429... Val Loss: 0.244935\n",
            "Epoch: 6/10... Step: 4100... Loss: 0.003968... Val Loss: 0.300566\n",
            "Epoch: 6/10... Step: 4200... Loss: 0.008989... Val Loss: 0.288229\n",
            "Epoch: 6/10... Step: 4300... Loss: 0.012970... Val Loss: 0.301604\n",
            "Epoch: 6/10... Step: 4400... Loss: 0.001680... Val Loss: 0.286281\n",
            "Epoch: 6/10... Step: 4500... Loss: 0.002704... Val Loss: 0.272539\n",
            "Epoch: 6/10... Step: 4600... Loss: 0.005021... Val Loss: 0.286730\n",
            "Epoch: 6/10... Step: 4700... Loss: 0.032302... Val Loss: 0.256319\n",
            "Epoch: 6/10... Step: 4800... Loss: 0.002435... Val Loss: 0.280552\n",
            "Epoch: 7/10... Step: 4900... Loss: 0.000460... Val Loss: 0.330942\n",
            "Epoch: 7/10... Step: 5000... Loss: 0.001510... Val Loss: 0.336448\n",
            "Epoch: 7/10... Step: 5100... Loss: 0.005025... Val Loss: 0.372500\n",
            "Epoch: 7/10... Step: 5200... Loss: 0.026119... Val Loss: 0.310588\n",
            "Epoch: 7/10... Step: 5300... Loss: 0.076481... Val Loss: 0.316086\n",
            "Epoch: 7/10... Step: 5400... Loss: 0.129013... Val Loss: 0.272955\n",
            "Epoch: 7/10... Step: 5500... Loss: 0.008158... Val Loss: 0.305793\n",
            "Epoch: 7/10... Step: 5600... Loss: 0.134766... Val Loss: 0.308435\n",
            "Epoch: 8/10... Step: 5700... Loss: 0.041195... Val Loss: 0.318236\n",
            "Epoch: 8/10... Step: 5800... Loss: 0.214365... Val Loss: 0.283332\n",
            "Epoch: 8/10... Step: 5900... Loss: 0.005053... Val Loss: 0.301188\n",
            "Epoch: 8/10... Step: 6000... Loss: 0.031969... Val Loss: 0.297753\n",
            "Epoch: 8/10... Step: 6100... Loss: 0.012092... Val Loss: 0.318504\n",
            "Epoch: 8/10... Step: 6200... Loss: 0.003226... Val Loss: 0.331629\n",
            "Epoch: 8/10... Step: 6300... Loss: 0.006407... Val Loss: 0.344724\n",
            "Epoch: 8/10... Step: 6400... Loss: 0.011649... Val Loss: 0.303454\n",
            "Epoch: 9/10... Step: 6500... Loss: 0.002616... Val Loss: 0.325288\n",
            "Epoch: 9/10... Step: 6600... Loss: 0.010566... Val Loss: 0.359467\n",
            "Epoch: 9/10... Step: 6700... Loss: 0.009181... Val Loss: 0.384707\n",
            "Epoch: 9/10... Step: 6800... Loss: 0.122300... Val Loss: 0.304752\n",
            "Epoch: 9/10... Step: 6900... Loss: 0.000398... Val Loss: 0.339212\n",
            "Epoch: 9/10... Step: 7000... Loss: 0.012907... Val Loss: 0.311084\n",
            "Epoch: 9/10... Step: 7100... Loss: 0.013801... Val Loss: 0.327415\n",
            "Epoch: 9/10... Step: 7200... Loss: 0.000138... Val Loss: 0.326556\n",
            "Epoch: 10/10... Step: 7300... Loss: 0.000271... Val Loss: 0.365672\n",
            "Epoch: 10/10... Step: 7400... Loss: 0.000299... Val Loss: 0.334392\n",
            "Epoch: 10/10... Step: 7500... Loss: 0.003949... Val Loss: 0.356653\n",
            "Epoch: 10/10... Step: 7600... Loss: 0.002126... Val Loss: 0.370728\n",
            "Epoch: 10/10... Step: 7700... Loss: 0.005676... Val Loss: 0.340178\n",
            "Epoch: 10/10... Step: 7800... Loss: 0.009186... Val Loss: 0.363372\n",
            "Epoch: 10/10... Step: 7900... Loss: 0.000491... Val Loss: 0.347098\n",
            "Epoch: 10/10... Step: 8000... Loss: 0.011609... Val Loss: 0.323371\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0lBMLThOpg9c",
        "outputId": "1f1965fa-ea73-4bbe-ec84-9af20268ab71"
      },
      "source": [
        "# Get test data loss and accuracy\n",
        "\n",
        "test_losses = [] # track loss\n",
        "num_correct = 0\n",
        "\n",
        "# init hidden state\n",
        "h = net.init_hidden(batch_size)\n",
        "\n",
        "net.eval()\n",
        "# iterate over test data\n",
        "for inputs, labels in test_loader:\n",
        "\n",
        "    # Creating new variables for the hidden state, otherwise\n",
        "    # we'd backprop through the entire training history\n",
        "    h = h.data\n",
        "\n",
        "    if(train_on_gpu):\n",
        "        inputs, labels = inputs.cuda(), labels.cuda()\n",
        "    \n",
        "    # get predicted outputs\n",
        "    output, h = net(inputs, h)\n",
        "    \n",
        "    # calculate loss\n",
        "    test_loss = criterion(output.squeeze(), labels.float())\n",
        "    test_losses.append(test_loss.item())\n",
        "    \n",
        "    # convert output probabilities to predicted class (0 or 1)\n",
        "    pred = torch.round(output.squeeze())  # rounds to the nearest integer\n",
        "    \n",
        "    # compare predictions to true label\n",
        "    correct_tensor = pred.eq(labels.float().view_as(pred))\n",
        "    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
        "    num_correct += np.sum(correct)\n",
        "\n",
        "\n",
        "# -- stats! -- ##\n",
        "# avg test loss\n",
        "print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
        "\n",
        "# accuracy over all test data\n",
        "test_acc = num_correct/len(test_loader.dataset)\n",
        "print(\"Test accuracy: {:.3f}%\".format(test_acc*100))\n"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test loss: 0.004\n",
            "Test accuracy: 99.875%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5q_whNSpg6U"
      },
      "source": [
        "\n",
        "def preprocess(review):\n",
        "    review = review.lower()\n",
        "    word_list = review.split()\n",
        "    num_list = []\n",
        "    #list of reviews\n",
        "    #though it contains only one review as of now\n",
        "    reviews_int = []\n",
        "    for word in word_list:\n",
        "        if word in review_dict.token2id:\n",
        "            num_list.append(review_dict.token2id[word])\n",
        "    reviews_int.append(num_list)\n",
        "    return reviews_int"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJY_Mswqpg3H"
      },
      "source": [
        "\n",
        "def predict(net, test_review, sequence_length=200):\n",
        "    ''' Prints out whether a give review is predicted to be \n",
        "        positive or negative in sentiment, using a trained model.\n",
        "        \n",
        "        params:\n",
        "        net - A trained net \n",
        "        test_review - a review made of normal text and punctuation\n",
        "        sequence_length - the padded length of a review\n",
        "        '''\n",
        "    #change the reviews to sequence of integers\n",
        "    int_rev = preprocess(test_review)\n",
        "    #pad the reviews as per the sequence length of the feature\n",
        "    features = pad_features(int_rev, L=seq_length)\n",
        "    \n",
        "    #changing the features to PyTorch tensor\n",
        "    features = torch.from_numpy(features)\n",
        "    \n",
        "    #pass the features to the model to get prediction\n",
        "    net.eval()\n",
        "    val_h = net.init_hidden(1)\n",
        "    val_h = val_h.data\n",
        "\n",
        "    if(train_on_gpu):\n",
        "        features = features.cuda()\n",
        "\n",
        "    output, val_h = net(features, val_h)\n",
        "    \n",
        "    #rounding the output to nearest 0 or 1\n",
        "    pred = torch.round(output)\n",
        "    \n",
        "    #mapping the numeric values to postive or negative\n",
        "    sent = [\"Positive\" if pred.item() == 1 else \"Negative\"]\n",
        "    \n",
        "    # print custom response based on whether test_review is pos/neg\n",
        "    print(sent,'  ',((pred.item()==1)*output.item()+(pred.item()==0)*(1-output.item()))*100,'%')"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_fc1miopg0R"
      },
      "source": [
        "positive_review = 'A great movie. Totally worth every penny. '"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l8ubEhmypgxE",
        "outputId": "b03cae55-b4e6-41db-e42f-c422b1384646"
      },
      "source": [
        "seq_length=200\n",
        "predict(net, positive_review, seq_length)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Positive']    99.9944806098938 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJGOmK9JpqEA"
      },
      "source": [
        "\n",
        "negative_review = 'Amongst the worst works of diresctor nick furry. I want my money back. Waste of time.'"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5YtYVuCrpqBx",
        "outputId": "4233a3c0-2ca2-4241-d5a4-f74675bd9ea5"
      },
      "source": [
        "seq_length=200\n",
        "predict(net, negative_review, seq_length)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Negative']    99.99911185304882 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HBZfddQrpp_L",
        "outputId": "f726e6a0-095c-479a-a2e5-78b011a1ae8c"
      },
      "source": [
        "predict(net,'waste',sequence_length=200)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Positive']    65.98355174064636 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ek8638nypp8d",
        "outputId": "5ec4d4e9-644b-4c50-dd55-9466da4dad4e"
      },
      "source": [
        "\n",
        "# Get test data loss and accuracy\n",
        "\n",
        "test_losses = [] # track loss\n",
        "num_correct = 0\n",
        "\n",
        "# init hidden state\n",
        "h = net.init_hidden(batch_size)\n",
        "\n",
        "net.eval()\n",
        "# iterate over test data\n",
        "for inputs, labels in valid_loader:\n",
        "\n",
        "    # Creating new variables for the hidden state, otherwise\n",
        "    # we'd backprop through the entire training history\n",
        "    h = h.data\n",
        "\n",
        "    if(train_on_gpu):\n",
        "        inputs, labels = inputs.cuda(), labels.cuda()\n",
        "    \n",
        "    # get predicted outputs\n",
        "    output, h = net(inputs, h)\n",
        "    \n",
        "    # calculate loss\n",
        "    test_loss = criterion(output.squeeze(), labels.float())\n",
        "    test_losses.append(test_loss.item())\n",
        "    \n",
        "    # convert output probabilities to predicted class (0 or 1)\n",
        "    pred = torch.round(output.squeeze())  # rounds to the nearest integer\n",
        "    \n",
        "    # compare predictions to true label\n",
        "    correct_tensor = pred.eq(labels.float().view_as(pred))\n",
        "    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
        "    num_correct += np.sum(correct)\n",
        "\n",
        "\n",
        "# -- stats! -- ##\n",
        "# avg test loss\n",
        "print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
        "\n",
        "# accuracy over all test data\n",
        "test_acc = num_correct/len(valid_loader.dataset)\n",
        "print(\"Test accuracy: {:.3f}%\".format(test_acc*100))"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test loss: 0.324\n",
            "Test accuracy: 93.905%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
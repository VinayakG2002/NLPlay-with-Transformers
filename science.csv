0,review
1,"HE HACKER TOURIST ventures forth across the wide and wondrous meatspace of three continents, chronicling the laying of the longest wire on Earth.

In which the hacker tourist ventures forth across the wide and wondrous meatspace of three continents, acquainting himself with the customs and dialects of the exotic Manhole Villagers of Thailand, the U-Turn Tunnelers of the Nile Delta, the Cable Nomads of Lan tao Island, the Slack Control Wizards of Chelmsford, the Subterranean Ex-Telegraphers of Cornwall, and other previously unknown and unchronicled folk; also, biographical sketches of the two long-dead Supreme Ninja Hacker Mage Lords of global telecommunications, and other material pertaining to the business and technology of Undersea Fiber-Optic Cables, as well as an account of the laying of the longest wire on Earth, which should not be without interest to the readers of WIRED.

INFORMATION MOVES, OR we move to it. Moving to it has rarely been popular and is growing unfashionable; nowadays we demand that the information come to us. This can be accomplished in three basic ways: moving physical media around, broadcasting radiation through space, and sending signals through wires. This article is about what will, for a short time anyway, be the biggest and best wire ever made.

Wires warp cyberspace in the same way wormholes warp physical space: the two points at opposite ends of a wire are, for informational purposes, the same point, even if they are on opposite sides of the planet. The cyberspace-warping power of wires, therefore, changes the geometry of the world of commerce and politics and ideas that we live in. The financial districts of New York, London, and Tokyo, linked by thousands of wires, are much closer to each other than, say, the Bronx is to Manhattan.

Today this is all quite familiar, but in the 19th century, when the first feeble bits struggled down the first undersea cable joining the Old World to the New, it must have made people's hair stand up on end in more than just the purely electrical sense—it must have seemed supernatural. Perhaps this sort of feeling explains why when Samuel Morse stretched a wire between Washington and Baltimore in 1844, the first message he sent with his code was ""What hath God wrought!""—almost as if he needed to reassure himself and others that God, and not the Devil, was behind it.

During the decades after Morse's ""What hath God wrought!"" a plethora of different codes, signalling techniques, and sending and receiving machines were patented. A web of wires was spun across every modern city on the globe, and longer wires were strung between cities. Some of the early technologies were, in retrospect, flaky: one early inventor wanted to use 26-wire cables, one wire for each letter of the alphabet. But it quickly became evident that it was best to keep the number of individual wires as low as possible and find clever ways to fit more information onto them.

ADVERTISEMENT

This requires more ingenuity than you might think—wires have never been perfectly transparent carriers of data; they have always degraded the information put into them. In general, this gets worse as the wire gets longer, and so as the early telegraph networks spanned greater distances, the people building them had to edge away from the seat-of-the-pants engineering practices that, applied in another field, gave us so many boiler explosions, and toward the more scientific approach that is the standard of practice today.

Still, telegraphy, like many other forms of engineering, retained a certain barnyard, improvised quality until the Year of Our Lord 1858, when the terrifyingly high financial stakes and shockingly formidable technical challenges of the first transatlantic submarine cable brought certain long-simmering conflicts to a rolling boil, incarnated the old and new approaches in the persons of Dr. Wildman Whitehouse and Professor William Thomson, respectively, and brought the conflict between them into the highest possible relief in the form of an inquiry and a scandal that rocked the Victorian world. Thomson came out on top, with a new title and name—Lord Kelvin.
Everything that has occurred in Silicon Valley in the last couple of decades also occurred in the 1850s. Anyone who thinks that wild-ass high tech venture capitalism is a late-20th-century California phenomenon needs to read about the maniacs who built the first transatlantic cable projects (I recommend Arthur C. Clarke's book How the World Was One). The only things that have changed since then are that the stakes have gotten smaller, the process more bureaucratized, and the personalities less interesting.

Those early cables were eventually made to work, albeit not without founding whole new fields of scientific inquiry and generating many lucrative patents. Undersea cables, and long-distance communications in general, became the highest of high tech, with many of the same connotations as rocket science or nuclear physics or brain surgery would acquire in later decades. Some countries and companies (the distinction between countries and companies is hazy in the telco world) became very good at it, and some didn't. AT&T acquired a dominance of the field that largely continues to this day and is only now being seriously challenged by a project called FLAG: the Fiberoptic Link Around the Globe.

In which the Hacker Tourist encounters: Penang, a microcosm of the Internet. Rubber, Penang's chief commodity, and its many uses: protecting wires from the elements and concupiscent wanderers from harmful DNA. Advantages of chastity, both for hacker tourists and for cable layers. Bizarre Spectacles in the jungles of southern Thailand. FLAG, its origins and its enemies.

5° 241 24.932' N, 100° 241 19.748' E City of George Town, Island of Penang, Malaysia

FLAG, a fiber-optic cable now being built from England to Japan, is a skinny little cuss (about an inch in diameter), but it is 28,000 kilometers long, which is long even compared to really big things like the planet Earth. When it is finished in September 1997, it arguably will be the longest engineering project in history. Writing about it necessitates a lot of banging around through meatspace. Over the course of two months, photographer Alex Tehrani and I hit six countries and four continents trying to get a grip on this longest, fastest, mother of all wires. I took a GPS receiver with me so that I could have at least a general idea of where the hell we were. It gave me the above reading in front of a Chinese temple around the corner from the Shangri-La Hotel in Penang, Malaysia, which was only one of 100 peculiar spots around the globe where I suddenly pulled up short and asked myself, ""What the hell am I doing here?""

You might well ask yourself the same question before diving into an article as long as this one. The answer is that we all depend heavily on wires, but we hardly ever think about them. Before learning about FLAG, I knew that data packets could get from America to Asia or the Middle East, but I had no idea how. I knew that it had something to do with wires across the bottom of the ocean, but I didn't know how many of those wires existed, how they got there, who controlled them, or how many bits they could carry.

According to legend, in 1876 the first sounds transmitted down a wire were Alexander Graham Bell saying ""Mr. Watson, come here. I want you."" Compared with Morse's ""What hath God wrought!'' this is disappointingly banal—as if Neil Armstrong, setting foot on the moon, had uttered the words: ""Buzz, could you toss me that rock hammer?'' It's as though during the 32 years following Morse's message, people had become inured to the amazing powers of wire.
Today, another 120 years later, we take wires completely for granted. This is most unwise. People who use the Internet (or for that matter, who make long-distance phone calls) but who don't know about wires are just like the millions of complacent motorists who pump gasoline into their cars without ever considering where it came from or how it found its way to the corner gas station. That works only until the political situation in the Middle East gets all screwed up, or an oil tanker runs aground on a wildlife refuge. In the same way, it behooves wired people to know a few things about wires—how they work, where they lie, who owns them, and what sorts of business deals and political machinations bring them into being.

In the hopes of learning more about the modern business of really, really long wires, we spent much of the summer of 1996 in pursuits such as: being arrested by toothless, shotgun-toting Egyptian cops; getting pushed around by a drunken smuggler queen on a Thai train; vaulting over rustic gates to take emergency shits in isolated fields; being kept awake by groovy Eurotrash backpackers singing songs; blowing Saharan dust out of cameras; scraping equatorial mold out of fountain pens; stuffing faded banknotes into the palms of Egyptian service-industry professionals; trying to persuade non-English-speaking taxi drivers that we really did want to visit the beach even though it was pouring rain; and laundering clothes by showering in them. We still missed more than half the countries FLAG touches.

Our method was not exactly journalism nor tourism in the normal sense but what might be thought of as a new field of human endeavor called hacker tourism: travel to exotic locations in search of sights and sensations that only would be of interest to a geek.

I will introduce sections with readings from my trusty GPS in case other hacker tourists would like to leap over the same rustic gates or get rained on at the same beaches

5° 26.325' N, 100° 17.417' E Penang Botanical Gardens

Penang, one of the first sites visited by this hacker tourist partly because of its little-known historical importance to wires, lies just off the west coast of the Malay Peninsula. The British acquired it from the local sultan in the late 1700s, built a pathetic fort above the harbor, and named it, appropriately, after the hapless General Cornwallis. They set up a couple of churches and established the kernel of a judicial system. A vigorous market grew up around them. A few kilometers away, they built a botanical garden.

This seems like an odd set of priorities to us today. But gardens were not mere decorations to the British—they were strategic installations.

The headquarters was Kew Gardens outside of London. Penang was one of the forward outposts, and it became incomparably more important than the nearby fort. In 1876, 70,000 seeds of the rubber tree, painstakingly collected by botanists in the Amazon rain forest, were brought to Kew Gardens and planted in a greenhouse. About 2,800 of them germinated and were shipped to the botanical gardens in Sri Lanka and Penang, where they propagated explosively and were used to establish rubber plantations.

Most of these plantations were on the neighboring Malay Peninsula, a lumpy, bony tentacle of land that stretches for 1,000 miles from Bangkok in the north to Singapore in the south, where it grazes the equator. The landscape is a stalemate between, on one hand, the devastatingly powerful erosive forces of continual tropical rainstorms and dense plant life, and, on the other hand, some really, really hard rocks. Anything with the least propensity to be eroded did so a long time ago and turned into a paddy. What's left are ridges of stone that rise almost vertically from the landscape and are still mostly covered with rain forest, notwithstanding efforts by the locals to cut it all down. The flat stuff is all used for something—coconuts, date palms, banana trees, and above all, rubber. Until artificial rubber was invented by the colony-impaired Germans, no modern economy could exist without the natural stuff. All of the important powers had tropical colonies where rubber was produced. For the Netherlands, it was Indonesia; for France, it was Indochina; for the British, it was what they then called Malaya, as well as many other places.

Without rubber and another kind of tree resin called gutta-percha, it would not have been possible to wire the world. Early telegraph lines were just naked conductors strung from pole to pole, but this worked poorly, especially in wet conditions, so some kind of flexible but durable insulation was needed. After much trial and error, rubber became the standard for terrestrial and aerial wires while gutta-percha (a natural gum also derived from a tree grown in Malaya) was used for submarine cables. Gutta-percha is humble-looking stuff, a nondescript brown crud that surrounds the inner core of old submarine cables to a thickness of perhaps 1 centimeter, but it was a wonder material back in those days, and the longer it remained immersed in salt water, the better it got.

So far, it was all according to the general plan that the British had in mind: find some useful DNA in the Americas, stockpile it at Kew Gardens, propagate it to other botanical gardens around the world, make money off the proceeds, and grow the economy. Modern-day Penang, however, is a good example of the notion of unintended consequences.

As soon as the British had established the rule of law in Penang, various kinds of Chinese people began to move in and establish businesses. Most of them were Hokkien Chinese from north of Hong Kong, though Cantonese, Hakka, and other groups also settled there. Likewise, Tamils and Sikhs came from across the Bay of Bengal. As rubber trees began to take over the countryside, a common arrangement was for Chinese immigrants to establish rubber plantations and hire Indian immigrants (as well as Malays) as laborers.

The British involvement, then, was more catalytic than anything else. They didn't own the rubber plantations. They merely bought the rubber on an open market from Chinese brokers who in turn bought it from producers of various ethnicities. The market was just a few square blocks of George Town where British law was enforced, i.e. where businessmen could rely on a few basics like property rights, contracts, and a currency.

During and after World War II, the British lost what presence they had here. Penang fell to the Japanese and became a base for German U-Boats patrolling the Indian Ocean. Later, there was a somewhat messy transition to independence involving a communist insurrection and a war with Indonesia. Today, Malaysia is one of Asia's economic supernovas and evidently has decided that it will be second to none when it comes to the Internet. They are furiously wiring up the place and have established JARING, which is the Malaysian Internet (this word is a somewhat tortured English acronym that happens to spell out the Malay word for the Net).

If you have a look at JARING's homepage (www.jaring.my/jaring), you will be confronted by a link that will take you to a page reciting Malaysia's censorship laws, which, like most censorship laws, are ridiculously vague and hence sort of creepy and yet, in the context of the Internet, totally unworkable.

In a way, the architects of JARING are trying to run the Kew Gardens experiment all over again. By adopting the Internet protocol for their national information infrastructure, they have copied the same DNA that, planted in the deregulated telecom environment of the United States, has grown like some unstoppable exotic weed. Now they are trying to raise the same plant inside a hothouse (because they want it to flourish) but in a pot (because they don't want it to escape into the wild).
They seem to have misunderstood both their own history and that of the Internet, which run strangely parallel. Today the streets of George Town, Penang's main city, are so vivid, crowded, and intensely multicultural that by comparison they make New York City look like Colonial Williamsburg. Every block has a mosque or Hindu temple or Buddhist shrine or Christian church. You can get any kind of food, hear any language. The place is thronged, but it's affluent, and it works. It's a lot like the Internet.

Both Penang and the Internet were established basically for strategic military reasons. In both cases, what was built by the military was merely a kernel for a much vaster phenomenon that came along later. This kernel was really nothing more than a protocol, a set of rules. If you wanted to follow those rules, you could participate, otherwise you were free to go elsewhere. Because the protocol laid down a standard way for people to interact, which was clearly set out and could be understood by anyone, it attracted smart, adaptable, ambitious people from all over the place, and at a certain point it flew completely out of control and turned into something that no one had ever envisioned: something thriving, colorful, wildly diverse, essentially peaceful, and plagued only by the congestion of its own success.

JARING's link to the global Internet is over an undersea cable that connects it to the United States. This is typical of many Southeast Asian countries, which are far better connected to the US than they are to one another. But in late June of 1996, a barge called the Elbe appeared off the coast of Penang. Divers and boats came ashore, braving an infestation of sea snakes, and floated in a segment of armored cable that will become Malaysia's link to FLAG. The capacity of that cable is theoretically some 5.3 Gbps. Much of this will be used for telephone and other non-Internet purposes, but it can't help but serve as a major floodgate between JARING, the censored pseudo-Internet of Malaysia, and the rest of the Net. After that, it will be interesting to see how long JARING remains confined to its pot.

FLAG facts
The FLAG system, that mother of all wires, starts at Porthcurno, England, and proceeds to Estepona, Spain; through the Strait of Gibraltar to Palermo, Sicily; across the Mediterranean to Alexandria and Port Said, Egypt; overland from those two cities to Suez, Egypt; down the Gulf of Suez and the Red Sea, with a potential branching unit to Jedda, Saudia Arabia; around the Arabian Peninsula to Dubai, site of the FLAG Network Operations Center; across the Indian Ocean to Bombay; around the tip of India and across the Bay of Bengal and the Andaman Sea to Ban Pak Bara, Thailand, with a branch down to Penang, Malaysia; overland across Thailand to Songkhla; up through the South China Sea to Lan Tao Island in Hong Kong; up the coast of China to a branch in the East China Sea where one fork goes to Shanghai and the other to Koje-do Island in Korea, and finally to two separate landings in Japan—Ninomiya and Miura, which are owned by rival carriers.

Phone company people tend to think (and do business) in terms of circuits. Hacker tourists, by contrast, tend to think in terms of bits per second. Converting between these two units of measurements is simple: on any modern phone system, the conversations are transmitted digitally, and the standard bit rate that is used for this purpose is 64 kbps. A circuit, then, in telephony jargon, amounts to a datastream of 64 kbps.Copper submarine cables of only a few decades ago could carry only a few dozen circuits—say, about 2,500 kbps total. The first generation of optical-fiber cables, by contrast, carries more than 1,000 times as much data—280 Mbps of data per fiber pair. (Fibers always come in pairs. This practice seems obvious to a telephony person, who is in the business of setting up symmetrical two-way circuits, but makes no particular sense to a hacker tourist who tends to think in terms of one-way packet transmission. The split between these two ways of thinking runs very deep and accounts for much tumult in the telecom world, as will be explained later.) The second generation of optical-fiber cables carries 560 Mbps per fiber pair. FLAG and other third-generation systems will carry 5.3 Gbps per pair. Or, in the system of units typically used by phone company people, they will carry 60,000 circuits on each fiber pair.

If you multiply 60,000 circuits times 64 kbps per circuit, you get a bit rate of only 3.84 Gbps, which leaves 1.46 Gbps unaccounted for. This bandwidth is devoted to various kinds of overhead, such as frame headers and error correction. The FLAG cable contains two sets of fiber pairs, and so its theoretical maximum capacity is 120,000 circuits, or (not counting the overhead) just under 8 Gbps of actual throughput.

These numbers really knock 'em dead in the phone industry. To the hacker tourist, or anyone who spends much time messing around with computer networks, they seem distinctly underwhelming. All this trouble and expense for a measly 8 Gbps? You've got to be kidding! Again, it comes down to a radical difference in perspective between telephony people and internet people.

In defense of telephony people, it must be pointed out that they are the ones who really know the score when it comes to sending bits across oceans. Netheads have heard so much puffery about the robust nature of the Internet and its amazing ability to route around obstacles that they frequently have a grossly inflated conception of how many routes packets can take between continents and how much bandwidth those routes can carry. As of this writing, I have learned that nearly the entire state of Minnesota was recently cut off from the Internet for 13 hours because it had only one primary connection to the global Net, and that link went down. If Minnesota, of all places, is so vulnerable, one can imagine how tenuous many international links must be.

Douglas Barnes, an Oakland-based hacker and cypherpunk, looked into this issue a couple of years ago when, inspired by Bruce Sterling's Islands in the Net, he was doing background research on a project to set up a data haven in the Caribbean. ""I found out that the idea of the Internet as a highly distributed, redundant global communications system is a myth,'' he discovered. ""Virtually all communications between countries take place through a very small number of bottlenecks, and the available bandwidth simply isn't that great.'' And he cautions: ""Even outfits like FLAG don't really grok the Internet. The undersized cables they are running reflect their myopic outlook.''

So the bad news is that the capacity of modern undersea cables like FLAG isn't very impressive by Internet standards, but the slightly better news is that such cables are much better than what we have now.Here's how they work: Signals are transmitted down the fiber as modulated laser light with a wavelength of 1,558 nanometers (nm), which is in the infrared range. These signals begin to fade after they have traveled a certain distance, so it's necessary to build amplifiers into the cable every so often. In the case of FLAG, the spacing of these amplifiers ranges from 45 to 85 kilometers. They work on a strikingly simple and elegant principle. Each amplifier contains an approximately 10-meter-long piece of special fiber that has been doped with erbium ions, making it capable of functioning as a laser medium. A separate semiconductor laser built into the amplifier generates powerful light at 1,480 nm—close to the same frequency as the signal beam, but not close enough to interfere with it. This light, directed into the doped fiber, pumps the electrons orbiting around those erbium ions up to a higher energy level.
The signal coming down the FLAG cable passes through the doped fiber and causes it to lase, i.e., the excited electrons drop back down to a lower energy level, emitting light that is coherent with the incoming signal—which is to say that it is an exact copy of the incoming signal, except more powerful.

The amplifiers need power—up to 10,000 volts DC, at 0.9 amperes. Since public 10,000-volt outlets are few and far between on the bottom of the ocean, this power must be delivered down the same cable that carries the fibers. The cable, therefore, consists of an inner core of four optical fibers, coated with plastic jackets of different colors so that the people at opposite ends can tell which is which, plus a thin copper wire that is used for test purposes. The total thickness of these elements taken together is comparable to a pencil lead; they are contained within a transparent plastic tube. Surrounding this tube is a sheath consisting of three steel segments designed so that they interlock and form a circular jacket. Around that is a layer of about 20 steel ""strength wires""—each perhaps 2 mm in diameter—that wrap around the core in a steep helix. Around the strength wires goes a copper tube that serves as the conductor for the 10,000-volt power feed. Only one conductor is needed because the ocean serves as the ground wire. This tube also is watertight and so performs the additional function of protecting the cable's innards. It then is surrounded by polyethylene insulation to a total thickness of about an inch. To protect it from the rigors of shipment and laying, the entire cable is clothed in good old-fashioned tarred jute, although jute nowadays is made from plastic, not hemp.

This suffices for the deep-sea portions of the cable. In shallower waters, additional layers of protection are laid on, beginning with a steel antishark jacket. As the shore is approached, various other layers of steel armoring wires are added.

This more or less describes how all submarine cables are being made as of 1996. Only a few companies in the world know how to make cables like this: AT&T Submarine Systems International (AT&T-SSI) in the US, Alcatel in France, and KDD Submarine Cable Systems (KDD-SCS) in Japan, among others. AT&T-SSI and KDD-SCS frequently work together on large projects and are responsible for FLAG. Alcatel, in classic French fasion, likes to go it alone.

This basic technology will, by the end of the century, be carrying most of the information between continents. Copper-based coaxial cable systems are still in operation in many places around the world, but all of them will have reached the end of their practical lifetimes within a few years. Even if they still function, they are not worth the trouble it takes to operate them. TPC-1 (Trans Pacific Cable #1), which connected Japan to Guam and hence to the United States in 1964, is still in perfect working order, but so commercially worthless that it has been turned over to a team at Tokyo University, which is using it to carry out seismic research. The capacity of such cables is so tiny that modern fiber cables could absorb all of their traffic with barely a hiccup if the right switches and routers were in place. Likewise, satellites have failed to match some of the latest leaps in fiber capacity and can no longer compete with submarine cables, at least until such time as low-flying constellations such as Iridium and Teledesic begin operating.

Within the next few years, several huge third-generational optical fiber systems will be coming online: not only FLAG but a FLAG competitor called SEA-ME-WE 3 (Southeast Asia-Middle East-Western Europe #3); TPC-5 (Trans-Pacific Cable #5); APCN (Asia-Pacific Cable Network), which is a web of cables interconnecting Japan, Korea, Hong Kong, Taiwan, Malaysia, Thailand, Indonesia, Singapore, Australia, and the Philippines; and the latest TAT (Transatlantic) cable. So FLAG is part of a trend that will soon bring about a vast increase in intercontinental bandwidth.What is unusual about FLAG is not its length (although it will be the longest cable ever constructed) or its technology (which is shared by other cables) but how it came into existence. But that's a business question which will be dealt with later. First, the hacker tourist is going to travel a short distance up the Malay Peninsula to southern Thailand, one of the two places where FLAG passes overland. On a world map this looks about as difficult as throwing an extension cord over a sandbar, but when you actually get there, it turns out to be a colossal project

7° 3.467' N,100° 22.489' EFLAG manhole production site, southern Thailand

Large portions of this section were written in a hotel in Ban Hat Yai, Thailand, which is one of the information-transfer capitals of the planet regardless of whether you think of information transfer as bits propagating down an optical fiber, profound and complex religious faiths being transmitted down through countless generations, or genetic material being interchanged between consenting adults. Male travelers approaching Ban Hat Yai will have a difficult time convincing travel agents, railway conductors, and taxi drivers that they are coming only to look at a big fat wire, but the hacker tourist must get used to being misunderstood.

We stayed in a hotel with all the glossy accoutrements of an Asian business center plus a few perks such as partially used jumbo condom packages squirreled away on closet shelves, disconcertingly huge love marks on the sofas, and extraordinarily long, fine, black hairs all over the bathroom. While writing, I sat before a picture window looking out over a fine view of: a well-maintained but completely empty swimming pool, a green Carlsberg Beer billboard written in Thai script, an industrial-scale whorehouse catering to Japanese ""businessmen,"" a rather fine Buddhist temple complex, and, behind that, a district of brand-new high-rise hotels built to cater to the burgeoning information-transfer industry, almost none of which has anything to do with bits and bytes. Tropical storms rolled through, lightning flashed, I sucked down European beers from the minibar and tried to cope with a bad case of information overload. FLAG is a huge project, bigger and more complicated than many wars, and to visit even chunks of this cable operation is to be floored by it.

We first met Jim Daily and Alan Wall underneath that big Carlsberg sign, sitting out in a late-afternoon rainstorm under an umbrella, having a couple of beers—""the only ferangs here,"" as Wall told me on the phone, using the local term for foreign devil. Daily is American, 2 meters tall, blond, blue-eyed, khaki-and-polo-shirted, gregarious, absolutely plain-spoken, and almost always seems to be having a great time. Wall is English, shorter, dark-haired, impeccably suited, cagey, reticent, and dry. Both are in their 50s. It is of some significance to this story that, at the end of the day, these two men unwind by sitting out in the rain and hoisting a beer, paying no attention whatsoever to the industrial-scale whorehouse next door. Both of them have seen many young Western men arrive here on business missions and completely lose control of their sphincters and become impediments to any kind of organized activity. Daily hired Wall because, like Daily, he is a stable family man who has his act together. They are the very definition of a complementary relationship, and they seem to be making excellent progress toward their goal, which is to run two really expensive wires across the Malay Peninsula.

Since these two, and many of the others we will meet on this journey, have much in common with one another, this is as good a place as any to write a general description. They tend to come from the US or the British Commonwealth countries but spend very little time living there. They are cheerful and outgoing, rudely humorous, and frequently have long-term marriages to adaptable wives. They tend to be absolutely straight shooters even when they are talking to a hacker tourist about whom they know nothing. Their openness would probably be career suicide in the atmosphere of Byzantine court-eunuch intrigue that is public life in the United States today. On the other hand, if I had an unlimited amount of money and woke up tomorrow morning with a burning desire to see a 2,000-hole golf course erected on the surface of Mars, I would probably call men like Daily and Wall, do a handshake deal with them, send them a blank check, and not worry about it.Daily works out of Bangkok, the place where banks are headquartered, contracts are written, and 50-ton cranes are to be had. Alan ""the ferang"" Wall lives in Ban Hat Yai, the center of the FLAG operation in Thailand, cruising the cable routes a couple of times a week, materializing unpredictably in the heart of the tropical jungle in a perfectly tailored dark suit to inspect, among other things, FLAG's chain of manhole-making villages.

There were seven of these in existence during the summer of 1996, all lying along one of the two highways that run across the isthmus between the Andaman and the South China Seas. These highways, incidentally, are lined with utility poles carrying both power and communications wires. The tops of the poles are guarded by conical baskets about halfway up.
"
2,"Towards the end of the summer of 1969 – a few weeks after the moon landings, a few days after Woodstock, and a month before the first broadcast of Monty Python's Flying Circus – a large grey metal box was delivered to the office of Leonard Kleinrock, a professor at the University of California in Los Angeles. It was the same size and shape as a household refrigerator, and outwardly, at least, it had about as much charm. But Kleinrock was thrilled: a photograph from the time shows him standing beside it, in requisite late-60s brown tie and brown trousers, beaming like a proud father.

Had he tried to explain his excitement to anyone but his closest colleagues, they probably wouldn't have understood. The few outsiders who knew of the box's existence couldn't even get its name right: it was an IMP, or ""interface message processor"", but the year before, when a Boston company had won the contract to build it, its local senator, Ted Kennedy, sent a telegram praising its ecumenical spirit in creating the first ""interfaith message processor"". Needless to say, though, the box that arrived outside Kleinrock's office wasn't a machine capable of fostering understanding among the great religions of the world. It was much more important than that.

It's impossible to say for certain when the internet began, mainly because nobody can agree on what, precisely, the internet is. (This is only partly a philosophical question: it is also a matter of egos, since several of the people who made key contributions are anxious to claim the credit.) But 29 October 1969 – 40 years ago next week – has a strong claim for being, as Kleinrock puts it today, ""the day the infant internet uttered its first words"". At 10.30pm, as Kleinrock's fellow professors and students crowded around, a computer was connected to the IMP, which made contact with a second IMP, attached to a second computer, several hundred miles away at the Stanford Research Institute, and an undergraduate named Charley Kline tapped out a message. Samuel Morse, sending the first telegraph message 125 years previously, chose the portentous phrase: ""What hath God wrought?"" But Kline's task was to log in remotely from LA to the Stanford machine, and there was no opportunity for portentousness: his instructions were to type the command LOGIN.

Advertisement
To say that the rest is history is the emptiest of cliches – but trying to express the magnitude of what began that day, and what has happened in the decades since, is an undertaking that quickly exposes the limits of language. It's interesting to compare how much has changed in computing and the internet since 1969 with, say, how much has changed in world politics. Consider even the briefest summary of how much has happened on the global stage since 1969: the Vietnam war ended; the cold war escalated then declined; the Berlin Wall fell; communism collapsed; Islamic fundamentalism surged. And yet nothing has quite the power to make people in their 30s, 40s or 50s feel very old indeed as reflecting upon the growth of the internet and the world wide web. Twelve years after Charley Kline's first message on the Arpanet, as it was then known, there were still only 213 computers on the network; but 14 years after that, 16 million people were online, and email was beginning to change the world; the first really usable web browser wasn't launched until 1993, but by 1995 we had Amazon, by 1998 Google, and by 2001, Wikipedia, at which point there were 513 million people online. Today the figure is more like 1.7 billion.

Unless you are 15 years old or younger, you have lived through the dotcom bubble and bust, the birth of Friends Reunited and Craigslist and eBay and Facebook and Twitter, blogging, the browser wars, Google Earth, filesharing controversies, the transformation of the record industry, political campaigning, activism and campaigning, the media, publishing, consumer banking, the pornography industry, travel agencies, dating and retail; and unless you're a specialist, you've probably only been following the most attention-grabbing developments. Here's one of countless statistics that are liable to induce feelings akin to vertigo: on New Year's Day 1994 – only yesterday, in other words – there were an estimated 623 websites. In total. On the whole internet. ""This isn't a matter of ego or crowing,"" says Steve Crocker, who was present that day at UCLA in 1969, ""but there has not been, in the entire history of mankind, anything that has changed so dramatically as computer communications, in terms of the rate of change.""

Advertisement
Looking back now, Kleinrock and Crocker are both struck by how, as young computer scientists, they were simultaneously aware that they were involved in something momentous and, at the same time, merely addressing a fairly mundane technical problem. On the one hand, they were there because of the Russian Sputnik satellite launch, in 1957, which panicked the American defence establishment, prompting Eisenhower to channel millions of dollars into scientific research, and establishing Arpa, the Advanced Research Projects Agency, to try to win the arms technology race. The idea was ""that we would not get surprised again,"" said Robert Taylor, the Arpa scientist who secured the money for the Arpanet, persuading the agency's head to give him a million dollars that had been earmarked for ballistic missile research. With another pioneer of the early internet, JCR Licklider, Taylor co-wrote the paper, ""The Computer As A Communication Device"", which hinted at what was to come. ""In a few years, men will be able to communicate more effectively through a machine than face to face,"" they declared. ""That is rather a startling thing to say, but it is our conclusion.""

On the other hand, the breakthrough accomplished that night in 1969 was a decidedly down-to-earth one. The Arpanet was not, in itself, intended as some kind of secret weapon to put the Soviets in their place: it was simply a way to enable researchers to access computers remotely, because computers were still vast and expensive, and the scientists needed a way to share resources. (The notion that the network was designed so that it would survive a nuclear attack is an urban myth, though some of those involved sometimes used that argument to obtain funding.) The technical problem solved by the IMPs wasn't very exciting, either. It was already possible to link computers by telephone lines, but it was glacially slow, and every computer in the network had to be connected, by a dedicated line, to every other computer, which meant you couldn't connect more than a handful of machines without everything becoming monstrously complex and costly. The solution, called ""packet switching"" – which owed its existence to the work of a British physicist, Donald Davies – involved breaking data down into blocks that could be routed around any part of the network that happened to be free, before getting reassembled at the other end.

""I thought this was important, but I didn't really think it was as challenging as what I thought of as the 'real research',"" says Crocker, a genial Californian, now 65, who went on to play a key role in the expansion of the internet. ""I was particularly fascinated, in those days, by artificial intelligence, and by trying to understand how people think. I thought that was a much more substantial and respectable research topic than merely connecting up a few machines. That was certainly useful, but it wasn't art.""

Still, Kleinrock recalls a tangible sense of excitement that night as Kline sat down at the SDS Sigma 7 computer, connected to the IMP, and at the same time made telephone contact with his opposite number at Stanford. As his colleagues watched, he typed the letter L, to begin the word LOGIN.

Advertisement

""Have you got the L?"" he asked, down the phone line. ""Got the L,"" the voice at Stanford responded.

Kline typed an O. ""Have you got the O?""

""Got the O,"" Stanford replied.

Kline typed a G, at which point the system crashed, and the connection was lost. The G didn't make it through, which meant that, quite by accident, the first message ever transmitted across the nascent internet turned out, after all, to be fittingly biblical:

""LO.""

Frenzied visions of a global conscious brain

One of the most intriguing things about the growth of the internet is this: to a select group of technological thinkers, the surprise wasn't how quickly it spread across the world, remaking business, culture and politics – but that it took so long to get off the ground. Even when computers were mainly run on punch-cards and paper tape, there were whispers that it was inevitable that they would one day work collectively, in a network, rather than individually. (Tracing the origins of online culture even further back is some people's idea of an entertaining game: there are those who will tell you that the Talmud, the book of Jewish law, contains a form of hypertext, the linking-and-clicking structure at the heart of the web.) In 1945, the American presidential science adviser, Vannevar Bush, was already imagining the ""memex"", a device in which ""an individual stores all his books, records, and communications"", which would be linked to each other by ""a mesh of associative trails"", like weblinks. Others had frenzied visions of the world's machines turning into a kind of conscious brain. And in 1946, an astonishingly complete vision of the future appeared in the magazine Astounding Science Fiction. In a story entitled A Logic Named Joe, the author Murray Leinster envisioned a world in which every home was equipped with a tabletop box that he called a ""logic"":

""You got a logic in your house. It looks like a vision receiver used to, only it's got keys instead of dials and you punch the keys for what you wanna get . . . you punch 'Sally Hancock's Phone' an' the screen blinks an' sputters an' you're hooked up with the logic in her house an' if somebody answers you got a vision-phone connection. But besides that, if you punch for the weather forecast [or] who was mistress of the White House durin' Garfield's administration . . . that comes on the screen too. The relays in the tank do it. The tank is a big buildin' full of all the facts in creation . . . hooked in with all the other tanks all over the country . . . The only thing it won't do is tell you exactly what your wife meant when she said, 'Oh, you think so, do you?' in that peculiar kinda voice ""

Despite all these predictions, though, the arrival of the internet in the shape we know it today was never a matter of inevitability. It was a crucial idiosyncracy of the Arpanet that its funding came from the American defence establishment – but that the millions ended up on university campuses, with researchers who embraced an anti-establishment ethic, and who in many cases were committedly leftwing; one computer scientist took great pleasure in wearing an anti-Vietnam badge to a briefing at the Pentagon. Instead of smothering their research in the utmost secrecy – as you might expect of a cold war project aimed at winning a technological battle against Moscow – they made public every step of their thinking, in documents known as Requests For Comments.

Advertisement

Deliberately or not, they helped encourage a vibrant culture of hobbyists on the fringes of academia – students and rank amateurs who built their own electronic bulletin-board systems and eventually FidoNet, a network to connect them to each other. An argument can be made that these unofficial tinkerings did as much to create the public internet as did the Arpanet. Well into the 90s, by the time the Arpanet had been replaced by NSFNet, a larger government-funded network, it was still the official position that only academic researchers, and those affiliated to them, were supposed to use the network. It was the hobbyists, making unofficial connections into the main system, who first opened the internet up to allcomers.

What made all of this possible, on a technical level, was simultaneously the dullest-sounding and most crucial development since Kleinrock's first message. This was the software known as TCP/IP, which made it possible for networks to connect to other networks, creating a ""network of networks"", capable of expanding virtually infinitely – which is another way of defining what the internet is. It's for this reason that the inventors of TCP/IP, Vint Cerf and Bob Kahn, are contenders for the title of fathers of the internet, although Kleinrock, understandably, disagrees. ""Let me use an analogy,"" he says. ""You would certainly not credit the birth of aviation to the invention of the jet engine. The Wright Brothers launched aviation. Jet engines greatly improved things.""

The spread of the internet across the Atlantic, through academia and eventually to the public, is a tale too intricate to recount here, though it bears mentioning that British Telecom and the British government didn't really want the internet at all: along with other European governments, they were in favour of a different networking technology, Open Systems Interconnect. Nevertheless, by July 1992, an Essex-born businessman named Cliff Stanford had opened Demon Internet, Britain's first commercial internet service provider. Officially, the public still wasn't meant to be connecting to the internet. ""But it was never a real problem,"" Stanford says today. ""The people trying to enforce that weren't working very hard to make it happen, and the people working to do the opposite were working much harder."" The French consulate in London was an early customer, paying Demon £10 a month instead of thousands of pounds to lease a private line to Paris from BT.

After a year or so, Demon had between 2,000 and 3,000 users, but they weren't always clear why they had signed up: it was as if they had sensed the direction of the future, in some inchoate fashion, but hadn't thought things through any further than that. ""The question we always got was: 'OK, I'm connected – what do I do now?'"" Stanford recalls. ""It was one of the most common questions on our support line. We would answer with 'Well, what do you want to do? Do you want to send an email?' 'Well, I don't know anyone with an email address.' People got connected, but they didn't know what was meant to happen next.""

Advertisement

Fortunately, a couple of years previously, a British scientist based at Cern, the physics laboratory outside Geneva, had begun to answer that question, and by 1993 his answer was beginning to be known to the general public. What happened next was the web.

The birth of the web

I sent my first email in 1994, not long after arriving at university, from a small, under-ventilated computer room that smelt strongly of sweat. Email had been in existence for decades by then – the @ symbol was introduced in 1971, and the first message, according to the programmer who sent it, Ray Tomlinson, was ""something like QWERTYUIOP"". (The test messages, Tomlinson has said, ""were entirely forgettable, and I have, therefore, forgotten them"".) But according to an unscientific poll of friends, family and colleagues, 1994 seems fairly typical: I was neither an early adopter nor a late one. A couple of years later I got my first mobile phone, which came with two batteries: a very large one, for normal use, and an extremely large one, for those occasions on which you might actually want a few hours of power. By the time I arrived at the Guardian, email was in use, but only as an add-on to the internal messaging system, operated via chunky beige terminals with green-on-black screens. It took for ever to find the @ symbol on the keyboard, and I don't remember anything like an inbox, a sent-mail folder, or attachments. I am 34 years old, but sometimes I feel like Methuselah.

I have no recollection of when I first used the world wide web, though it was almost certainly when people still called it the world wide web, or even W3, perhaps in the same breath as the phrase ""information superhighway"", made popular by Al Gore. (Or ""infobahn"": did any of us really, ever, call the internet the ""infobahn""?) For most of us, though, the web is in effect synonymous with the internet, even if we grasp that in technical terms that's inaccurate: the web is simply a system that sits on top of the internet, making it greatly easier to navigate the information there, and to use it as a medium of sharing and communication. But the distinction rarely seems relevant in everyday life now, which is why its inventor, Tim Berners-Lee, has his own legitimate claim to be the progenitor of the internet as we know it. The first ever website was his own, at CERN: info.cern.ch.

The idea that a network of computers might enable a specific new way of thinking about information, instead of just allowing people to access the data on each other's terminals, had been around for as long as the idea of the network itself: it's there in Vannevar Bush's memex, and Murray Leinster's logics. But the grandest expression of it was Project Xanadu, launched in 1960 by the American philosopher Ted Nelson, who imagined – and started to build – a vast repository for every piece of writing in existence, with everything connected to everything else according to a principle he called ""transclusion"". It was also, presciently, intended as a method for handling many of the problems that would come to plague the media in the age of the internet, automatically channelling small royalties back to the authors of anything that was linked. Xanadu was a mind-spinning vision – and at least according to an unflattering portrayal by Wired magazine in 1995, over which Nelson threatened to sue, led those attempting to create it into a rabbit-hole of confusion, backbiting and ""heart-slashing despair"". Nelson continues to develop Xanadu today, arguing that it is a vastly superior alternative to the web. ""WE FIGHT ON,"" the Xanadu website declares, sounding rather beleaguered, not least since the declaration is made on a website.

Advertisement

Web browsers crossed the border into mainstream use far more rapidly than had been the case with the internet itself: Mosaic launched in 1993 and Netscape followed soon after, though it was an embarrassingly long time before Microsoft realised the commercial necessity of getting involved at all. Amazon and eBay were online by 1995. And in 1998 came Google, offering a powerful new way to search the proliferating mass of information on the web. Until not too long before Google, it had been common for search or directory websites to boast about how much of the web's information they had indexed – the relic of a brief period, hilarious in hindsight, when a user might genuinely have hoped to check all the webpages that mentioned a given subject. Google, and others, saw that the key to the web's future would be helping users exclude almost everything on any given topic, restricting search results to the most relevant pages.

Without most of us quite noticing when it happened, the web went from being a strange new curiosity to a background condition of everyday life: I have no memory of there being an intermediate stage, when, say, half the information I needed on a particular topic could be found online, while the other half still required visits to libraries. ""I remember the first time I saw a web address on the side of a truck, and I thought, huh, OK, something's happening here,"" says Spike Ilacqua, who years beforehand had helped found The World, the first commercial internet service provider in the US. Finally, he stopped telling acquaintances that he worked in ""computers"", and started to say that he worked on ""the internet"", and nobody thought that was strange.

It is absurd – though also unavoidable here – to compact the whole of what happened from then onwards into a few sentences: the dotcom boom, the historically unprecedented dotcom bust, the growing ""digital divide"", and then the hugely significant flourishing, over the last seven years, of what became known as Web 2.0. It is only this latter period that has revealed the true capacity of the web for ""generativity"", for the publishing of blogs by anyone who could type, for podcasting and video-sharing, for the undermining of totalitarian regimes, for the use of sites such as Twitter and Facebook to create (and ruin) friendships, spread fashions and rumours, or organise political resistance. But you almost certainly know all this: it's part of what these days, in many parts of the world, we call ""just being alive"".

The most confounding thing of all is that in a few years' time, all this stupendous change will probably seem like not very much change at all. As Crocker points out, when you're dealing with exponential growth, the distance from A to B looks huge until you get to point C, whereupon the distance between A and B looks like almost nothing; when you get to point D, the distance between B and C looks similarly tiny. One day, presumably, everything that has happened in the last 40 years will look like early throat-clearings — mere preparations for whatever the internet is destined to become. We will be the equivalents of the late-60s computer engineers, in their horn-rimmed glasses, brown suits, and brown ties, strange, period-costume characters populating some dimly remembered past.

Will you remember when the web was something you accessed primarily via a computer? Will you remember when there were places you couldn't get a wireless connection? Will you remember when ""being on the web"" was still a distinct concept, something that described only a part of your life, instead of permeating all of it? Will you remember Google? … as you're joining us from India, we have a small favour to ask. Tens of millions have placed their trust in the Guardian’s high-impact journalism since we started publishing 200 years ago, turning to us in moments of crisis, uncertainty, solidarity and hope. More than 1.5 million readers, from 180 countries, have recently taken the step to support us financially – keeping us open to all, and fiercely independent.


With no shareholders or billionaire owner, we can set our own agenda and provide trustworthy journalism that’s free from commercial and political influence, offering a counterweight to the spread of misinformation. When it’s never mattered more, we can investigate and challenge without fear or favour.


Unlike many others, Guardian journalism is available for everyone to read, regardless of what they can afford to pay. We do this because we believe in information equality. Greater numbers of people can keep track of global events, understand their impact on people and communities, and become inspired to take meaningful action.


We aim to offer readers a comprehensive, international perspective on critical events shaping our world – from the Black Lives Matter movement, to the new American administration, Brexit, and the world's slow emergence from a global pandemic. We are committed to upholding our reputation for urgent, powerful reporting on the climate emergency, and made the decision to reject advertising from fossil fuel companies, divest from the oil and gas industries, and set a course to achieve net zero emissions by 2030.


If there were ever a time to join us, it is now. Every contribution, however big or small, powers our journalism and sustains our future. Support the Guardian from as little as $1 – it only takes a minute. If you can, please consider supporting us with a regular amount each month. Thank you."
3,"A Nation Of Echo Chambers: How The Internet Closed Off The World. Will Leitch, senior writer at Sports On Earth, culture writer for Bloomberg Politics, contributing editor at New York magazine and founder of Deadspin, is doing his yearly fill-in for Drew Magary on today's Thursday Afternoon NFL Dick Joke Jamboroo. (Here is 2011's version, and here's 2012's and here's 2013's.) Leitch has written four books. Find more of his business at his Twitter feed and his official site.

In 2012, actor Rob Schneider, famous for something or other, spoke to a California television station about AB 2109, a California bill that required parents to get a physician's approval to opt out of vaccinating their children (something no sentient physician would ever approve). I only came across this interview recently. It is amazing. You can almost follow along with Schneider's browser history as he continues to ramble on; there's the mom message board, there's the InfoWars THINGS THEY DON'T WANT YOU TO KNOW thread, there's the blog of the doctor with the degree-by-mail who is the only one willing to tell parents the truth. You can tell Schneider spent all night preparing for this interview, jotting down the words he wanted to emphasize, ""efficacy,"" ""toxicity,"" ""Nuremberg laws,"" ""forced sterilization."" He even ends with ""people have to stand up and get educated. Know all the facts.""

In this four-minute clip, I think you can encapsulate the last two years of American culture. But I think it's more than that. I think that they are convinced they're right because they are only talking to other people who are convinced they are right. They have blocked out opposing voices — because they can. If you are an anti-vaccine activist, you can read so much ""information"" supporting your position that, as far as you can tell, you are right. That's what Schneider's talking about up there, that ""get educated"" business. Schneider doesn't see these beliefs as theories, or even as ""beliefs:"" He sees them as stone cold facts. On something about which he is so obviously wrong.

So here's my question: If 20 percent of the country can be so wrong on something so clearly incorrect (and harmful) as child vaccination, primarily because they can choose their evidence over your evidence ... what hope do any of us have? Because the rest of the world is helluva lot more complicated and confusing than whether or not to vaccinate your damn kid.

Chris Rock, when he was doing his big Truth Bomb press tour to promote Top Five, said something fascinating about the difference between President Bush and President Obama. He called Bush a ""cable network"" President; unlike Obama, he only catered to his subscribers. Rock also, astutely, points out ""whoever's the next president will do what Bush did.""

I'm not exactly certain that's true — I have no idea who the subscribers to, say, the Hillary Clinton cable channel are — but in the long term, there is zero doubt that he is right. You see this in every aspect of American life, from entertainment (where the only things anyone watches communally are sports, live musicals, or zombies attempting to eat the brains of thinly drawn caricatures) to politics (where the Republicans just won back the House the same way Bush beat Kerry, by appealing only to their base and not worrying about anyone else) to media (which is so fractured and desperate that it'll pump up whatever dumb Twitter shitstorm happens to be invading their feeds that afternoon, throw it on their front page, and pray; basically, outrage has become America's Assignment Desk). We are run by niche cultures right now. We've seen it from Gamergate to Sony Pictures to you name it. We don't have to build coalitions anymore; we just have to build a bigger coalition than you. We don't have to be right; we just have to be louder than the other guys. It's like that old joke about being chased by a bear.

This increased niche culture is a trademark of the web, and we used to think of it as a positive one; 20 years ago, if you didn't know any Quentin Tarantino or Woody Allen obsessives near you, you could go online and find them. (Theoretically.) The internet opened up a world that was truly revolutionary. But now, now that we're all online, and any novelty to this fact has worn off, the internet has closed that world. We now only have to interact with people who agree with us; if I use Twitter as my primary news source, as so many people do, I can carefully curate my feed to exclude anyone who disagrees with me about anything. (And if someone who slips in there who does, I can call them a horrible person.) Pauline Kael, the late film critic for the New Yorker, was once lambasted (unfairly, and inaccurately) for saying she couldn't believe Nixon was elected because she only personally knew one person who voted for him. But this is now accepted public policy. You don't have to find anyone to contradict you, if you don't want to.

This isn't just common practice now: This is how you win. The entire strategy for succeeding at anything, whether it's winning elections, selling a product or attracting visitors for your Website, revolves around pitching yourself as loudly as you can to those people on your side and turning those who disagree with you into the worst version of themselves, demonizing them into something subhuman and venal. Nuance is tossed out, even if you know a situation is desperately nuanced, in favor of quick points and splash; we've all become the New York Post.

This is simply how communication is done now. The idea of unifying anyone on anything is passé, old thinking, a waste of time. A horrible tragedy happens, and your first reaction, rather than taking a moment to mourn or quietly search for some grace and peace, is instead to start screaming and claiming that those with whom you disagree have blood on their hands. You are rewarded with this by the top slot on the news, a video that goes viral, and everyone on your side applauding you. And when you accept that's all you want to do—to turn away from the fundamental complexity at the heart of the human experience—you find you have no reason to return: After all, every time you say something loudly and strongly enough, the people who agree with you tell you how great you are. Those who disagree? Fuck the haters. Sic 'em, guys.

It can be so demoralizing, so exhausting, to watch this day after day after day. We have begun to shout at those with whom we disagree as if they are terrible drivers and we're within the safety of our own cars; they're the anonymous, faceless monsters we shower with the worst possible motives, just because they happen to be in our way when we're in a hurry. Except they can hear us. And so can everyone else.

So one tries to find hope.

I tend to find it outside, where people, you know, are. Because we drop this act during those strange, disorienting times when we find ourselves in mixed company, lo, real life. The things we do online, or when we think someone is watching, we don't do these things in the real world. In regular, everyday life, we accept all the time that those who disagree with us exist; sometimes we even like them.

They're our families, they're our friends, they're our neighbors, they're the people we open the door for at the supermarket. They're human beings, idiot, scared, just-trying-to-hang-on human beings like every single one of the rest of us. The world is uncertain and terrifying; life is hard and bewildering and unpredictable. We allow for this in our daily interactions in a way we do not in our virtual conversations. We accept human frailty, that we do not share a cerebral cortex with every other person on the planet and therefore will not always see eye-to-eye on all matters. The world is a massive place. There are currently 7.28 billion people on the planet and every single one of them is different. This is a good thing: This is humbling. This is an acceptance that we're all stupid, that we're all overwhelmed, that we're all trying, dammit.

We all have friends and family who believe things we personally find abhorrent. They do things that drive us crazy. Their faults flash brightly above their heads every day. And none of these things matter. We still find a way to love them anyway. Not everybody is just like everybody else. This is a good thing. This might be the only thing.

The Games
All games in the Jamboroo are evaluated for sheer watchability on a scale of 1 to 5 Throwgasms. Five Throwgasms

Lions at Packers. Whew. I swear to God, I write about four different pieces a day, every day, all year, and no single piece stresses me out more than the lead to my fill-in week for Drew at the Jamboroo every December. I have no idea why. It just wears me out. I'm already glad it's over and can just get to the football part down here. Anyway: Hi! I'm Will. I started this joint — back when ""Kinja"" was three Hungarian man-boys being pelted with Werther's Originals until they pedaled fast enough to reboot the Gawker server — though now I'm just the guy who pops up every once in a while with the pretentious movie reviews and the B- ratings. Don't worry, Drew will be back next week for your ALL CAPS GOODNESS; I'll be out of here soon. It's Christmas. Just be happy you're getting any free content at all. I am fairly certain I'm the only person other than LeBron James working today.

Bengals at Steelers. Like everybody else in the professional media, I wrote constantly about Stephen Colbert last week; in our backlash-to-the-backlash-before-anyone-has-even-lashed-in-the-first-place culture, by Friday, everyone was so sick of Colbert tributes they'd forgotten why they liked him in the first place. But just you wait: When David Letterman retires this May, it's going to be that times infinity times a googol. Literally every single person — even dead people! maybe even some farmyard animals! — is going to tell you about how much Letterman meant to them, how he was an inspiration to them, how their whole idea of comedy was forged by David Letterman. These pieces are already starting, so get yours in quick. No criticism from me, though: I will read every single one of these, and I'll probably write a few myself. I apologize for all of us in advance.

Panthers at Falcons. For all the talk about the travesty of an NFC South team finishing below .500 and still hosting a playoff game ... it is worth noting that whoever wins this game is almost certainly going to be favored over an 11-or-12-win Buzzsaw That Is The Arizona Cardinals team in the first round of the playoffs. The 49ers, who haven't won in more than a month, are six-point favorites over the Buzzsaw this week. But we'll get to that. Chargers at Chiefs. Thank you all, by the way, for taking time out from your schedule of hopping from theater to theater to see The Interview and popping in to read this. I made a big fuss out of this myself, but I'd bet now that the movie is actually showing in theaters not owned by spineless weasels, people won't bother to go see it. It makes sense that this would be the cycle:

Stage One: I don't want to watch The Interview.

Stage Two: I can't watch The Interview?

Stage Three: I must watch The Interview!

Stage Four: Oh, I can watch The Interview now?

Stage Five: I don't want to watch The Interview.

Browns at Ravens. The worst part about this NFL season, now that it's almost over: That we didn't get one last cameo appearance from Rex Grossman, who turned down the Browns' offer to join the team for this final game. I miss the Sex Cannon. I'd have liked to have been able to say goodbye one last time. Jaguars at Texans. Before I decided to once again spend 1,600 words sniffing my own rectum, I was originally going to start this column with a thought project: Who is the next Bill Cosby? I don't mean ""who will start systematically drugging and assaulting women over a 40 year period,"" because we all know that's obviously going to be Jonah Hill. I mean: What's the story that media sort of glossed over at some point in last 20 years, only to have it explode on social media when the person involved attempts to promote something? After all, for all the talk about Hannibal Buress, the first time I'd seen Cosby's crimes brought up this year was Tom Scocca's ""knock knock oh yeah Bill Cosby is a rapist"" post back in February. That's all it takes, you know. Just a little push. So what are we going to remember that we didn't investigate next? Seriously, I'd love to hear your suggestions. I'll get a reporter right on it, unless of course I really admire the work of the person who committed all those crimes.

Rams at Seahawks. After my Buzzsaw That Is The Arizona Cardinals won that hideous Thursday night game two weeks ago, SB Nation's Matt Ufford, a Seahawks fan, taunted me on Twitter:@williamfleitch excited for your garbage team to be embarrassed in primetime games and the playoffs over the next month. I've known Matt a long time — here we are flanking a nearly dead Drew Magary in 2007 — so I was eager to slap him back. But my Twitter revenge would be a dish best served cold! So I decided not to respond in the moment, instead waiting until my Buzzsaw had shocked the world Sunday night by beating Seattle and clinching home field throughout the NFC (and NFL) playoffs. I was all set to go: ""@mattufford #garbageteam"" would ROCK MATT'S WORLD as he watched his Seahawks fall to the mighty Buzzsaw! That would show Matt! That'll learn him to insult my favorite football franchise over social media! YOU MESS WITH THE BULL YOU GET THE HORNS.

Cardinals at 49ers. You know, I didn't end up sending any Tweets Ufford's way."
4,"Decades before the rise of social media, polarisation plagued discussions about language. By and large, it still does. Everyone who cares about the topic is officially required to take one of two stances. Either you smugly preen about the mistakes you find abhorrent – this makes you a so-called prescriptivist – or you show off your knowledge of language change, and poke holes in the prescriptivists’ facts – this makes you a descriptivist. Group membership is mandatory, and the two are mutually exclusive.

But it doesn’t have to be this way. I have two roles at my workplace: I am an editor and a language columnist. These two jobs more or less require me to be both a prescriptivist and a descriptivist. When people file me copy that has mistakes of grammar or mechanics, I fix them (as well as applying The Economist’s house style). But when it comes time to write my column, I study the weird mess of real language; rather than being a scold about this or that mistake, I try to teach myself (and so the reader) something new. Is this a split personality, or can the two be reconciled into a coherent philosophy? I believe they can.

Language changes all the time. Some changes really are chaotic, and disruptive. Take decimate, a prescriptivist shibboleth. It comes from the old Roman practice of punishing a mutinous legion by killing every 10th soldier (hence that deci­- root). Now we don’t often need a word for destroying exactly a 10th of something – this is the ‘etymological fallacy’, the idea that a word must mean exactly what its component roots indicate. But it is useful to have a word that means to destroy a sizeable proportion of something. Yet many people have extended the meaning of decimate until now it means something approaching ‘to wipe out utterly’.

Descriptivists – that is, virtually all academic linguists – will point out that semantic creep is how languages work. It’s just something words do: look up virtually any nontechnical word in the great historical Oxford English Dictionary (OED), which lists a word’s senses in historical order. You’ll see things such as the extension of decimate happening again and again and again. Words won’t sit still. The prescriptivist position, offered one linguist, is like taking a snapshot of the surface of the ocean and insisting that’s how ocean surfaces must look.

Be that as it may, retort prescriptivists, but that doesn’t make it any less annoying. Decimate doesn’t have a good synonym in its traditional meaning (to destroy a portion of), and it has lots of company in its new meaning: destroy, annihilate, devastate and so on. If decimate eventually settles on this latter meaning, we lose a unique word and gain nothing. People who use it the old way and people who use it the new way can also confuse each other.

Or take literally, on which I am a traditionalist. It is a delight to be able to use a good literally: when my son fell off a horse on a recent holiday, I was able to reassure my mother that ‘He literally got right back in the saddle,’ and this pleased me no end. So when people use literally to say, for example, We literally walked a million miles, I sigh a little sigh. I know that James Joyce, Vladimir Nabokov and many others used a figurative literally, but as a mere intensifier it’s not particularly useful or lovely, and it is particularly useful and lovely in the traditional sense, where it has no good substitute.

So I do believe that when change happens in a language it can do harm. Not the end of the world, but harm.

There is another fact to bear in mind: no language has fallen apart from lack of care. It is just not something that happens – literally. Prescriptivists cannot point to a single language that became unusable or inexpressive as a result of people’s failure to uphold traditional vocabulary and grammar. Every language existing today is fantastically expressive. It would be a miracle, except that it is utterly commonplace, a fact shared not only by all languages but by all the humans who use them.

How can this be? Why does change of the decimate variety not add up to chaos? If one such ‘error’ is bad, and these kinds of things are happening all the time, how do things manage to hold together?

The answer is that language is a system. Sounds, words and grammar do not exist in isolation: each of these three levels of language constitutes a system in itself. And, extraordinarily, these systems change as systems. If one change threatens disruption, another change compensates, so that the new system, though different from the old, is still an efficient, expressive and useful whole.

Begin with sounds. Every language has a characteristic inventory of contrasting sounds, called phonemes. Beet and bit have different vowels; these are two phonemes in English. Italian has only one, which is why Italians tend to make homophones of sheet and shit.

There is something odd about the vowels of English. Have you ever noticed that every language in Europe seems to use the letter A the same way? From latte to lager to tapas, Italian, German and Spanish all seem to use it for the ah sound. And at some level, this seems natural; if you learn frango is ‘chicken’ in Portuguese, you will probably know to pronounce it with an ah, not an ay. How, then, did English get A to sound like it does in plate, name, face and so on?

Look around the other ‘long’ vowels in English, and they seem out of whack in similar ways. The letter I has an ee sound from Nice to Nizhni Novgorod; why does it have the sound it does in English write and ride? And why do two Os yield the sound they do in boot and food?

Nobody in a 15th-century tavern (men carried knives back then) wants to confuse meet, meat and mate

The answer is the Great Vowel Shift. From the middle English period and continuing into the early modern era, the entire set of English long vowels underwent a radical disruption. Meet used to be pronounced a bit like modern mate. Boot used to sound like boat. (But both vowels were monophthongs, not diphthongs; the modern long A is really pronounced like ay-ee said quickly, but the vowel in medieval meet was a pure single vowel.)

During the Great Vowel Shift, ee and oo started to move towards the sounds they have today. Nobody knows why. It’s likely that some people noticed at the time and groused about it. In any case, there was really a problem: now ee was too close to the vowel in time, which in that era was pronounced tee-muh. And oo was too close to the vowel in house, which was then pronounced hoose.

Speakers didn’t passively accept the confusion. What happened next shows the genius of what economists call spontaneous order. In response to their new pushy neighbours in the vowel space, the vowels in time and house started to change, too, becoming something like tuh-eem and huh-oos. Other changes prompted yet more changes, too: the vowel in mate – then pronounced mah-tuh – moved towards the sound of the modern vowel in cat. That made it a little too close to meat, which was pronounced like a drawn-out version of the modern met. So the vowel in meat changed too.

Throughout the system, vowels were on the move. Nobody in a 15th-century tavern (men carried knives back then) wants to confuse meet, meat and mate. So they responded to a potentially damaging change by changing something else. A few vowels ended up merging. So meet and meat became homophones. But mostly the system just settled down with each vowel in a new place. It was the Great Vowel Shift, not the Great Vowel Pile-Up.

Such shifts are common enough that they have earned a name: ‘chain shifts’. These are what happens when one change prompts another, which in turn prompts yet another, and so on, until the language arrives at a new equilibrium. There is a chain shift underway now: the Northern Cities Shift, noticed and described in the cities around the Great Lakes of North America by William Labov, the pioneer of sociolinguistics. There is also a California Shift. In other words, these things happen. The local, individual change is chaotic and random, but the system responds to keep things from coming to harm.

What about words? There are only so many vowels in a language, but many thousands of words. So changes in the meanings of words might not be as orderly as the chain shifts seen in the Great Vowel Shift and others. Nonetheless, despite potential harm done by an individual word’s change in meaning, cultures tend to have all the words they need for all the things they want to talk about.

In researching Samuel Johnson’s dictionary for my new book, Talk on the Wild Side (2018), I made a startling find. Johnson, in describing his plan for the dictionary to the Earl of Chesterfield in 1747, wrote that

[B]uxom, which means only obedient, is now made, in familiar phrases, to stand for wanton; because in an ancient form of marriage, before the Reformation, the bride promised complaisance and obedience, in these terms: ‘I will be bonair and buxom in bed and at board.’
When most people think of buxom today, neither ‘obedient’ nor ‘wanton’ is what comes to mind (To my wife: this is why a Google Images search for buxom is in my search history, I promise.)

Turning to the OED, I found that buxom had come from a medieval word buhsam, cognate to the modern German biegsam, or ‘bendable’. From physical to metaphorical (the natural extension), it came to mean ‘pliable’ of a person, or – as Johnson put it – obedient. Then buxom kept on moving: a short hop from ‘obedient’ to ‘amiable’, and then another one to ‘lively, gay’. (William Shakespeare describes a soldier of ‘buxom valour’ in Henry V.) From there, it is another short jump to ‘healthy, vigorous’, which seems to have been the current meaning around Johnson’s time. From ‘good health’ it was another logical extension to physical plumpness, then to plumpness specifically on a woman, to big-breasted.

The leap from ‘obedient’ to ‘busty’ seems extraordinary until we look at it step by step. Nice used to mean ‘foolish’. Silly used to mean ‘holy’. Assassin is from the plural of the Arabic word for ‘hashish(-eater)’, and magazine from the Arabic word for a storehouse. This is just what words do. Prestigious used to be pejorative, meaning glittery but not substantive. These kinds of changes are common.

I don’t know how we did without hangry so long in English, because I spent about a third of every day hangry

Two paragraphs ago, I used the words ‘leap’ and ‘jump’. But we see the ‘leaps’ only when lexicographers, looking back, chop up a word’s history into meanings for their dictionaries. Words change meaning gradually, as a small number of speakers use them in a new way, and they in turn cause others to do so. This is how words can change meaning so totally and utterly; mostly, they do so in steps too small to notice.

Again, no chaos results. Every time buxom changed meaning, it could have theoretically left a hole in the lexicon for the meaning it had left behind. But in each case, another word filled its place: in fact, the ones I have used above (pliable, obedient, amiable, lively, gay, healthy, plump and so on). For useful concepts, it seems, the lexicon abhors a vacuum. (I don’t know how we did without hangry so long in English, because I spent about a third of every day hangry. But sure enough, someone coined it.)

There are several predictable ways that words change meaning. Some people insist that nauseous means only ‘causing nausea’. But going from cause to experiencer is a common semantic shift, just as many words can be used in both active and agentless constructions (consider I broke the dishwasher and The dishwasher broke). Yet true confusion is rare. For nauseous’s old meaning we have nauseating.

Words also weaken with frequent use: The Lego Movie (2014) was on to something with its song ‘Everything Is Awesome’, because Americans really do use this word rather a lot. Once powerful, it can now be used for anything even slightly good, as in This burrito is awesome. It can even be near-meaningless, as in Steven Pinker’s lovely example: ‘If you could pass the guacamole, that would be awesome.’

But do we really lack ways of communicating that we’re impressed by something? No language does, and English-speakers are spoiled for choice from the likes of incredible, fantastic, stupendous and brilliant. (All of which have changed from their etymological meanings of ‘unbelievable’, ‘like a fantasy’, ‘inducing stupor’ and ‘shiny, reflective’, by the way.) When those get overused (and all are in danger of that), people coin new ones still: sick, amazeballs, kick-ass.

The thousands of words in the language are a swirling mass constantly on the move. Again, when one piece moves, threatening a gap or an overlap, something else moves too. The individual, short-term change is random; the overall, long-term change is systemic.

At the level of grammar, change might seem the most unsettling, threatening a deeper kind of harm than a simple mispronunciation or new use for an old word. Take the long-term decline of whom, which signals that something in a question or relative clause is an object (direct or indirect), as in That’s the man whom I saw. Most people today would either say That’s the man who I saw or just That’s the man I saw.

What word is the subject in a clause, and what is the object, is a deeply important fact. And yet, precisely because this is so, even radical grammatical change leaves this distinction intact. Readers of Beowulf are in no doubt that virtually every word in that epic poem is vastly different from its modern counterpart. What those who can’t read Old English might not realise is how different the grammar is. English was a language like Russian or Latin: it had case endings everywhere: on nouns, adjectives and determiners (words such as the and a). In other words, they all behaved like who/whom/whose does (there was even a fourth case).

Today, just six words (I, he, she, we, they and who) change form when they are direct or indirect objects (me, him, her, us, them and whom). In a longer view, modern Anglophones speak godawful, brokendown Anglo-Saxon, lacking all the communicative power that those endings provided. How, one can imagine Alfred the Great asking, do English-speakers know what is the subject of a sentence and what are the objects without those crucial case endings?

The answer is boring: word order. English is a subject-verb-object language. In I love her, case is evident by the form of I (a subject, in the nominative case) and her (a direct object, in the objective case). But the meaning of Steve loves Sally is just as clear, despite the lack of case endings. Subject-verb-object order can be violated in special circumstances (Her I love the most) but it is expected; and that expectation, shared by all native speakers, does the work that the case endings once did.

To my six-year-old, everything is epic, which strikes my ear as awesome must have done my parents’

Why did the case endings disappear? We don’t know, but it was probably sped up as a result of two waves of conquest: adult Vikings and Normans coming to Britain, and learning Anglo-Saxon imperfectly. Then as now, things such as fiddly inflections are hard for adults to learn in a foreign language. Many adult learners would have neglected all those endings and relied on word order, raising children who heard their parents’ slightly stripped-down version. The children would then have used the endings less than earlier generations, until they disappeared entirely.

Once again, the grammar responded as a system. No civilisation can afford to leave the distinction between subjects and objects to guesswork. Word order was relatively flexible in the Anglo-Saxon period. Then the loss of case endings fixed it in more rigid form. The gradual disappearance of case signalling resulted in a potential loss of information, but the solidification of word order made up for it.

We now have a framework in which both the prescriptivists and the descriptivists can have their say. Sound changes can be seen as wrong, understandably, by people who learned an older pronunciation: to my ear, nucular sounds uneducated and expresso is just wrong. But in the long run, sound systems make up for any confusion in a delicate dance of changes that makes sure the language’s necessary distinctions remain. Word meanings change, by both type (a change in meaning) and by force (a change in how powerful a word is). To my six-year-old, everything is epic, which strikes my ear the way awesome must have done to my parents. A lunch just cannot be epic. But when epic is exhausted, his kids will press something else into service – or coin something new.

Even the deepest-seeming change – to the grammar – never destroys the language system. Some distinctions can disappear: classical Arabic has singular, dual and plural number; the modern dialects mostly use just singular and plural, like English. Latin was full of cases; its daughter languages – French, Spanish and so on – lack them, but their speakers get on with life just the same. Sometimes languages get more complex: the Romance languages also pressed freestanding Latin words into service until they wore down and became mere endings on verbs. That turned out OK, too.

Spontaneous order doesn’t sit well with people. We are all tempted to think that complex systems need management, a benign but firm hand. But just as market economies turn out better than command economies, languages are too complex, and used by too many people, to submit to command management. Individual decisions can be bad ones, and merit correction, but we can be optimistic that, in the long run, change is inevitable and it will turn out all right. Broadly trusting the distributed intelligence of your fellow humans to keep things in order can be hard to do, but it’s the only way to go. Language is self-regulating. It’s a genius system – with no genius."
5,"The World’s Most Efficient Languages
How much do you really need to say to put a sentence together?

By John McWhorter. JUNE 29, 2016
SHARE
Just as fish presumably don’t know they’re wet, many English speakers don’t know that the way their language works is just one of endless ways it could have come out. It’s easy to think that what one’s native language puts words to, and how, reflects the fundamentals of reality.

MAKE YOUR INBOX MORE INTERESTING

Each weekday evening, get an overview of the day’s biggest news, along with fascinating ideas, images, and people.

Email Address (required)
Enter your email
Sign Up
THANKS FOR SIGNING UP!

But languages are strikingly different in the level of detail they require a speaker to provide in order to put a sentence together. In English, for example, here’s a simple sentence that comes to my mind for rather specific reasons related to having small children: “The father said ‘Come here!’” This statement specifies that there is a father, that he conducted the action of speaking in the past, and that he indicated the child should approach him at the location “here.” What else would a language need to do?

Well, for a German speaker, more. In “Der Vater sagte ‘Komm her!’”, although it just seems like a variation on the English sentence, more is happening. “Der,” the word for “the,” is a choice among other possibilities: It’s the one used for masculine nouns only. If the sentence were about a mother, it would have to use the feminine die, or if about a girl, the neuter das (for reasons unnecessary to broach here!). The word for “said,” sagte, is marked with a suffix for the third-person singular; if it were “you said,” then it would be sagtest—in English, those forms don’t vary in the past tense. Then, her for “here” means “to here”: In German one must become what feels to an English speaker rather Shakespearean and say “hither” when that’s what is meant. “Here” in the sense of just sitting “here” is a different word, hier.

RECOMMENDED READING
The World’s Most Musical Languages
bw portriat digitally breaking up
One by One, My Friends Were Sent to the Camps
TAHIR HAMUT IZGIL
England's national men's soccer team blurred in rainbow colors
What Euro 2020 Has Revealed About Englishness
YASMEEN SERHAN
This German sentence, then, requires you to pay more attention to the genders of people and things, to whether it’s me, you, her, him, us, y’all, or them driving the action. It also requires specifying not just where someone is but whether that person is moving closer or farther away. German is, overall, busier than English, and yet Germans feel their way of putting things is as normal as English speakers feel their way is.

Other languages occupy still other places on the linguistic axis of “busyness,” from prolix to laconic, and it’s surprising what a language can do without. In Mandarin Chinese, a way of saying “The father said ‘Come here!’” is “Fùqīn shuō ‘Guò lái zhè lǐ!’” Just as in English, there is no marker for the father’s gender, nor does the form of the word shuō for “said” indicate whether the speaker is me, you, or him. The word for “here,” zhè lǐ, can mean either “right here” or “to here,” just like in English. But Mandarin is even more telegraphic. There is no definite article like “the.” The word for “said” lacks not only a suffix for person, but is also not marked for tense; it just means “say.” It is assumed that context will indicate that this event happened in the past. Much of learning Mandarin involves getting a sense of how much one can not say in an acceptable sentence.


Moreover, anyone who has sampled Chinese, or Persian, or Finnish, knows that a language can get along just fine with the same word for “he” and “she.”* And whereas Mandarin can mark tense but often doesn’t, in the Maybrat language of New Guinea, there’s pretty much no way to mark it at all—context takes care of it and no one bats an eye.

If there were a prize for the busiest language, then a language like Kabardian, also known as Circassian and spoken in the Caucasus, would win. In the simple sentence “The men saw me,” the word for “saw” is sǝq’ayǝƛaaɣwǝaɣhaś (pronounced roughly “suck-a-LAGH-a-HESH”). This seems like a majestic monster of a word, and yet despite its air of “supercalifragilisticexpealidocious,” the word for “saw” is every bit as ordinary for Kabardian-speakers as English-speakers’ “saw” is for them. It’s just that Kabardian-speakers have to pack so much more into their version. In sǝq’ayǝƛaaɣwǝaɣhaś, other than the part meaning “see,” there is a bit that reiterates that it’s me who was seen, even though the sentence would include a separate word for “me” elsewhere. Then there are other bits that show that the seeing was most significant to “me” rather than to the men or anyone else; that the seeing was done by more than one person (despite the sentence spelling out elsewhere that it was plural “men” who did the seeing); that this event did not happen in the present; that on top of this, the event happened specifically in the past rather than the future; and finally a bit indicating that the speaker really means what he’s saying.

Is a Kabardian shopkeeper in the Caucasus more exquisitely attuned to the nuances of experience than a Riau Indonesian-speaking fisherman in Sumatra?
The prize for most economical language could go to certain colloquial dialects of Indonesian that are rarely written but represent the daily reality of Indonesian in millions of mouths. For example, in the Riau dialect spoken in Sumatra, ayam means chicken and makan means eat, but “Ayam makan” doesn’t mean only “The chicken is eating.” Depending on context, “Ayam makan” can mean the “chickens are eating,” “a chicken is eating,” “the chicken is eating,” “the chicken will be eating,” “the chicken eats,” “the chicken has eaten,” “someone is eating the chicken,” “someone is eating for the chicken,” “someone is eating with the chicken,” “the chicken that is eating,” “where the chicken is eating,” and “when the chicken is eating.” If chickens and eating are à propos, the assumption is that everybody in the conversation knows what’s what. Thus for a wide variety of situations the equivalent of “chicken eat” will do—and does.

So does the contrast between Riau Indonesian’s “chicken eat” and Kabardian’s “they saw me and it affected me, not now, and I really mean it” mean that each language gives its speakers a different way of looking at the world? It’s an intriguing idea, first formulated by anthropologist and linguist Edward Sapir and amateur linguist (and fire inspector!) Benjamin Whorf. If it were correct, an English-speaker would generally think about the past more than a Chinese-speaker would, while Germans would think more about movement than Americans or Brits.

Experiments have shown that this is often true to a faint, flickering degree a psychologist can detect in the artifice of experimental conditions. But does this mean a different way of experiencing life? Is a Kabardian shopkeeper in the Caucasus more exquisitely attuned to the nuances of experience than a Riau Indonesian-speaking fisherman in Sumatra? If that Kabardian shopkeeper’s jam-packed verbs mean that he vibrates in tune to the jots and tittles of life, then doesn’t one have to say that the Riau Indonesian speaker, whose grammar directs his attention to so few details, is something of a limp string on the guitar? We would run into similarly hopeless comparisons around the world. The Zulu speaker would be hypervigilant given the complexities of his language, the Samoan speaker inattentive given the less obsessively complicated nature of hers.

If thought and culture aren’t why some languages pile it on while others take it light, then what is the reason? Part of the answer is unsatisfying but powerful: chance. Time and repetition wear words out, and what wears away is often a nugget of meaning. This happens in some languages more than others. Think of the French song “Alouette, gentille alouette …” (“lark, nice lark”) in which one sings “ahh-loo-eh-tuh.” In running speech the word has long been pronounced just “ah-loo-ett” with no -uh at the end. That –uh in the song today is a leftover from the way the word actually was once pronounced normally, and it indicated the word’s feminine gender to the listener. Today, beyond marginal contexts like that song, only the final e in the spelling of alouette indicates its gender; hearing it in a sentence we’d have to rely on the definite article la alone to know that the word is feminine.

Even if languages’ differences in busyness can’t be taken as windows on psychological alertness, the differences remain awesome.
In a language where final sounds take the accent, such sounds tend to hold on longer because they are so loud and clear—you’re less likely to mumble it and people listening are more likely to hear it. In Hebrew, “Thank you very much,” is “Toda raba,” pronounced “toe-DAH rah-BAH.” The sounds at the end of the word mark gender in Hebrew, too, and they aren’t going anywhere anytime soon because they are enunciated with force.

When a language seems especially telegraphic, usually another factor has come into play: Enough adults learned it at a certain stage in its history that, given the difficulty of learning a new language after childhood, it became a kind of stripped-down “schoolroom” version of itself. Because all languages, are, to some extent, busier than they need to be, this streamlining leaves the language thoroughly complex and nuanced, just lighter on the bric-a-brac that so many languages pant under. Even today, Indonesian is a first language to only one in four of its speakers; the language has been used for many centuries as a lingua franca in a vast region, imposed on speakers of several hundred languages. This means that while other languages can be like overgrown lawns, Indonesian’s grammar has been regularly mowed, such that especially the colloquial forms are tidier. Lots of adult learning over long periods of time is also why, for example, the colloquial forms of Arabic like Egyptian and Moroccan are somewhat less elaborated than Modern Standard Arabic—they were imposed on new people as Islam spread after the seventh century.

In contrast, one cannot help suspecting that not too many adults have been tackling the likes of sǝq’ayǝƛaaɣwǝaɣhaś. Kabardian has been left to its own devices, and my, has it hoarded a lot of them. This is, as languages go, normal, even if Kabardian is rather extreme. By contrast, only a few languages have been taken up as vehicles of empire and imposed on millions of unsuspecting and underqualified adults. Long-dominant Mandarin, then, is less “busy” than Cantonese and Taiwanese, which have been imposed on fewer people. English came out the way it did because Vikings, who in the first millennium forged something of an empire of their own in northern and western Europe, imposed themselves on the Old English of the people they invaded and, as it were, mowed it. German, meanwhile, stayed “normal.”


Even if languages’ differences in busyness can’t be taken as windows on psychological alertness, the differences remain awesome. In a Native American language of California called Atsugewi (now extinct), if a tree was burned and we found the ashes in a creek afterward, we would have said that soot w’oqhputíc’ta into the creek. W’oqhputíc’ta is a conglomeration of bits that mean “it moved like dirt, in a falling fashion, into liquid, and for real.” In English, we would just say “flowed.”

* This article originally stated that Japanese had the same word for “he” and “she.” We regret the error.

John McWhorter is a contributing writer at The Atlantic. He teaches linguistics at Columbia University, hosts the podcast Lexicon Valley, and is the author of the upcoming Nine Nasty Words: English in the Gutter Then, Now and Always."
6,"In our experience the past is the past and the future is the future, but sometimes the two can cross over

Share on Facebook
Share on Twitter
Share on Reddit
Share on StumbleUpon
Share on Google+
Share by Email
By Philip Ball
11 July 2016
Science has a habit of asking stupid questions. Stupid, that is, by the standards of common sense. But time and time again we have found that common sense is a poor guide to what really goes on in the world.

So if your response to the question ""Why does time always go forwards, not backwards?"" is that this is a daft thing to ask, just be patient.

Surely we can just say that the future does not affect the past because (duh!) it has not happened yet? Not really, for the question of where time's arrow comes from is more subtle and complicated than it seems.

What's more, that statement might not even be true. Some scientists and philosophers think the future might indeed affect the past – although we would only find out when the future arrives. And it may be able to due to an emergent property of quantum mechanics.To all intents and purposes, time seems to have a direction.

Our everyday experience insists that things only happen one way. Cups of coffee always get colder, never warmer, when left to stand. If they are knocked to the floor, the cup becomes shards and the coffee goes everywhere, but shards and splashes never spontaneously reassemble into a cup of coffee.

Yet none of this one-way flow of time is apparent when you look at the fundamental laws of physics: the laws, say, that describe how atoms bounce off each other.

Those laws of motion make no distinction about the direction of time. If you watched a video of two billiard balls colliding and bouncing away, you would be unable to tell if it was being run forwards or backwards.

The same time symmetry is found in the equations of quantum mechanics, which govern the behaviour of tiny things like atoms. So where does time's arrow come into the picture?

There is a long-standing answer to this, which says that the arrow only enters once you start thinking about lots and lots of particles.
 The process of two atoms colliding looks perfectly reversible. But when there are lots of atoms, their interactions lead inevitably to an increase in randomness – simply because that is by far the most likely thing to happen.

Say you have a gas of nitrogen molecules in one half of a box and oxygen molecules in the other, separated by a partition. If you take away the partition, the random movements of the molecules will quickly mix the two gases completely.

There is nothing in the laws of physics to prevent the reverse. A mixture of the two gases could spontaneously separate into oxygen in one half of the box and nitrogen in the other, just by chance.

But this is never likely to happen in practice, because the chance of all those billions of molecules just happening to move this way in concert is tiny. You would have to wait for longer than the age of the Universe for spontaneous separation to occur.

This inexorable growth of randomness is enshrined in the second law of thermodynamics. The amount of randomness is measured by a quantity called entropy, and the second law says that, in any process, the total entropy of the Universe always increases.

Of course, we can decrease the entropy of a group of molecules, say by sorting them one from another. But doing that work inevitably releases heat, which creates more disorder – more entropy – somewhere else. Ordinarily, there is no getting around this.
However, the entropic arrow of time gets less well-defined at smaller scales. For example, the chances of three oxygen and two nitrogen molecules briefly ""un-mixing"" are pretty good.

This was illustrated by a 2015 study. Researchers studying single molecules found evidence that the growth of entropy is a good measure of how far the system was from being reversible in time.

This argument about entropy, which was worked out in the late 19th Century by the Austrian scientist Ludwig Boltzmann, is often seen as a complete and satisfying answer to the puzzle of time's arrow.

But it turns out the Universe holds deeper secrets. When you start looking at very small things, Boltzmann's neat story gets increasingly muddled.

In Boltzmann's picture, it takes a while for the arrow of time to find its direction. In the tiny fractions of a second after the partition between the two gases is removed, before any of the molecules have really moved anywhere, there is nothing to show which direction of time is forwards.

Entropy increases when collisions between atoms even out their energies, as for example when the heat of hot coffee spreads out into the surrounding air. This process, which washes away reservoirs of energy, is called dissipation.

Until dissipation starts to happen, a process looks much the same backwards or forwards in time. It does not really have a thermodynamic arrow.

But there is a one-way process in quantum mechanics that happens much faster. It is called decoherence.
At the quantum scale, particles behave as if they were waves. This has peculiar consequences.

For example, if you shoot individual electrons or whole atoms through two closely spaced slits in a screen , they will interfere with each other as if they were waves. But this does not happen with ordinary-sized objects. If you throw two coffee cups through two open windows, they do not interfere with each other.

Decoherence explains why objects on the everyday scale of coffee cups do not show the wave-like behaviour of quantum objects.

It arises because quantum particles can be coordinated in their quantum waviness, but if there are lots of them – like the countless atoms in a coffee cup – they rapidly lose any coordination. This means the object they constitute cannot show quantum behaviour.

Decoherence happens because of interactions between the objects and their environment: for example, the impact of air molecules on the cup. Quantum theory shows that these interactions rapidly cause a large object's quantumness to ""leak"" into its environment.

This means the object takes on unique characteristics. Quantum theory tells us that objects can show one of many possible properties when they are measured, but in our everyday world objects only have a single well-defined position, speed and so on. Decoherence is thought to be how this ""choice"" is enforced.
Quantum decoherence is incredibly fast, because interactions between particles are extremely efficient at dispersing quantum coherence.

For a dust grain one-thousandth of a centimetre across, floating in air, the collisions of other air molecules will destroy any quantum behaviour in around 0.0000000000000000000000000000001 seconds. That is a trillionth of the time it takes for light to cross the face of a single hydrogen atom.

This is much faster than the time it takes for heat in a dust grain to get redistributed in the environment. In other words, decoherence is faster than dissipation – and it seems to only work one way. That means decoherence reveals the arrow of time faster than dissipation.

This implies that the arrow of time really comes from quantum mechanics, not thermodynamics as Boltzmann thought.In a sense it has to, because everything is made of atoms, and quantum mechanics is the right theory to use for atoms. ""The thermodynamic arrow of time must emerge from the quantum one,"" says George Ellis of the University of Cape Town in South Africa.

Yet in the end, the quantum and thermodynamic explanations amount to the same thing: the scrambling of information.

It is easy to see that the mixing of two types of gas molecule is a kind of scrambling, a destruction of orderliness.

But decoherence involves scrambling too: of the coordination between the ""waves"" that describe quantum objects. In effect, decoherence comes from the way interactions with atoms, photons and so on in an object's environment carry away information about the object and scatter it around. This is, in fact, a quantum version of entropy.

In both the classical and the quantum cases, then, time's arrow comes from a loss of information.This offers a better way to think about time's arrow. It points in the direction in which information is lost and can never be retrieved.

A process is only truly irreversible when the information about the change is lost, so that you cannot retrace your steps. If you could keep track of the movement of every single particle, then in principle you could reverse it and get back to exactly where you started. But once you have lost some of that information, there is no return.

""The loss of information is a key aspect,"" says Ellis. ""At the macroscopic scale this gives the second law.""

It is still not entirely clear when, in the quantum world, the information is truly lost.

Some researchers think that decoherence alone is enough. But others say that the information, although smeared and dispersed in the environment, is still recoverable in principle. They think an additional, rather mysterious process called ""collapse of the wave function"" – in which the quantum waviness is irreversibly lost – takes place. Only then, they say, does the arrow of time point unambiguously in one direction.

In either case, in quantum physics we can only really say that an event has happened if we have lost the option of making it ""unhappen"".

The arrow of time seems to reflect a process of the Universe ""committing itself"" to something, rather than hedging its bets by allowing for many different outcomes. It is this ""crystallising"" of the classical present from the quantum past, says Ellis, that produces a direction in time.

This idea fits neatly with one of the most famous thought experiments in physics.
The second law of thermodynamics says that things tend to become more random, but only because randomness is so likely. In the late nineteenth century, the Scottish physicist James Clerk Maxwell came up with what looked like a way to get around this.

Maxwell imagined a tiny intelligent being that could observe the random motions of molecules. This ""demon"" could un-mix two gases, by opening and closing a door at just the right moments.

Maxwell's demon seems to violate the second law, but in fact it cannot. The reason is that the demon has to accumulate information in its brain as it observes the molecular motions. To keep this up, it has to delete the older information, and that increases the entropy.

It is the act of erasure, of forgetting, that guarantees the thermodynamic arrow of time. Once again, the key thing is loss of information.

However, the quantum-mechanical picture of time's arrow leads to something deeply peculiar. In some experiments, it looks as though influences can work backwards in time. The future can affect the past.
Take the double-slit experiment, in which a quantum particle such as a photon of light is fired at two narrow slits in a screen.

Suppose we do not measure which way the particle went, and so cannot tell which slit it went through. In this case, we see an interference pattern – a series of light and dark bands – when the particles emerge on the far side.

This reflects the wave-like character of quantum particles, because interference is a wave property. The interference even persists when the particles pass through the slits one at a time, which only ""makes sense"" intuitively if we imagine each particle passing, wave-like, through both slits at once.

However, now suppose we place a detector by the slits to reveal which one the particle passes through. In this case the interference pattern disappears, and the particles act more like sand grains being fired through holes. Measuring a particle's path destroys its waviness.

Here is the really strange thing. We can set up the experiment so that we only detect which slit a particle passed through after it has done so. And yet we still see no interference.

How does the particle ""know"" that it is going to be detected after passing through the screen, so that when it reaches the slits it ""knows"" whether to go through both slits or just one? How can the later measurement seem to affect the earlier behaviour?

This effect is called ""retrocausality"", and it seems to imply that the arrow of time is not as strictly one-way as it seems. But does it really?Most physicists think that retrocausality in these delayed-choice experiments is an illusion created by the counterintuitive nature of quantum mechanics.

Detecting a particle ""after"" it has passed through the slits does not really influence the path it takes, they say. That is just the way we are forced to imagine what is happening when we try to apply our classical intuition to quantum events.

""Post-selection is like a parlour trick that makes it seem like there is backwards causation where there actually is none,"" says Todd Brun of the University of Southern California. ""It's like the guy who shoots at the side of a barn and then goes and draws a target around the bullet hole.""

However, others say that backwards effects in time are a perfectly valid way of interpreting such processes.

According to Ellis, we can regard retrocausality as a kind of fuzziness in the ""crystallisation of the present"". ""Quantum physics appears to allow some degree of influence of the present on the past, as indicated by delayed-choice experiments,"" he says.

Ellis has argued that the past is not always fully defined at any instant. It is like a block of ice that contains little blobs of water that have not yet crystallized.

Even though the broad outline of events at a particular instant has been decided, some of the fine details remain fluid until a later time. Then, when this ""fixing"" of the details happens, it looks like they have retrospective consequences.

It is hard to find the right words to describe this situation.Certainly we should not imagine a ""force"" reaching back through time and altering earlier events. Instead, those things are slow to acquire the true status of events – of stuff that has actually ""happened"" – at all.

Alternatively, perhaps those past events really did happen at the time, but the laws of quantum mechanics forbid us from seeing them until later. ""If the future detector choice causes the particle to behave a certain way in the past, one should consider the past behavior 'real' when it originally happened,"" says Ken Wharton of San José State University in California.

Quantum retrocausality crops up quite naturally if, instead of trying to force an arrow of time onto quantum theory, we simply let quantum mechanics work equally well in both directions in time. Wharton's colleague Huw Price of the University of Cambridge in the UK has argued that such a theory really would allow backwards causation.Still, Wharton admits that such retrocausal theories are speculative.Only a handful of physicists and philosophers have embraced retrocausality. Most consider backwards causality ""too high a price to swallow"", says Wharton.

But he feels that we only resist this idea because we are not used to seeing it in daily life.

""The view that the past does not depend on the future is largely anthropocentric,"" says Wharton. ""We should take apparent backwards causation more seriously than we usually do. Our intuition has been wrong before, and this time symmetry on quantum scales is a reason to think we could be wrong again.""

If time's arrow is not quite as one-way as it seems, that raises one last question: why do we perceive it as always pointing one way? Why should the ""psychological arrow of time"" be aligned with the physical ones?Again, it might sound like an odd question. If the laws of physics dictate an arrow of time, surely our perception of it should follow quite naturally? But that is not as obvious as it sounds.

Suppose you have some particles moving around in a box according to physical laws. If you know all the exact positions and speeds of the particles right now, you can predict the future exactly.

In principle, there is nothing in the future that could not be known in the present. Why could that not be experienced as a ""memory"" of the future; a little like a master chess player seeing well in advance how the game will inevitably end?

This question is usually swept under the carpet. ""Since biology rests on a foundation of chemistry, which rests in turn on foundations of quantum mechanics and thermodynamics, I think most people believe that the biological arrow of time is a consequence of the thermodynamic arrow,"" says Brun.

But this is not a foregone conclusion. Brun and his colleague Leonard Mlodinow at the California Institute of Technology in Pasadena have argued that the psychological and thermodynamic arrows of time are actually independent. They only coincide because of how our memories work. The first part of the argument was put forward by David Wolpert of the Santa Fe Institute in New Mexico. He said that, before you can remember something, you must initialize your memory in some standard starting state; like wiping your hard drive to make space for new data.

But as Maxwell's demon showed us, erasing information always increases entropy. This means initialization is an irreversible process, and so only works forward in time.

However, Mlodinow and Brun say that this argument is not quite complete. In principle, you can eliminate any need for erasure and initialization just by remembering everything. Then, recording information in the memory can be fully reversed, and has no arrow of time.

In this case, they argue that the psychological arrow of time is still preserved, by something they call ""generality"".The problem with remembering the future now is that there can always be ""surprise"" events that wreck the link between the system's state now and its state in the future. I can ""remember"" that my clock will show 11 o'clock in an hour's time – but if the battery runs out in ten minutes, my ""memory"" will be wrong.

A real memory cannot be contingent on the system behaving a certain way, Mlodinow and Brun say. It has to remain general, meaning it is true whatever happens. So even if memory is recorded in a reversible system, the requirement of generality imposes a directionality in time.

In this case, says Brun, the reason we cannot remember the future is not simply because it has not happened yet. Instead, it is because the ""memory"" would really only be a prediction, which might or might not be correct. And it is not a true memory unless it is correct.

In other words, a putative ""future memory"" is fine-tuned to a particular outcome, and must be readjusted if any slight change occurs between now and the appearance of the ""remembered"" future state. That means it is too fragile to count as a genuine memory.
Not everyone is convinced that Brun and Mlodinow have cracked the problem.

Some think their argument is circular. They had to put in the asymmetry of time by hand, making it possible for contingent events to intervene in the future but not the past – which some feel is a bit of a cheat.

All the same, simply by posing the question, Brun and Mlodinow have shown that the answer is not as obvious as we might suppose. Our perception of time may not have much to do with the actual passage of time.

What is clear is that the arrow of time, which seems like such a common-sense fact of life, is actually a profoundly tricky concept. The closer we look, the less we can be sure that the arrow is really always one-way.

Join over five million BBC Earth fans by liking us on Facebook, or follow us on Twitter and Instagram.

If you liked this story, sign up for the weekly bbc.com features newsletter called ""If You Only Read 6 Things This Week"". A handpicked selection of stories from BBC Future, Earth, Culture, Capital, Travel and Autos, delivered to your inbox every Friday."
7,"THE SOCIAL LIFE OF GENES
Your DNA is not a blueprint. Day by day, week by week, your genes are in a conversation with your surroundings. Your neighbors, your family, your feelings of loneliness: They don't just get under your skin, they get into the control rooms of your cells. Inside the new social science of genetics.
DAVID DOBBSUPDATED:JUN 14, 2017ORIGINAL:SEP 3, 2013. (Illustration: Jeremy Dimmock)

A few years ago, Gene Robinson, of Urbana, Illinois, asked some associates in southern Mexico to help him kidnap some 1,000 newborns. For their victims they chose bees. Half were European honeybees, Apis mellifera ligustica, the sweet-tempered kind most beekeepers raise. The other half were ligustica’s genetically close cousins, Apis mellifera scutellata, the African strain better known as killer bees. Though the two subspecies are nearly indistinguishable, the latter defend territory far more aggressively. Kick a European honeybee hive and perhaps a hundred bees will attack you. Kick a killer bee hive and you may suffer a thousand stings or more. Two thousand will kill you.

Working carefully, Robinson’s conspirators—researchers at Mexico’s National Center for Research in Animal Physiology, in the high resort town of Ixtapan de la Sal—jiggled loose the lids from two African hives and two European hives, pulled free a few honeycomb racks, plucked off about 250 of the youngest bees from each hive, and painted marks on the bees’ tiny backs. Then they switched each set of newborns into the hive of the other subspecies.


Robinson, back in his office at the University of Illinois at Urbana-Champaign’s Department of Entomology, did not fret about the bees’ safety. He knew that if you move bees to a new colony in their first day, the colony accepts them as its own. Nevertheless, Robinson did expect the bees would be changed by their adoptive homes: He expected the killer bees to take on the European bees’ moderate ways and the European bees to assume the killer bees’ more violent temperament. Robinson had discovered this in prior experiments. But he hadn’t yet figured out how it happened.

He suspected the answer lay in the bees’ genes. He didn’t expect the bees’ actual DNA to change: Random mutations aside, genes generally don’t change during an organism’s lifetime. Rather, he suspected the bees’ genes would behave differently in their new homes—wildly differently.

This notion was both reasonable and radical. Scientists have known for decades that genes can vary their level of activity, as if controlled by dimmer switches. Most cells in your body contain every one of your 22,000 or so genes. But in any given cell at any given time, only a tiny percentage of those genes is active, sending out chemical messages that affect the activity of the cell. This variable gene activity, called gene expression, is how your body does most of its work.


THE FISH UNDERWENT MASSIVE SURGES IN GENE EXPRESSION THAT IMMEDIATELY BLINGED UP HIS PEWTER COLORING WITH LURID RED AND BLUE STREAKS AND, IN A MATTER OF HOURS, CAUSED HIM TO GROW SOME 20 PERCENT. IT WAS AS IF JASON SCHWARTZMAN, COMING TO WORK ONE DAY TO LEARN THE BIG OFFICE STUD HAD QUIT, MORPHED INTO ARNOLD SCHWARZENEGGER BY CLOSE OF BUSINESS.
Sometimes these turns of the dimmer switch correspond to basic biological events, as when you develop tissues in the womb, enter puberty, or stop growing. At other times gene activity cranks up or spins down in response to changes in your environment. Thus certain genes switch on to fight infection or heal your wounds—or, running amok, give you cancer or burn your brain with fever. Changes in gene expression can make you thin, fat, or strikingly different from your supposedly identical twin. When it comes down to it, really, genes don’t make you who you are. Gene expression does. And gene expression varies depending on the life you live.

Every biologist accepts this. That was the safe, reasonable part of Robinson’s notion. Where he went out on a limb was in questioning the conventional wisdom that environment usually causes fairly limited changes in gene expression. It might sharply alter the activity of some genes, as happens in cancer or digestion. But in all but a few special cases, the thinking went, environment generally brightens or dims the activity of only a few genes at a time.

Robinson, however, suspected that environment could spin the dials on “big sectors of genes, right across the genome”—and that an individual’s social environment might exert a particularly powerful effect. Who you hung out with and how they behaved, in short, could dramatically affect which of your genes spoke up and which stayed quiet—and thus change who you were.

Robinson was already seeing this in his bees. The winter before, he had asked a new post-doc, Cédric Alaux, to look at the gene-expression patterns of honeybees that had been repeatedly exposed to a pheromone that signals alarm. (Any honeybee that detects a threat emits this pheromone. It happens to smell like bananas. Thus “it’s not a good idea,” says Alaux, “to eat a banana next to a bee hive.”)

To a bee, the pheromone makes a social statement: Friends, you are in danger. Robinson had long known that bees react to this cry by undergoing behavioral and neural changes: Their brains fire up and they literally fly into action. He also knew that repeated alarms make African bees more and more hostile. When Alaux looked at the gene-expression profiles of the bees exposed again and again to alarm pheromone, he and Robinson saw why: With repeated alarms, hundreds of genes—genes that previous studies had associated with aggression—grew progressively busier. The rise in gene expression neatly matched the rise in the aggressiveness of the bees’ response to threats.

Robinson had not expected that. “The pheromone just lit up the gene expression, and it kept leaving it higher.” The reason soon became apparent: Some of the genes affected were transcription factors—genes that regulate other genes. This created a cascading gene-expression response, with scores of genes responding.

This finding inspired Robinson’s kidnapping-and-cross-fostering study. Would moving baby bees to wildly different social environments reshape the curves of their gene-expression responses? Down in Ixtapan, Robinson’s collaborators suited up every five to 10 days, opened the hives, found about a dozen foster bees in each one, and sucked them up with a special vacuum. The vacuum shot them into a chamber chilled with liquid nitrogen. The intense cold instantly froze the bees’ every cell, preserving the state of their gene activity at that moment. At the end of six weeks, when the researchers had collected about 250 bees representing every stage of bee life, the team packed up the frozen bees and shipped them to Illinois.

There, Robinson’s staff removed the bees’ sesame-seed-size brains, ground them up, and ran them through a DNA microarray machine. This identified which genes were busy in a bee’s brain at the moment it met the bee-vac. When Robinson sorted his data by group—European bees raised in African hives, for instance, or African bees raised normally among their African kin—he could see how each group’s genes reacted to their lives.


Robinson organized the data for each group onto a grid of red and green color-coded squares: Each square represented a different gene, and its color represented the group’s average rate of gene expression. Red squares represented genes that were especially active in most of the bees in that group; the brighter the red, the more bees in which that gene had been busy. Green squares represented genes that were silent or underactive in most of the group. The printout of each group’s results looked like a sort of cubist Christmas card.

When he got the cards, says Robinson, “the results were stunning.” For the bees that had been kidnapped, life in a new home had indeed altered the activity of “whole sectors” of genes. When their gene expression data was viewed on the cards alongside the data for groups of bees raised among their own kin, a mere glance showed the dramatic change. Hundreds of genes had flipped colors. The move between hives didn’t just make the bees act differently. It made their genes work differently, and on a broad scale.

What’s more, the cards for the adopted bees of both species came to ever more resemble, as they moved through life, the cards of the bees they moved in with. With every passing day their genes acted more like those of their new hive mates (and less like those of their genetic siblings back home). Many of the genes that switched on or off are known to affect behavior; several are associated with aggression. The bees also acted differently. Their dispositions changed to match that of their hive mates. It seemed the genome, without changing its code, could transform an animal into something very like a different subspecies.

These bees didn’t just act like different bees. They’d pretty much become different bees. To Robinson, this spoke of a genome far more fluid—far more socially fluid—than previously conceived.

ps_break1.jpg
Robinson soon realized he was not alone in seeing this. At conferences and in the literature, he kept bumping into other researchers who saw gene networks responding fast and wide to social life. David Clayton, a neurobiologist also on the University of Illinois campus, found that if a male zebra finch heard another male zebra finch singing nearby, a particular gene in the bird’s forebrain would ""re up—and it would do so differently depending on whether the other finch was strange and threatening, or familiar and safe.

Others found this same gene, dubbed ZENK ramping up in other species. In each case, the change in ZENK's activity corresponded to some change in behavior: a bird might relax in response to a song, or become vigilant and tense. Duke researchers, for instance, found that when female zebra finches listened to male zebra finches’ songs, the females’ ZENK gene triggered massive gene-expression changes in their forebrains—a socially sensitive brain area in birds as well as humans. The changes differed depending on whether the song was a mating call or a territorial claim. And perhaps most remarkably, all
of these changes happened incredibly fast—within a half hour, sometimes within just five minutes.

Gene Robinson, an entomologist at the University of Illinois, found that when European honeybees are raised among more aggressive African killer bees, they not only start to become as belligerent as their new hive mates—they come to genetically resemble them. (Photo: Courtesy of Gene Robinson)
Gene Robinson, an entomologist at the University of Illinois, found that when European honeybees are raised among more aggressive African killer bees, they not only start to become as belligerent as their new hive mates—they come to genetically resemble them. (Photo: Courtesy of Gene Robinson)

ZENK, it appeared, was a so-called “immediate early gene,” a type of regulatory gene that can cause whole networks of other genes to change activity. These sorts of regulatory gene-expression response had already been identified in physiological systems such as digestion and immunity. Now they also seemed to drive quick responses to social conditions.


One of the most startling early demonstrations of such a response occurred in 2005 in the lab of Stanford biologist Russell Fernald. For years, Fernald had studied the African cichlid Astatotilapia burtoni, a freshwater fish about two inches long and dull pewter in color. By 2005 he had shown that among burtoni, the top male in any small population lives like some fishy pharaoh, getting far more food, territory, and sex than even the No. 2 male. This No. 1 male cichlid also sports a bigger and brighter body. And there is always only one No. 1.

I wonder, Fernald thought, what would happen if we just removed him?

So one day Fernald turned out the lights over one of his cichlid tanks, scooped out big flashy No. 1, and then, 12 hours later, flipped the lights back on. When the No. 2 cichlid saw that he was now No. 1, he responded quickly. He underwent massive surges in gene expression that immediately blinged up his pewter coloring with lurid red and blue streaks and, in a matter of hours, caused him to grow some 20 percent. It was as if Jason Schwartzman, coming to work one day to learn the big office stud had quit, morphed into Arnold Schwarzenegger by close of business.

These studies, says Greg Wray, an evolutionary biologist at Duke who has focused on gene expression for over a decade, caused quite a stir. “You suddenly realize birds are hearing a song and having massive, widespread changes in gene expression in just 15 minutes? Something big is going on.”

This big something, this startlingly quick gene-expression response to the social world, is a phenomenon we are just beginning to understand. The recent explosion of interest in “epigenetics”—a term literally meaning “around the gene,” and referring to anything that changes a gene’s effect without changing the actual DNA sequence—has tended to focus on the long game of gene-environment interactions: how famine among expectant mothers in the Netherlands during World War II, for instance, affected gene expression and behavior in their children; or how mother rats, by licking and grooming their pups more or less assiduously, can alter the wrappings around their offspring’s DNA in ways that influence how anxious the pups will be for the rest of their lives. The idea that experience can echo in our genes across generations is certainly a powerful one. But to focus only on these narrow, long-reaching effects is to miss much of the action where epigenetic influence and gene activity is concerned. This fresh work by Robinson, Fernald, Clayton, and others—encompassing studies of multiple organisms, from bees and birds to monkeys and humans—suggests something more exciting: that our social lives can change our gene expression with a rapidity, breadth, and depth previously overlooked.

Why would we have evolved this way? The most probable answer is that an organism that responds quickly to fast-changing social environments will more likely survive them. That organism won’t have to wait around, as it were, for better genes to evolve on the species level. Immunologists discovered something similar 25 years ago: Adapting to new pathogens the old-fashioned way—waiting for natural selection to favor genes that create resistance to specific pathogens—would happen too slowly to counter the rapidly changing pathogen environment. Instead, the immune system uses networks of genes that can respond quickly and flexibly to new threats.

We appear to respond in the same way to our social environment. Faced with an unpredictable, complex, ever-changing population to whom we must respond successfully, our genes behave accordingly—as if a fast, fluid response is a matter of life or death.

About the time Robinson was seeing fast gene expression changes in bees, in the early 2000s, he and many of his colleagues were taking notice of an up-and-coming UCLA researcher named Steve Cole.

Cole, a Californian then in his early 40s, had trained in psychology at the University of California-Santa Barbara and Stanford; then in social psychology, epidemiology, virology, cancer, and genetics at UCLA. Even as an undergrad, Cole had “this astute, fine-grained approach,” says Susan Andersen, a professor of psychology now at NYU who was one of his teachers at UC Santa Barbara in the late 1980s. “He thinks about things in very precise detail.”

""IF YOU ACTUALLY MEASURE STRESS, USING OUR BEST AVAILABLE INSTRUMENTS, IT CAN'T HOLD A CANDLE TO SOCIAL ISOLATION. SOCIAL ISOLATION IS THE BEST-ESTABLISHED, MOST ROBUST SOCIAL OR PSYCHOLOGICAL RISK FACTOR FOR DISEASE OUT THERE. NOTHING CAN COMPETE.""
In his post-doctoral work at UCLA, Cole focused on the genetics of immunology and cancer because those fields had pioneered hard-nosed gene-expression research. After that, he became one of the earliest researchers to bring the study of whole-genome gene-expression to social psychology. The gene’s ongoing, real-time response to incoming information, he realized, is where life works many of its changes on us. The idea is both reductive and expansive. We are but cells. At each cell’s center, a tight tangle of DNA writes and hands out the cell’s marching orders. Between that center and the world stand only a series of membranes.

“Porous membranes,” notes Cole.

“We think of our bodies as stable biological structures that live in the world but are fundamentally separate from it. That we are unitary organisms in the world but passing through it. But what we’re learning from the molecular processes that actually keep our bodies running is that we’re far more fluid than we realize, and the world passes through us.”

Cole told me this over dinner. We had met on the UCLA campus and walked south a few blocks, through bright April sun, to an almost empty sushi restaurant. Now, waving his chopsticks over a platter of urchin, squid, and amberjack, he said, “Every day, as our cells die off, we have to replace one to two percent of our molecular being. We’re constantly building and re-engineering new cells. And that regeneration is driven by the contingent nature of gene expression.

“This is what a cell is about. A cell,” he said, clasping some amberjack, “is a machine for turning experience into biology.”

When Cole started his social psychology research in the early 1990s, the microarray technology that spots changes in gene expression was still in its expensive infancy, and saw use primarily in immunology and cancer. So he began by using the tools of epidemiology—essentially the study of how people live their lives. Some of his early papers looked at how social experience affected men with HIV. In a 1996 study of 80 gay men, all of whom had been HIV-positive but healthy nine years earlier, Cole and his colleagues found that closeted men succumbed to the virus much more readily.

He then found that HIV-positive men who were lonely also got sicker sooner, regardless of whether they were closeted. Then he showed that closeted men without HIV got cancer and various infectious diseases at higher rates than openly gay men did. At about the same time, psychologists at Carnegie Mellon finished a well-controlled study showing that people with richer social ties got fewer common colds.

Something about feeling stressed or alone was gumming up the immune system—sometimes fatally.

“You’re besieged by a virus that’s going to kill you,” says Cole, “but the fact that you’re socially stressed and isolated seems to shut down your viral defenses. What’s going on there?”

He was determined to find out. But the research methods on hand at the time could take him only so far: “Epidemiology won’t exactly lie to you. But it’s hard to get it to tell you the whole story.” For a while he tried to figure things out at the bench, with pipettes and slides and assays. “I’d take norepinephrine [a key stress hormone] and squirt it on some infected T-cells and watch the virus grow faster. The norepinephrine was knocking down the antiviral response. That’s great. Virologists love that. But it’s not satisfying as a complete answer, because it doesn’t fully explain what’s happening in the real world.

“You can make almost anything happen in a test tube. I needed something else. I had set up all this theory. I needed a place to test it.”

His next step was to turn to rhesus monkeys, a lab species that allows controlled study. In 2007, he joined John Capitanio, a primatologist at the University of California-Davis, in looking at how social stress affected rhesus monkeys with SIV, or simian immunodeficiency virus, the monkey version of HIV. Capitanio had found that monkeys with SIV fell ill and died faster if they were stressed out by constantly being moved into new groups among strangers—a simian parallel to Cole’s 1996 study on lonely gay men.

Capitanio had run a rough immune analysis that showed the stressed monkeys mounted weak antiviral responses. Cole offered to look deeper. First he tore apart the lymph nodes—“ground central for infection”—and found that in the socially stressed monkeys, the virus bloomed around the sympathetic nerve trunks, which carry stress signals into the lymph node.

“This was a hint,” says Cole: The virus was running amok precisely where the immune response should have been strongest. The stress signals in the nerve trunks, it seemed, were getting either muted en route or ignored on arrival. As Cole looked closer, he found it was the latter: The monkeys’ bodies were generating the appropriate stress signals, but the immune system didn’t seem to be responding to them properly. Why not? He couldn’t find out with the tools he had. He was still looking at cells. He needed to look inside them.

Finally Cole got his chance. At UCLA, where he had been made a professor in 2001, he had been working hard to master gene-expression analysis across an entire genome. Microarray machines—the kind Gene Robinson was using on his bees—were getting cheaper. Cole got access to one and put it to work.

Thus commenced what we might call the lonely people studies.

First, in collaboration with University of Chicago social psychologist John Cacioppo, Cole mined a questionnaire about social connections that Cacioppo had given to 153 healthy Chicagoans in their 50s and 60s. Cacioppo and Cole identified the eight most socially secure people and the six loneliest and drew blood samples from them. (The socially insecure half-dozen were lonely indeed; they reported having felt distant from others for the previous four years.) Then Cole extracted genetic material from the blood’s leukocytes (a key immune-system player) and looked at what their DNA was up to.

He found a broad, weird, strongly patterned gene-expression response that would become mighty familiar over the next few years. Of roughly 22,000 genes in the human genome, the lonely and not-lonely groups showed sharply different gene-expression responses in 209. That meant that about one percent of the genome—a considerable portion—was responding differently depending on whether a person felt alone or connected. Printouts of the subjects’ gene-expression patterns looked much like Robinson’s red-and-green readouts of the changes in his cross-fostered bees: Whole sectors of genes looked markedly different in the lonely and the socially secure. And many of these genes played roles in inflammatory immune responses.

Now Cole was getting somewhere.

Normally, a healthy immune system works by deploying what amounts to a leashed attack dog. It detects a pathogen, then sends inflammatory and other responses to destroy the invader while also activating an anti-inflammatory response—the leash—to keep the inflammation in check. The lonely Chicagoans’ immune systems, however, suggested an attack dog off leash—even though they weren’t sick. Some 78 genes that normally work together to drive inflammation were busier than usual, as if these healthy people were fighting infection. Meanwhile, 131 genes that usually cooperate to control inflammation were underactive. The underactive genes also included key antiviral genes.

This opened a whole new avenue of insight. If social stress reliably created this gene-expression profile, it might explain a lot about why, for instance, the lonely HIV carriers in Cole’s earlier studies fell so much faster to the disease.

But this was a study of just 14 people. Cole needed more.

Over the next several years, he got them. He found similarly unbalanced gene-expression or immune-response profiles in groups including poor children, depressed people with cancer, and people caring for spouses dying of cancer. He topped his efforts off with a study in which social stress levels in young women predicted changes in their gene activity six months later. Cole and his collaborators on that study, psychologists Gregory Miller and Nicolas Rohleder of the University of British Columbia, interviewed 103 healthy Vancouver-area women aged 15 to 19 about their social lives, drew blood, and ran gene-expression profiles, and after half a year drew blood and ran profiles again. Some of the women reported at the time of the initial interview that they were having trouble with their love lives, their families, or their friends. Over the next six months, these socially troubled subjects took on the sort of imbalanced gene-expression profile Cole found in his other isolation studies: busy attack dogs and broken leashes. Except here, in a prospective study, he saw the attack dog breaking free of its restraints: Social stress changed these young women’s gene-expression patterns before his eyes.

In early 2009, Cole sat down to make sense of all this in a review paper that he would publish later that year in Current Directions in Psychological Science. Two years later we sat in his spare, rather small office at UCLA and discussed what he’d found. Cole, trimly built but close to six feet tall, speaks in a reedy voice that is slightly higher than his frame might lead you to expect. Sometimes, when he’s grabbing for a new thought or trying to emphasize a point, it jumps a register. He is often asked to give talks about his work, and it’s easy to see why: Relaxed but animated, he speaks in such an organized manner that you can almost see the paragraphs form in the air between you. He spends much of his time on the road. Thus the half-unpacked office, he said, gesturing around him. His lab, down the hall, “is essentially one really good lab manager”—Jesusa M. Arevalo, whom he frequently lists on his papers—“and a bunch of robots,” the machines that run the assays.

“We typically think of stress as being a risk factor for disease,” said Cole. “And it is, somewhat. But if you actually measure stress, using our best available instruments, it can’t hold a candle to social isolation. Social isolation is the best-established, most robust social or psychological risk factor for disease out there. Nothing can compete.”

This helps explain, for instance, why many people who work in high-stress but rewarding jobs don’t seem to suffer ill effects, while others, particularly those isolated and in poverty, wind up accruing lists of stress-related diagnoses—obesity, Type 2 diabetes, hypertension, atherosclerosis, heart failure, stroke.

Despite these well-known effects, Cole said he was amazed when he started finding that social connectivity wrought such powerful effects on gene expression.

“Or not that we found it,” he corrected, “but that we’re seeing it with such consistency. Science is noisy. I would’ve bet my eyeteeth that we’d get a lot of noisy results that are inconsistent from one realm to another. And at the level of individual genes that’s kind of true—there is some noise there.” But the kinds of genes that get dialed up or down in response to social experience, he said, and the gene networks and gene-expression cascades that they set off, “are surprisingly consistent—from monkeys to people, from five-year-old kids to adults, from Vancouver teenagers to 60-year-olds living in Chicago.”

Cole's work carries all kinds of implications—some weighty and practical, some heady and philosophical. It may, for instance, help explain the health problems that so often haunt the poor. Poverty savages the body. Hundreds of studies over the past few decades have tied low income to higher rates of asthma, flu, heart attacks, cancer, and everything in between. Poverty itself starts to look like a disease. Yet an empty wallet can’t make you sick. And we all know people who escape poverty’s dangers. So what is it about a life of poverty that makes us ill?

Cole asked essentially this question in a 2008 study he conducted with Miller and Edith Chen, another social psychologist then at the University of British Columbia. The paper appeared in an odd forum: Thorax, a journal about medical problems in the chest. The researchers gathered and ran gene-expression profiles on 31 kids, ranging from nine to 18 years old, who had asthma; 16 were poor, 15 well-off. As Cole expected, the group of well-off kids showed a healthy immune response, with elevated activity among genes that control pulmonary inflammation. The poorer kids showed busier inflammatory genes, sluggishness in the gene networks that control inflammation, and—in their health histories—more asthma attacks and other health problems. Poverty seemed to be mucking up their immune systems.

Cole, Chen, and Miller, however, suspected something else was at work—something that often came with poverty but was not the same thing. So along with drawing the kids’ blood and gathering their socioeconomic information, they showed them films of ambiguous or awkward social situations, then asked them how threatening they found them.

The poorer kids perceived more threat; the well-off perceived less. This difference in what psychologists call “cognitive framing” surprised no one. Many prior studies had shown that poverty and poor neighborhoods, understandably, tend to make people more sensitive to threats in ambiguous social situations. Chen in particular had spent years studying this sort of effect.

But in this study, Chen, Cole, and Miller wanted to see if they could tease apart the effect of cognitive framing from the effects of income disparity. It turned out they could, because some of the kids in each income group broke type. A few of the poor kids saw very little menace in the ambiguous situations, and a few well-off kids saw a lot. When the researchers separated those perceptions from the socioeconomic scores and laid them over the gene-expression scores, they found that it was really the kids’ framing, not their income levels, that accounted for most of the difference in gene expression. To put it another way: When the researchers controlled for variations in threat perception, poverty’s influence almost vanished. The main thing driving screwy immune responses appeared to be not poverty, but whether the child saw the social world as scary.

But where did that come from? Did the kids see the world as frightening because they had been taught to, or because they felt alone in facing it? The study design couldn’t answer that. But Cole believes isolation plays a key role. This notion gets startling support from a 2004 study of 57 school-age children who were so badly abused that state social workers had removed them from their homes. The study, often just called “the Kaufman study,” after its author, Yale psychiatrist Joan Kaufman, challenges a number of assumptions about what shapes responses to trauma or stress.

The Kaufman study at first looks like a classic investigation into the so-called depression risk gene—the serotonin transporter gene, or SERT—which comes in both long and short forms. Any single gene’s impact on mood or behavior is limited, of course, and these single-gene, or “candidate gene,” studies must be viewed with that in mind. Yet many studies have found that SERT's short form seems to render many people (and rhesus monkeys) more sensitive to environment; according to those studies, people who carry the short SERT are more likely to become depressed or anxious if faced with stress or trauma.

Kaufman looked first to see whether the kids’ mental health tracked their SERT variants. It did: The kids with the short variant suffered twice as many mental-health problems as those with the long variant. The double whammy of abuse plus short SERT seemed to be too much.

Then Kaufman laid both the kids’ depression scores and their SERT variants across the kids’ levels of “social support.” In this case, Kaufman narrowly defined social support as contact at least monthly with a trusted adult figure outside the home. Extraordinarily, for the kids who had it, this single, modest, closely defined social connection erased about 80 percent of the combined risk of the short SERT variant and the abuse. It came close to inoculating kids against both an established genetic vulnerability and horrid abuse.

""A CELL,"" STEVE COLE SAID, CLASPING SOME AMBERJACK, ""IS A MACHINE FOR TURNING EXPERIENCE INTO BIOLOGY.""
Or, to phrase it as Cole might, the lack of a reliable connection harmed the kids almost as much as abuse did. Their isolation wielded enough power to raise the question of what’s really most toxic in such situations. "
8,"The Biology of Attraction
Much of courtship and mating is choreographed by nature. In fact, nature designed men and women to work together.
By Helen E. Fisher published April 1, 1993 - last reviewed on June 9, 2016

Share on FacebookShare
Share on TwitterTweet
Share via EmailEmail
In an apocryphal story, a colleague once turned to the great British geneticist J.B.S. Haldane, and said, ""Tell me, Mr. Haldane, knowing what you do about nature, what can you tell me about God?"" Haldane replied, ""He has an inordinate fondness for beetles."" Indeed, the world contains over 300,000 species of beetles. I would add that ""God"" loves the human mating game, for no other aspect of our behavior is so complex, so subtle, or so pervasive. And although these sexual strategies differ from one individual to the next, the essential choreography of human courtship, love, and marriage has myriad designs that seem etched into the human psyche, the product of time, selection, and evolution. They begin the moment men and women get within courting range—with the way we flirt.

In describing these strategies, I make no effort to be ""politically correct."" Nature designed men and women to work together. But I cannot pretend that they are alike. They are not. And I have given evolutionary and biological explanations for their differences where I find them appropriate.

Flirting
Women from places as different as the jungles of Amazonia, the salons of Paris, and the highlands of New Guinea apparently flirt with the same sequence of expressions.

First the woman smiles at her admirer and lifts her eyebrows in a swift, jerky motion as she opens her eyes wide to gaze at him. Then she drops her eyelids, tilts her head down and to the side, and looks away. Frequently she also covers her face with her hands, giggling nervously as she retreats behind her palms. This sequential flirting gesture is so distinctive that [German ethologist Irenaus] Eibl-Eibesfeldt was convinced it is innate, a human female courtship ploy that evolved eons ago to signal sexual interest.

Men also employ courting tactics similar to those seen in other species. Have you ever walked into the boss's office and seen him leaning back in his chair, hands clasped behind his head, elbows high, and chest thrust out? Perhaps he has come from behind his desk, walked up to you, smiled, arched his back, and thrust his upper body in your direction? If so, watch out. He may be subconsciously announcing his dominance over you. If you are a woman, he may be courting you instead.

The ""chest thrust"" is part of a basic postural message used across the animal kingdom—""standing tall."" Dominant creatures puff up. Codfish bulge their heads and thrust our their pelvic fins. Snakes, frogs, and toads inflate their bodies. Antelope and chameleons turn broadside to emphasize their bulk. Mule deer look askance to show their antlers. Cats bristle. Pigeons swell. Lobsters raise themselves onto the tips of their walking legs and extend their open claws. Gorillas pound their chests. Men just thrust out their chests.

""Copulatory"" Gaze
The gaze is probably the most striking human courting ploy. Eye language. In Western cultures, where eye contact between the sexes is permitted, men and women often stare intently at potential mates for about two to three seconds during which their pupils may dilate—a sign of extreme interest. Then the starer drops his or her eyelids and looks away.

No wonder the custom of the veil has been adopted in so many cultures. Eye contact seems to have an immediate effect. The gaze triggers a primitive part of the human brain, calling forth one of two basic emotions—approach or retreat. You cannot ignore the eyes of another fixed on you; you must respond. You may smile and start conversation. You may look away and edge toward the door. But first you will probably tug at an earlobe, adjust your sweater, yawn, fidget with your eyeglasses, or perform some other meaningless movement—a ""displacement gesture""—to alleviate anxiety while you make up your mind how to acknowledge this invitation, whether to flee the premises or stay and play the courting game.

article continues after advertisement
Baboon Love
Baboons gaze at each other during courtship too. These animals may have branched off of our human evolutionary tree more than 19 million years ago, yet this similarity in wooing persists. As anthropologist Barbara Smuts had said of a budding baboon courtship on the Eburru cliffs of Kenya, ""It looked like watching two novices in a singles bar.""

The affair began one evening when a female baboon, Thalia, turned and caught a young male, Alex staring at her. They were about 15 feet apart. He glanced away immediately. So she stared at him—until he turned to look at her. Then she intently fiddled with her toes. On it went. Each time she stared at him, he looked away; each time he stared at her, she groomed her feet. Finally Alex caught Thalia gazing at him—the ""return gaze.""

Immediately he flattened his ears against his head, narrowed his eyelids, and began to smack his lips, the height of friendliness in baboon society. Thalia froze. Then, for a long moment, she looked him in the eye. Only after this extended eye contact had occurred did Alex approach her, at which point Thalia began to groom him—the beginning of a friendship and sexual liaison that was still going strong six years later, when Smuts returned to Kenya to study baboon friendships.

At The Bar
Could these courting cues be part of a larger human mating dance?

According to David Givens, an anthropologist, and Timothy Perper, a biologist, who spent several hundred hours in American cocktail lounges watching men and women flirt, American singles-bar courtship has several stages, each with distinctive escalation points. I shall divide them into five. The first is the ""attention getting"" phase. Young men and women do this somewhat differently. As soon as they enter the bar, both males and females typically establish a territory—a seat, a place to lean, a position near the jukebox or dance floor. Once settled, they begin to attract attention to themselves.

article continues after advertisement
Tactics vary. Men tend to pitch and roll their shoulders, stretch, exaggerate their body movements. Instead of using the wrist to stir a drink, men often employ the entire arm, as if stirring mud. The normally smooth motion necessary to light a cigarette becomes a whole-body gesture, ending with an elaborate shaking from the elbow to extinguish the match.

Then there is the swagger with which young men often move to and fro. Male baboons on the grasslands of East Africa also swagger when they foresee a potential sexual encounter. A male gorilla walks back and forth stiffly as he watches a female out of the corner of his eye. The parading gait is known to primatologists as bird-dogging. Males of many species also preen. Human males pat their hair, adjust their clothes, tug their chins, or perform other self-clasping or grooming movements that diffuse nervous energy and keep the body moving.

Young women begin the attention-getting phase with many of the same maneuvers that men use—smiling, gazing, shifting, swaying, preening, stretching, moving in their territory to draw attention to themselves. Often they incorporate a battery of feminine moves as well. They twist their curls, tilt their heads, look up coyly, giggle, raise their brows, flick their tongues, lick their upper lips, blush, and hide their faces in order to signal, ""I am here.""

Some women also have a characteristic walk when courting; they arch their backs, thrust out their bosoms, sway their hips, and strut. No wonder many women wear high-heeled shoes. This bizarre Western custom, invented by Catherine de Medici in the 1500s, unnaturally arches the back, tilts the buttocks, and thrusts the chest out into a female come-hither pose. The clomping noise of their spiky heels draws attention too.

Keeping Time
Body synchrony is the final and most intriguing component of the pickup. As potential lovers become comfortable, they pivot or swivel until their shoulders become aligned, their bodies face-to-face. This rotation toward each other may start before they begin to talk or hours into conversation, but after a while the man and woman begin to move in tandem. Only briefly at first. When he crosses his legs, she crosses hers; as he leans left, she leans left; when he smoothes his hair, she smoothes hers. They move in perfect rhythm as they gaze deeply into each other's eyes.

article continues after advertisement
Called interactional synchrony, this human mirroring begins in infancy. By the second day of life, a newborn has begun to synchronize its body movements with the rhythmic patterns of the human voice. And it is now well established that people in many other cultures get into rhythm when they feel comfortable together. Our need to keep each other's time reflects a rhythmic mimicry common to many animals. Chimps sometimes sway from side to side as they stare into one another's eyes just prior to copulation. Cats circle. Red deer prance. Howler monkeys court with rhythmic tongue movements. Stickleback fish do a zigzag jig. From bears to beetles, courting couples perform rhythmic rituals to express their amorous intentions.

Wooing Messages
Human courtship has other similarities to courtship in ""lower"" animals. Normally people woo each other slowly. Caution during courtship is also characteristic of spiders. The male wolf spider, for example, must enter the long, darker entrance of a female's compound in order to court and copulate. This he does slowly. If he is overeager, she devours him.

Men and women who are too aggressive at the beginning of the courting process also suffer unpleasant consequences. If you come too close, touch too soon, or talk too much, you will probably be repelled. Like wooing among wolf spiders, baboons, and other creatures, the human pickup runs on message. At every juncture in the ritual each partner must respond correctly, otherwise the courtship fails.

The Dinner Date
Probably no ritual is more common to Western would-be lovers than the ""dinner date."" If the man is courting, he pays—and a woman instinctively knows her partner is wooing her. In fact, there is no more widespread courtship ploy than offering food in hopes of gaining sexual favors. Around the world men give women presents prior to lovemaking. A fish, a piece of meat, sweets, and beer are among the delicacies men have invented as offerings.

This ploy is not exclusive to men. Black-tipped hang flies often catch aphids, daddy longlegs, or houseflies on the forest floor. When a male has felled a particularly juicy prey, he exudes secretions from an abdominal scent gland that catch the breeze, announcing a successful hunting expedition. Often a passing female hang fly stops to enjoy the meal—but not without copulating while she eats.

article continues after advertisement
""Courtship feeding,"" as this custom is called, probably predates the dinosaurs, because it has an important reproductive function. By providing food to females, males show their abilities as hunters, providers, worthy procreative partners.

Odor Lures
Every person smells slightly different; we all have a personal ""odor print"" as distinctive as our voice, our hands, our intellect. As newborn infants we can recognize our mother by her smell. Both men and women have ""apocrine glands in their armpits, around their nipples, and in the groin that become active at puberty. These scent boxes differ from ""eccrine"" glands, which cover much of the body and produce an odorless liquid, because their exudate, in combination with bacteria on the skin, produce the acrid, gamy smell of perspiration.

Today in parts of Greece and the Balkans, some men carry their handkerchiefs in their armpits during festivals and offer these odoriferous tokens to the women they invite to dance: they swear by the results.

But could a man's smell actually trigger infatuation in a woman? This possible link between male essence and female reproductive health may provide a clue to attraction. Women perceive odors better than men do. They are a hundred times more sensitive to Exaltolide, a compound much like men's sexual musk; they can smell a mild sweat from about three feet away; and at midcycle, during ovulation, women can smell men's musk even more strongly. Perhaps ovulating women become more susceptible to infatuation when they can smell male essence and are unconsciously drawn toward it to maintain menstrual cycling.

A woman's or a man's smell can release a host of memories too. So the right human smell at the right moment could touch off vivid pleasant memories and possibly ignite that first, stunning moment of romantic adoration.

But Americans, the Japanese, and many other people find odors offensive; for most of them the smell of perspiration is more likely to repel than to attract. Some scientists think the Japanese are unduly disturbed by body odors because of their long tradition of arranged marriages: men and women were forced into close contact with partners they found unappealing. Why Americans are phobic about natural body smells, I do not know. Perhaps our advertisers have swayed us in order to sell their deodorizing products.

Love Maps
A more important mechanism by which human beings become captivated by ""him"" or ""her"" may be what sexologist John Money called your love map. Long before you fixate on Ray as opposed to Bill, Sue instead of Ceciley, you have developed a mental map, a template replete with brain circuitry that determines what arouses you sexually, what drives you to fall in love with one person rather than another.

These love maps vary from one individual to the next. Some people get turned on by a business suit or a doctor's uniform, by big breasts, small feet, or a vivacious laugh. But averageness still wins. In one study, psychologists selected 32 faces of American Caucasian women and, using computers, averaged all of their features. Then they showed these images to college peers. Of 94 photographs of real female faces, only four were rated more appealing than these fabrications.

As you would guess, the world does not share the sexual ideals of Caucasian students from Wyoming. Despite wildly dissimilar standards of beauty and sex appeal, however, there are a few widely shared opinions about what incites romantic passion. Men and women around the world are attracted to those with good complexions. Everywhere people are drawn to partners whom they regard as clean. And men in most places generally prefer plump, wide-hipped women to slim ones. Looks count.

So does money. From rural Zulus to urban Brazilians, men are attracted to young, good-looking, spunky women, while women are drawn to men with property or money. Americans are no exception.

These male/female appetites are probably innate. it is to a males' genetic advantage to fall in love with a women who will produce viable offspring; it is to a woman's biological advantage to become captivated by a man who can help support her young. As Montaigne, the 16th-century French essayist, summed it up, ""We do not marry for ourselves, whatever we say; we marry just as much or more for our posterity.""

Love At First Sight
Could this human ability to adore another within moments of meeting come out of nature? I think it does. In fact, love at first sight may have a critical adaptive function among animals. During the mating season a female squirrel, for example, needs to breed. It is not to her advantage to copulate with a porcupine. But if she sees a healthy squirrel, she should waste no time. She should size him up. And if he looks suitable, she should grab her chance to copulate. Perhaps love at first sight is no more than an inborn tendency in many creatures that evolved to spur the mating process. Then among our human ancestors what had been animal attraction evolved into the human sensation of infatuation at a glance.

Infatuation Fades
Alas, infatuation fades. As Emerson put it, ""Love is strongest in pursuit, friendship in possession."" At some point, that old black magic wanes. Yet there does seem to be a general length to this condition. Psychologist Dorothy Tennov measured the duration of romantic love, from the moment infatuation hit to when a ""feeling of neutrality"" for one's love object began. She concluded, ""The most frequent interval, as well as the average, is between approximately 18 months and three years"" John Money agrees, proposing that once you begin to see your sweetheart regularly the passion lasts two to three years.

Psychiatrist Michael Liebowitz suspected that the end of infatuation is also grounded in brain physiology. He theorized that the brain cannot eternally maintain the revved-up site of romantic bliss. As he summed it up, ""If you want a situation where you and your long-term partner can still get very excited about each other, you will have to work on it, because in some ways you are bucking a biological tide.""

Harem Building
Only 16 percent of the 853 cultures on record actually prescribe monogyny, in which a man is permitted only one wife at a time. Western cultures are among them. We are in the minority, however. A whopping 84 percent of all human societies permit a man to take more than one wife at once—polygyny.

Men seek polygyny to spread their genes, while women join harems to acquire resources and ensure the survival of their young. If you ask a man why he wants a second bride, he might say he is attracted to her wit, her business acumen, her vivacious spirit, or splendid thighs. If you ask a women why she is willing to ""share"" a man, she might tell you that she loves the way he looks or laughs or takes her to fancy vacation spots.

But no matter what reasons people offer, polygyny enables men to have more children; under the right conditions women also reap reproductive benefits. So long ago ancestral men who sought polygyny and ancestral women who acquiesced to harem life disproportionately survived.

Man Is Monogamous
Because of the genetic advantages of polygyny for men and because so many societies permit polygyny, many anthropologists think that harem building is a badge of the human animal. But in the vast majority of societies where polygyny is permitted, only about five to 10 percent of men actually have several wives simultaneously. Although polygyny is widely discussed, it is much less practiced.

Whereas gorillas, horses, and animals of many other species always form harems, among human beings polygyny and polyandry seem to be optional opportunistic exceptions; monogamy is the rule. Human beings almost never have to be cajoled into pairing. Instead, we do this naturally. We flirt. We feel infatuation. We fall in love. We marry. And the vast majority of us marry only one person at a time.

Pair-bonding is a trademark of the human animal.

Unfaithfully Yours
Although we flirt, fall in love, and marry, human beings also tend to be sexually unfaithful to a spouse. Americans are no exception. Despite our attitude that philandering is immoral, regardless of our sense of guilt when we engage in trysts, in spite of the risks to family, friends, and livelihood that adultery entails, we indulge in extramarital affairs with avid regularity.

A survey of 106,000 readers of Cosmopolitan magazine in the early 1980s indicated that 54 percent of the married women had participated in at least one affair, and a poll of 7,239 men reported that 72 percent of those married over two years had been adulterous.

Why? From a Darwinian perspective, it is easy to explain. If a man has two children by one woman, he has, genetically speaking, ""reproduced"" himself. But if he also engages in dalliances with more women and, by chance, sires two more young, he doubles his contribution to the next generation. Those men who seek variety also tend to have more children. These young survive and pass to subsequent generations whatever it is in the male genetic makeup that seeks ""fresh features,"" as Byron said of men's need for sexual novelty.

Unlike a man, a woman cannot breed every time she copulates. In fact, anthropologist Donald Symons has argued that, because the number of children a woman can bear is limited, women are biologically less motivated to seek fresh features.

Sexual Variety
Are women really less interested in sexual variety? My own modest proposal is that during our long evolutionary history most males pursued trysts to spread their genes, while females evolved two alternative strategies to acquire resources: some women elected to be faithful to a single man in order to reap a lot of benefits from him; others engaged in clandestine sex with many men to acquire resources from each. This scenario roughly coincides with common beliefs: man, the natural playboy; women, madonna or whore.

In a study by Donald Symons and Bruce Ellis, for example, 415 college students were asked whether they would have sex with an anonymous student of the opposite sex. In this imaginary scenario, participants were told that all risk of pregnancy, discovery, and disease was absent. The results were those you would expect. Males were consistently more likely to say yes, leading these researchers once again to conclude that men are more interested in sexual variety than women are.

But here's the glitch. This study takes into consideration the primary genetic motive for male philandering (to fertilize young women). But not the primary motive for female philandering—the acquisition of resources.

There is no evidence whatsoever that women are sexually shy or that they shun clandestine sexual adventures. Instead, both men and women seem to exhibit a mixed reproductive strategy: monogamy and adultery are our fare.

Parting
We all have our share of troubles. But probably one of the hardest things we do is leave a spouse. From the tundras of Siberia to the jungles of Amazonia, people accept divorce as regrettable—although sometimes necessary. They have specific social or legal procedures for divorce. And they do divorce. Moreover, unlike many Westerners, traditional peoples do not make divorce a moral issue. The Mongols of Siberia sum up a common worldwide attitude, ""If two individuals cannot get along harmoniously together, they had better live apart.""

Why do people divorce? Bitter quarrels, insensitive remarks, lack of humor, watching too much television, inability to listen, drunkenness, sexual rejection—the reasons men or women give for why they leave a marriage are as varied as their motives for having wedded in the first place.

Overt adultery heads the list. Sterility and barrenness come next. Cruelty, particularly by the husband, ranks third among worldwide reasons for divorce. I am not surprised that adultery and infertility are paramount. Darwin theorized that people marry primarily to breed.

The Four-Year Itch
Hoping to get some insight into the nature of divorce, I turned to the demographic yearbooks of the United Nations. Divorce generally occurs early in marriage—peaking in or around the fourth year after wedding—followed by a gradual decline in divorce as more years of marriage go by. The American divorce peak hovers somewhat below the common four-year peak. Purely as a guess, I would say that this may have something to do with American attitudes toward marriage itself. We tend not to marry for economic, political, or family reasons. Instead, as anthropologist Paul Bohannen once said, ""Americans marry to enhance their inner, largely secret selves.""

I find this remark fascinating—and correct. We marry for love and to accentuate, balance out, or mask parts of our private selves. This is why you sometimes see a reserved accountant married to a blond bombshell or a scientist married to a poet. Perhaps it is no coincidence that the American divorce peak corresponds perfectly with the normal duration of infatuation—two to three years. If partners are not satisfied with the match, they bail out soon after the infatuation wears off. So there are exceptions to the four-year itch.

Divorce Is For The Young
Another pattern to emerge from the United Nations data regards ""divorce with dependent children."" Among the hundreds of millions of people recorded in 45 societies between 1950 and 1989, 39 percent of all divorces occurred among couples with no dependent children, 26 percent among those with one dependent child, 19 percent among couples with two, 7 percent among those with three children, 3 percent among couples with four young, and couples with five or more dependent young rarely split. Hence, it appears that the more children a couple bear, the less likely they are to divorce.

This pattern is less conclusively demonstrated by the U.N. data than the first two. Yet it is strongly suggested and it makes genetic sense. From a Darwinian perspective, couples with no children should break up; both individuals will mate again and probably go on to bear young—ensuring their genetic futures. As couples bear more children they become less economically able to abandon their growing family. And it is genetically logical that they remain together to raise their flock.

Planned Obsolescence Of The Pair Bond
Marriage clearly shows several general patterns of decay. Divorce counts peak among couples married about four years. And the longer a couple remain together, the older the partners get, and probably the more offspring they produce, the less likely spouses are to leave each other.

This is not to say that everybody fits this mold. But Shakespeare did. Etched in Shakespeare's marriage and in all these other divorces recorded from around the world is a blue print, a primitive design. The human animal seems built to court, to fall in love, and to marry one person at a time; then, at the height of our reproductive years, often with single child, we divorce; then, a few years later, we remarry once again.

Adapted from Anatomy of Love; The Natural History of Monogamy, Adultery, and Divorce, by Helen E. Fisher. Copyright C 1992 by Helen E. Fisher. Reprinted by arrangement with W. W. Norton & Co., Inc."
9,"Extreme chemistry: experiments at the edge of the periodic table
As the chase for new elements slows, scientists focus on deepening their understanding of the superheavy ones they already know.Illustration by Señor Salme

If you wanted to create the world’s next undiscovered element, number 119 in the periodic table, here’s a possible recipe. Take a few milligrams of berkelium, a rare radioactive metal that can be made only in specialized nuclear reactors. Bombard the sample with a beam of titanium ions, accelerated to around one-tenth the speed of light. Keep this up for about a year, and be patient. Very patient. For every 10 quintillion (1018) titanium ions that slam into the berkelium target — roughly a year’s worth of beam time — the experiment will probably produce only one atom of element 119.

On that rare occasion, a titanium and a berkelium nucleus will collide and merge, the speed of their impact overcoming their electrical repulsion to create something never before seen on Earth, maybe even in the Universe. But the new atom will fall apart within perhaps one-tenth of a millisecond. As it decays, it will spit out α-particles and γ-rays, which hit silicon detectors placed around the target to verify that element 119, fleetingly, existed.


Nature special: The periodic table

Researchers have tried this experiment. Chemists in Germany spent several months in 2012 on it, but gave up with no sightings. Scientists in Japan have tried other combinations of beam and target, and both they and a team in Russia have sought element 120, too, but with no luck.

The quest to extend the periodic table is not over, but it is grinding to a halt. Since Russian chemist Dmitri Mendeleev published his periodic table 150 years ago, researchers have been adding elements to it at the average rate of one every two or three years (see ‘When elements were discovered’). Having found all the elements that are stable enough to persist naturally, researchers started to create their own, and are now up to element 118, oganesson. Although they still hope to find more, they agree that prospects of venturing beyond element 120 are dim. “We’re reaching the area of diminishing returns in the synthesis of new elements, at least with our current level of technology,” says Jacklyn Gates, who works on heavy-element chemistry at the Lawrence Berkeley National Laboratory in California.


Source: F-X Coudert, CNRS

As a result, research on the edge of the periodic table is shifting focus. Rather than chasing new elements, scientists are going back to deepen their understanding of the superheavy ones — roughly speaking, those with an atomic number above 100 — that they have already made. Studying the chemical properties of these elements could show whether the most massive ones obey the organizing principle of the table — which sorts elements into groups with similar behaviours on the basis of periodically recurring patterns of chemical reactivity. And although the heaviest elements decay in less than the blink of an eye, researchers still hope that they might arrive at the fabled ‘island of stability’: a hypothesized region of element-land where some superheavy isotopes — atoms that have the same number of protons in their nucleus, but differing numbers of neutrons — might exist for minutes, days or even longer.

Creating new elements is conceptually straightforward, if technically immensely difficult and slow. But studying the chemistry and nuclear physics of atoms that decay in less than a second pushes both computational and experimental work to their limits. The results that have been obtained so far are already raising questions about the concept of chemical periodicity at these extremes. “The discovery of superheavy elements sometimes reminds me of the opening of Pandora’s Box,” says Yuri Oganessian, an 85-year-old Russian nuclear physicist, who is only the second scientist in history to have an element (oganesson) named after him while still alive. “The problems that were flung out from the box are much more complicated than the discovery of one more element.”
Rare fruit
Work on superheavies has its origins in wartime science in the 1940s. Some of the first non-natural elements were discovered in radioactive debris from the fallout of atom-bomb tests; others were made in particle accelerators. From the 1950s to the 1970s, when most of the research was conducted at either Berkeley or at the Joint Institute for Nuclear Research (JINR) in Dubna, Russia — the group that Oganessian leads — it took place in an atmosphere of cold-war competition. In the 1980s, Germany joined the race; an institute in Darmstadt now named the Helmholtz Center for Heavy Ion Research (GSI) made all the elements between 107 and 112.

The competitive edge of earlier years has waned, says Christoph Düllmann, who heads the GSI’s superheavy-elements department: now, researchers frequently talk to each other and carry out some experiments collaboratively. The credit for creating later elements up to 118 has gone variously, and sometimes jointly, to teams from Germany, the United States and Russia; a Japanese team at the RIKEN Nishina Center for Accelerator-based Science in Wako was credited with making element 113.
The modern-day hunt for superheavy elements resembles particle-physics experiments on a miniature scale, in that it involves teams with diverse scientific expertise sifting through masses of collision data for very rare events, says Matthias Schädel, who has been involved in element synthesis at the GSI for four decades. “Sitting in a control room for hours, days, maybe weeks, just looking for the long-awaited signal from a detector can become very boring and tiring,” he says. “However, observing the first event can be most exciting and joyful — and an enormous relief.”

“The synthesis of artificial elements has always been difficult and painstaking work,” says Oganessian. But expanding the periodic table beyond 118 consumes accelerator beam ime with diminishing returns. “Element 117 was obtained in the amount of one atom per week, and element 118 one atom per month. There is no reason to believe that the yield will increase for the still-unknown elements 119 and 120,” he says. It would help if researchers could increase the intensity of atomic beams, or the thickness of the targets, but both tasks are “technologically very challenging”, says nuclear physicist Dirk Rudolph of Lund University in Sweden.

The GSI has suspended its hunt for now, says Düllmann, because superheavy research at the German facility is being reined in during construction of an accelerator known as the Facility for Antiproton and Ion Research for studying astrophysical phenomena. Meanwhile, researchers at the JINR “practically stopped” searching for elements at the end of 2014, says Oganessian. Instead, over the past five years they have focused on building a laboratory, which Oganessian calls a superheavy-element factory, to churn out tens or hundreds of times more atoms of known elements, sometimes as new isotopes. The laboratory can accommodate five simultaneous experiments, says Oganessian; two 50-day demonstrations to make elements 114 and 115 are planned to start in April.

The group at Berkeley gave up element hunting some years back. “There are frequently discussions about whether or not we should try to make a new element,” says Gates, “but I think that the available beam time is better used by performing more detailed studies on the presently known elements.” RIKEN is still hunting for elements, but it is also consolidating what we know of existing ones: Hiromitsu Haba, who leads the centre’s superheavy-element production team, says his group is going to study the chemical properties of elements 104, 105 and 106.
The end of periodicity?﻿
Most researchers have always felt that there is as much value in probing the chemistry and nuclear physics of known elements as there is in making new ones. A key question is how much the superheavy elements sustain the periodicity in chemical behaviour that underpins Mendeleev’s table. The chemistry of an element depends on the reactivity of its outermost electrons. In atoms, electrons occupy discrete, fuzzy clouds called orbitals that surround the nucleus, and those with the highest energy are the ones involved in forming chemical bonds and ions. Elements in the same column in the periodic table (‘homologues’) share similar chemical properties because they have similar electron configurations, with the same number of electrons in their outermost shells.

So seaborgium (element 106), for example, would be expected to resemble elements above it in the periodic table (such as molybdenum and tungsten), in forming compounds in which the six outermost electrons have been shared or transferred to surrounding atoms. And by carrying seaborgium quickly to a reaction chamber so that its compounds can be produced and separated by both gas and liquid chromatography, chemists proved two decades ago that this is true1,2. The atoms can be tracked as they move between chemical phases because of their radioactive instability: they spit out α-particles as they decay. No one has managed to examine liquid-phase reactions beyond seaborgium, but hassium (element 108) has been shown to bind to four oxygen atoms like its homologue osmium — forming hassium tetroxide — while borne along within a carrier gas3.

But it is extremely hard to conduct such experiments with the heaviest of the superheavies. Valeria Pershina, a theoretical chemist at the GSI, says that such experiments currently require isotopes that have a half-life no shorter than 1 second; their feasibility might also depend on the rate at which atoms can be produced. Although some isotopes for elements 109–111 are relatively long-lived — some lasting more than a minute — these are all formed in decay chains and are not suitable for chemical studies (see ‘The superheavy realm’), although radiochemist Andreas Türler at the University of Bern in Switzerland says he is confident that they exist. “The difficulty is to produce them at a rate that would allow experiments,” he says.


Source: IAEA

But some of the even heavier elements have turned out to be amenable to other kinds of chemical study. A relatively simple technique to probe superheavies, for instance, measures how strongly the atoms, carried along in a surrounding gas, become attached to surfaces. Experiments at the GSI have shown, for example, that flerovium (114) forms a metal–metal bond on a gold surface like its homologue lead, but that the bond is much weaker: in other words, the superheavy element is less reactive and more likely to stay in the gas phase4. Copernicium (112), meanwhile, interacts with gold much less strongly than its homologue mercury does — in fact, it behaves almost like a noble gas5. Nihonium (113) has been harder to study experimentally, but preliminary observations at the JINR6, as well as Pershina’s calculations7, suggest that it forms relatively strong chemical bonds to a surface, albeit weaker ones than its homologue thallium.

This is somewhat as expected: the strength of chemical bonding tends to decrease down a periodic group, as atoms get larger. But to fully explain superheavies’ chemistry, Pershina’s calculations must also take into account relativistic effects. In very heavy atoms, which have super-strong interactions between the innermost electrons and the highly charged nuclei, the electrons are travelling so fast (potentially at more than 80% of the speed of light) that their mass increases, as special relativity predicts. This pulls them farther in towards the nucleus, which can mean that they screen the outer electrons from the nuclear charge more effectively. That alters the outer electrons’ energies and, consequently, their chemical reactivity.

Relativistic extremes
Relativistic effects are already understood — they are responsible for the yellowish colour of gold and the low melting point of mercury, for example. But superheavy elements display these effects in extreme form, whereby they are hard to calculate precisely from first principles. The 2002 discovery8 of how relativistic effects cause dubnium (105) to behave rather differently from its homologue tantalum in group 5 of the periodic table was “so exciting for us that it created an enormous momentum to continue probing the chemistry of the superheavy elements”, says Schädel.

Chemists don’t think that the relativistic deviations observed so far destroy the broad idea that elements in the same group share similar properties that distinguish them from other groups, says Düllmann. In his opinion, researchers won’t see a marked loss of chemical periodicity until they find elements beyond 120. Here, he says, “several orbitals start to be so close in energy that no regular pattern is expected anymore”. That tendency seems already borne out by calculations last year, which suggest that oganesson might not be an unreactive noble gas like its homologues xenon and radon. Its outermost electron orbits are smeared together so that it might be more reactive than its position in the periodic table seems to imply9.

To measure electrons’ energy levels directly, physicists and chemists have long used spectroscopy. Essentially, this involves firing light at atoms to measure the energies at which electrons absorb — and later emit — photons as the particles jump up to higher energy levels or relax down again. But doing this on single, short-lived atoms is extremely challenging: how do you do the measurement, with enough sensitivity, before your quarry has vanished? Still, in 2016, a team working at the GSI was able to measure the ionization potential of single atoms of nobelium, element 102, that had a half-life of 51.2 seconds10. The researchers made the nobelium atoms at a rate of around four per second by colliding a calcium ion beam into a lead target, before slowing down the atoms in argon gas to collect them on a filament of tantalum. Periodically, the researchers heated the filament to release the nobelium atoms into the gas phase and excited them with lasers, kicking off an electron in a two-step process — all in a matter of seconds. In later work, they used such spectroscopic measurements to infer the shape and structure of the nuclei of three nobelium isotopes, concluding that they are not spherical, but a rugby-ball shape: a distortion that affects electronic structure11. The group is now working to extend its measurements to lawrencium, element 103.

Cathrina Ullmann from the ion source machinery
Cathrina Ullman stands inside the apparatus for accelerating ions at Germany’s GSI Center for Heavy Ion Research in Darmstadt.Credit: J. Hosan/GSI Helmholtzzentrum for Schwerionenforschung GmbH

Researchers at the Japan Atomic Energy Agency (JAEA) in Tokai have developed a different approach for measuring ionization energies. In their set-up, atoms of a superheavy element recoiling out of a target are taken up in a jet of helium carrier gas passing through a heated tube of tantalum. The atoms transfer an electron to the metal surface, before being taken to an α-particle detector that verifies their identity. In this way “we can measure ionization energies of elements with a half-life of a few seconds”, says Yuichiro Nagame, a member of the research group for heavy-element nuclear science at the JAEA. Nagame and his colleagues have used the method to measure this quantity for all elements from fermium (100) to lawrencium (103)12,13. As calculations had predicted, relativistic effects make the ionization potential of lawrencium even lower, relative to its lighter homologue lutetium, than the usual periodic trends would imply. Nagame says his group is now developing a new method for measuring ionization potentials of elements beyond lawrencium, which are not volatile enough for this surface technique to be used.

Things fall apart
Observing the chemical behaviour of elements beyond 118, even if they can be made, will be an immense challenge because they are expected to fall apart so quickly. “I do not want to say that it is impossible, but just now I do not know how to approach this task,” says Oganessian.

Yet nuclear scientists have speculated that long-lasting superheavy isotopes could exist. Like electrons, the protons and neutrons in atomic nuclei are also arrayed in shell-like configurations that make their structure more or less easy to disrupt. And particular ‘magic numbers’ of these particles, corresponding to filled shells, are predicted to confer stability, prolonging the time before an atom decays. (The effect is roughly analogous to how noble gases are relatively stable and unreactive because they have filled shells of electrons).

A leading candidate for a stable superheavy element is the isotope flerovium-298, with 114 protons and 184 neutrons. “Most reasonable models favour this proton and neutron number to be those that should give extra stability in the region,” says Rudolph. Researchers dream that particular isotopes within this island of stability might last long enough for significant amounts of the element to be accumulated. Already, the longer half-lives of some isotopes of copernicium and flerovium offer “a first indication that this concept is correct, and that more long-lived isotopes just await discovery”, says Düllmann. Although experiments support the existence of the island of stability, Sebastian Raeder, a nuclear chemist at the GSI, says that “it will be very difficult to reach it in the near future”, because no one knows how to load that many neutrons into a nucleus.

Superconductive accelerator for core machinery at JINR.
Apparatus for accelerating ion beams at the Joint Institute for Nuclear Research in Dubna, Russia.Credit: Sovfoto/UIG/Getty

At times, researchers discuss among themselves where the periodic table absolutely has to end. That might happen, for example, because the outermost electrons of a very heavy atom might not have any states in which they are actually bound to the nucleus, and so there will be no real chemistry to speak of. Or the nuclei themselves might fall apart as soon as they are formed. Last May, the governing body of chemistry, the International Union of Pure and Applied Chemistry (IUPAC), reasserted its position that an element should persist at least for 10–14 seconds. But some chemists question whether atoms that don’t have time to interact can be meaningfully assigned chemical properties and so qualify as an element.

That is really a question about whether, at its farthest extremes, the periodic table will remain a scheme based on chemistry — as it was for Mendeleev — or whether it becomes about the physics of matter at very high nucleon densities. New elements have to be put somewhere in the table, but their positioning might become an empty formality, rather than indicating something useful about chemistry.

To reach that point, however, chemists will need to create those heavier elements. For all its technical difficulty, the hunt continues. “We started the search for element 119 last June,” says RIKEN researcher Hideto En’yo. “It will certainly take a long time — years and years — so we will continue the same experiment intermittently for 100 or more days per year, until we or somebody else discovers it.”

“I am quite optimistic that elements 119 and 120 will be found in the next ten years,” says Düllmann. “The longer-term outlook may seem dim, but when I was a PhD student in the 1990s, papers were published by giants in the field that explained why 112 was close to the limit. Twenty years later, we have 118. I guess we should not underestimate the next generation.”

Nature 565, 552-555 (2019)

doi: https://doi.org/10.1038/d41586-019-00285-9

UPDATES & CORRECTIONS
Correction 28 February 2019: An earlier version of this feature erred in its explanation of why none of the currently identified isotopes for elements 109–111 are suitable for chemical studies. In fact, the problem is not the length of the half-lives, but that the isotopes are created in decay chains, not through collisions. Also, the Y-axis of the graph in ‘The superheavy realm’ was mislabelled.

References
1.
Schädel, M. et al. Nature 388, 55–57 (1997).

Article
 
Google Scholar
 

2.
Schädel, M. et al. Radiochim. Acta 77, 149–159 (1997).

Article
 
Google Scholar
 

3.
Düllmann, C. E. et al. Nature 418, 859–862 (2002).

PubMed
 
Article
 
Google Scholar
 

4.
Yakushev, A. et al. Inorg. Chem. 53, 1624–1629 (2014).

PubMed
 
Article
 
Google Scholar
 

5.
Eichler, R. et al. Angew. Chem. Int. Ed. 47, 3262–3266 (2008).

Article
 
Google Scholar
 

6.
Dmitriev, S. N. et al. Mendeleev Commun. 24, 253–256 (2014).

Article
 
Google Scholar
 

7.
Pershina, V. Inorg. Chem. 57, 3948–3955 (2018).

PubMed
 
Article
 
Google Scholar
 

8.
Schädel, M. J. Nucl. Radiochem. Sci. 3, 113–120 (2002).

Article
 
Google Scholar
 

9.
Jerabek, P. et al. Phys. Rev. Lett. 120, 053001 (2018).

PubMed
 
Article
 
Google Scholar
 

10.
Laatiaoui, M. et al. Nature 538, 495–498 (2016).

PubMed
 
Article
 
Google Scholar
 

11.
Raeder, S. et al. Phys. Rev. Lett. 120, 232503 (2018).

PubMed
 
Article
 
Google Scholar
 

12.
Sato, T. K. et al. Nature 520, 209–211 (2015).

PubMed
 
Article
 
Google Scholar
 

13.
Sato, T. K. et al. J. Am. Chem. Soc. 140, 14609–14613 (2018).

PubMed
 
Article
 
Google Scholar"
10,"Key trends for property investors to watch in 2020Key trends for property investors to watch in 2020
By Jerald Solis, Director, Experience Invest

The new year invariably acts as a time for reflection, prediction and target setting. This is true for businesses, consumers and investors.

Jerald Solis
Jerald Solis

In 2020, this period has become a little more interesting; just last month the Conservative Party secured an overwhelming majority in the General Election, while at the end of the current month the UK will – as it stands – leave the European Union (EU). As such, after a long period of uncertainty and hesitancy, we could be about to witness widespread and rapid change across the country.

This is certainly true in the property sector. There is a palpable sense among many businesses that the coming 12 months will provide an opportunity to address pressing issues, allowing the market to make meaningful progress after a challenging three years.

But what changes are we likely to see? Here are my thoughts on some of the key trends that will shape the property market over the coming year…

Brexit to release pent-up demand

To date, there have been four different Brexit deadlines. It has already been moved from 29th March 2019 to 12th April to 31st October 2019, and then to 31st January 2020. But it now seems likely that the UK’s departure from the EU will be confirmed at the end of this month.

In December’s election, Prime Minister Boris Johnson campaigned primarily on the promise to “get Brexit done”; the majority his party won has strengthened his position to do so. Indeed, on 9th January MPs voted 330 to 231 in favour of the Withdrawal Agreement Bill – the bill that will implement the UK government’s Brexit deal.

But what does this mean for the UK property market?

The prevailing sentiment among property experts is that once Brexit has passed, there will be an upturn in market activity. Buyers and sellers who had become immobilised by Brexit uncertainty may soon be sparked back into life.

In Q3 2019 Experience Invest surveyed more than 1,000 UK property investors, finding that just over half (51%) of the respondents were confident there will be an increase in property listings and sales once Brexit is complete. The majority (55%) also admitted they had paused on their investment plans over the course of 2019 as they awaited the outcome of Brexit.

Brexit now looks set to go ahead on 31st January 2020. With this will come greater clarity and certainty over how investment markets will perform and what state the UK economy will be in – one would expect this to provide the foundations for greater activity in the property market over the next 12 months.

Property prices to grow, but probably not in London

Since 2010, the average UK house price has risen from £167,469 to £232,944. This impressive growth has come in a period that included the aftermath of the global financial crisis, four general elections, three new prime ministers and the Brexit saga.

This upward trend looks set to continue; according to a poll of 27 property market analysts carried out by Reuters, UK house prices are predicted to rise by 1.5%in 2020 and 2.3% in 2021.

However, there are sub-trends hiding within this data. Most notably, the way the London market is performing compared to the rest of the UK.

The aforementioned Reuters poll also suggests that prices in London will fall by 1.5% in 2020, which is lowering the overall total. In fact, this is a trend that has been prevalent over recent years, with prices in the capital stagnating or declining while other regions grow at pace.

Analysis of 2019’s data by Zoopla shines a light on this trend. It found that on average a property in London fell in value by £71.23 each day in the first six months of 2019 – by comparison, real estate in the South East (+£35.32), NorthWest (+£20.39), Wales (+£18.03), Yorkshire (+£12.37) and North East (+£6.97) was all on the rise, with the West Midlands leading the way (+£36.58).

This trend looks set to continue in 2020. In fact, regional house price growth could be spurred on further by the Prime Minister’s promises to reignite the Northern Powerhouse initiative – given Johnson’s party enjoyed much success across the North in December’s election, there is an onus on the Conservatives to reward its newer supporters with greater investment into the region.

New-builds to remain on the public agenda

Looking beyond the prospects for market activity and price growth, one of the defining property trends of 2020 will almost certainly be new-build developments.

The Housing Crisis remains one of the country’s most-talked-about domestic issues, with the shortage of available housing impacting on most areas of society.

As a result, successive governments throughout the 21st Century have made bold statements and set bolder targets for increasing the country’s housing stock. And the new Conservative Government is no exception: it has promised to deliver a million new homes by 2025.

Only time will tell if the Government can reach this target. Evidence from the past two decades does not paint a particularly pretty picture; house-building has typically fallen well short of the levels proposed. This makes the upcoming Budget – taking place on 11th March – hugely important; will Chancellor Sajid Javid unveil any major reforms or spending commitments that could fuel activity in the new-build space?

At Experience Invest we work closely with property developers and local councils to ensure private investment is made available for much-needed real estate projects. It will be interesting to see if the government can do more during the current five-year parliament to ensure there is more collaboration between the public and private sector when it comes to building more new homes across the UK.

Jerald Solis is Business Development & Acquisitions Director at Experience Invest. Experience Invest provides property investors in the UK and overseas access to exclusive investments across a variety of asset classes. The London-based company has been running for 15 years, working closely with developers and investors to deliver excellent real estate investment opportunities."
11,
12,"What Cause Traffic Jams? The Physics Behind You Need To Know
by Smart Motorist
I live in Seattle and my two daily commutes last about 45 minutes. (That’s when I’m lucky; sometimes it’s more like two hours each.) This has given me an immense amount of time watching the interesting patterns in the cars. Boredom led me to fantasize about the traffic being like a flowing liquid, with cars acting as giant water molecules. Over many months I slowly realized that this was not just a fantasy.

Why had I never noticed all the “traffic fluid dynamics” out there? Once my brain became sensitized to it, I started seeing quite a variety of interesting things occurring. Eventually, I started using my car to poke at the flowing traffic. Observation eventually leads to experimentation, no? There are amazing things you can do as an “amateur traffic dynamicist.” But first, some basic phenomena.

Have you ever been driving on an interstate highway when traffic suddenly slows to a crawl? You inch along for many minutes while waiting to see the accident which must have caused the jam. At the same time, you also curse the “rubberneckers” who are causing the whole problem. But then all the cars ahead of you take off at high speed. The jam is over, but no accident, no police cars, nothing.

WHAT THE HECK WAS THAT! A traffic jam with no cause? In the rear-view mirror, you see all the poor saps behind you still stuck in the jam. But why? If all those people could just speed up at the same time, the whole traffic jam would evaporate. Why don’t they ever do that? What caused the mysterious slowdown in the first place?

After experiencing many of these “invisible accidents”, I came up with the following explanation. To best understand this, imagine that you look down on traffic from an aerial viewpoint. Pretend you’re in a Traffic Reporter’s helicopter looking downwards.


Fig 1: Cars lining up behind an accident
Above in fig. 1 I’ve drawn a one-lane road, an accident, and a row of cars stuck behind the wreck. Other cars are approaching from the left and stopping too. Suppose that the “wrecked” car (the red one) has simply become temporarily stuck. Maybe it spun out on ice. What will happen when the red car moves and unplugs the flow?


Fig 2: A wave of ‘condensed’ traffic creeps backward
Refer to fig. 2 above. In the top row (fig. 2A) the flow is suddenly unplugged. But not all the cars can move, since most cars are stuck behind drivers who are stopped. Figure 2B shows the traffic a few moments later, and figure 2C shows it a few moments after that. Notice the orange car in 2A, and see how it eventually becomes unjammed in 2D and begins moving. At the same time, the red car in 2A approaches the jam and is swallowed up.

A MOVING WAVE OF “JAM”
Traffic Jams 4
After the wreck is removed, there seems to be no reason for the traffic jam to persist. Yet it does. The reason for this is sensible: if I am stuck behind a car that is stopped, then I have to stop too, and so does the car behind me.

All the cars in the jam are in this situation. Even though the wreck is gone, they remain locked at a standstill because if they want to move, they ALL have to move at once. They never do, because each driver is waiting for the car ahead to move. If I am in the traffic jam, I’m not going to move forward because I have no room to do so. I’d bump the car ahead of me. We all think like this, so none of us can move.

When the car in front of me leaves, I still cannot accelerate instantly, so I will remain stopped for a moment. I must delay leaving for a moment. If I started up instantly, I’d stay too close to the car ahead of me, and that would not be safe.

Each departing car must delay in the same way, and this causes the jam to “evaporate” starting from the forward downstream end. It evaporates in a wave which begins at the forward end of the jam, (near the wreck). The wave eats into the jam from right to left.

Starting at figure 2A, the cars depart from the jam in sequence. In 2B the wave of “evaporation” has moved away from the wreck site, and in 2C and 2D it is far from the wreck. But notice an interesting thing: even though the CARS THEMSELVES are moving from left to right, the “wave of evaporation” moves in the opposite direction. It moves leftwards as it eats into the traffic jam.

There is a second important thing to notice. While some cars are still jammed, more cars are piling up behind them at the trailing end of the jam. Even after the wreck is removed, more cars are still “condensing” onto the back of the jam. The traffic jam is like a solid object whose front end is evaporating and whose back end is growing like a crystal. Cars move left to right, yet watch the group of stopped cars.

The stoppage is creeping slowly upstream, in the opposite direction to the moving cars. The accident is gone, but a moving wave of stopped cars remains behind. It’s not a traffic jam, it’s a shock wave which propagates through the “automotive material”. It’s a traffic-clot in the blood vessel. It’s a traveling wave of traffic-condensation.

NOT CAUSED BY ACCIDENTS
Traffic Jams 1
These sorts of traveling waves are common during heavy traffic conditions. An accident isn’t needed to create them, sometimes they are caused by near-misses, by people cutting each other off, by merging lanes at a construction site, or simply by extra cars entering from an on-ramp. In traffic engineering lingo, they can be caused by “incidents” on the highway. A single “rubbernecker” could cause one by momentarily stopping to look at something interesting. Whenever you slow way down in order to merge across a lane to get to your upcoming exit, YOU could create one.

Sometimes they have no cause at all. They are like sand ripples and sand dunes, and they just appear for no clear reason. They are like ocean waves caused by the steady breeze, or like the waves which move along a flapping flag. They just “emerge” spontaneously from the writhing lines of traffic. In the science of Nonlinear Dynamics, this is called an “EMERGENT PHENOMENON.”

   Trending content from Smart Motorist
Play Video
How long will the “traffic wave” last after the accident is cleared? Its lifetime depends upon the amount of traffic, and on the number of cars trapped in the jam, but sometimes these things can persist for many hours. When traffic is slight, the traffic jam might shrink rapidly to nothing. But if traffic remains heavy, then there’s no reason for the traveling wave to ever dissipate at all. Also, if the conditions are just right (if the “condensation” happens faster than the “evaporation”,) then even a tiny wave could grow large and larger. Sort of like dropping a tiny seed crystal into a supersaturated solution. When traffic is heavy and unstable, a single driver can cause the traffic to freeze into a gigantic crystal. Like Kurt Vonnegut’s end of the world story CAT’S CRADLE, it’s the “Ice Nine” of the highways.

So, next time you are commuting and you approach a stoppage, don’t think of it as a stupid f@#$% traffic jam. Think of it as a pressure wave which has approached your car and engulfed it. Think of it as a simple living thing which is made of cars rather than molecules. Stay hopeful that the crystalline amoeba poops your car out soon. Take an aerial viewpoint, and visualize the wave which is moving backward as you move forward.

Merging-Lane Traffic Jams, A Simple Cure


 

<<<<< ON THE LEFT: NORMAL DRIVERS WHO PACK THEMSELVES TIGHTLY TOGETHER WHENEVER THE TRAFFIC COMES TO A STOP. NOBODY CAN MERGE EXCEPT AT THE END OF THE JAM. NOTE THEIR LOW SPEED.

ON THE RIGHT >>>>> DRIVERS WITH UNUSUAL BEHAVIOR: THEY ENCOURAGE OTHERS TO MERGE AHEAD OF THEM, AND THEY TEND TO MAINTAIN LARGE SPACES AHEAD, EVEN IF TRAFFIC SLOWS TO A CRAWL. MERGING IS EASY. SEE HOW MUCH FASTER THEY GO?

Traffic jams often occur on highways wherever two lanes must merge into one. Lanes of cars cannot merge if there are no large gaps between cars. Therefore, drivers who create large gaps between cars will ease this type of traffic jam.

To Ease This Type Of Jam:
Traffic Jams 5
Maintain a large space ahead of your car.
Encourage one, two even three cars to merge ahead of you.
If traffic slows to a complete stop, KEEP TWO CAR-LENGTHS OF SPACE OPEN AHEAD OF YOU.
Never “punish” merging drivers by closing your gap.
Other suggestions
Amazingly enough, it is not necessary that EVERYONE do this. If only a few drivers will maintain large gaps during heavy traffic, then merging traffic is not forbidden, and the situation in the left-hand diagram can be prevented.

Yes, you’re right, you cannot eliminate every problem by simply making a big gap in front of your car. When there are too many cars on the road, traffic slows down. But if we use these special driving habits, the smaller jams can be erased, and stop-and-go traffic can be smoothed out. Since many traffic jams are caused by merging lanes, many traffic jams can be improved by the actions of just one driver.

TRAFFIC “EXPERIMENTS” AND A CURE FOR WAVES & JAMS
Traffic Jams 6
My First ‘Experiment’
Once upon a time, years ago, I was driving through a number of stop/go traffic waves on I-520 at rush hour in Seattle. I decided to try something. On a day when I immediately started hitting the usual “waves” of stopped traffic, I decided to drive slow. Rather than repeatedly rushing ahead with everyone else, only to come to a halt, I decided to try to drive at the average speed of the traffic. I let a huge gap open up ahead of me, and timed things so I was arriving at the next “stop-wave” just as the last red brake lights were turning off ahead of me. It certainly felt weird to have that huge empty space ahead of me, but I knew I was driving no slower than anyone else. Sometimes I hit it just right and never had to touch the brakes at all, but sometimes I was too fast or slow. There were many “waves” that evening, and this gave me many opportunities to improve my skill as I drove along.

I kept this up for maybe half an hour while approaching the city. Finally, I happened to glance at my rearview mirror. There was an interesting sight.

It was dusk, the headlights were on, and I was going down a long hill to the bridges. I had a view of miles of highway behind me. In the other lane, I could see maybe five of the traffic stop-waves. But in the lane behind me, for miles, TOTALLY UNIFORM DISTRIBUTION. I hadn’t realized it, but by driving at the average speed, my car had been “eating” traffic waves. Everyone ahead of me was caught in the stop/go cycle, while everyone behind me was forced to go at a nice smooth 35MPH or so. My single tiny car had erased miles and miles of stop-and-go traffic. Just one single “lubricant atom” had a profound effect on the turbulent particle flow within the “tube.”

It’s always a good idea to drive without changing speed and without competing with other drivers for bits of headway. But I’d always assumed that the reasons were philosophical rather than practical (i.e. try to be a calm, nice person.) But my above experience shows differently. A single solitary driver, if they stop “competing” and instead adopt some unusual driving habits, can actually wipe away some of the frustrating traffic patterns on a highway. That “nice” noncompetitive driver can erase traffic waves. I suspect that the opposite is also true: normal competitive behavior CREATES the traffic waves.

Suppose we push constantly ahead, change lanes to grab a bit of headway, and always eliminate our forward space in order to prevent other drivers from “cutting us off”. If tiny traffic waves appear, we will rush ahead and then brake hard, leaving larger waves behind us. Repeated action causes the waves to grow. Ironic that the angry people who drive as fast as possible might unwittingly participate in “amplifying” the very waves that they hate so much.

MORE EXPERIMENTS
I rarely commute on 520 where the good traffic waves appear. I started to miss having opportunities to cancel them. However, I soon realized that the same process could be used to affect small traffic jams too. Traffic waves are simply a series of small traffic jams with even spacing.

Each little jam is destroyed when a large empty space approaches it from behind. If no new cars are feeding into the jam from behind, yet cars are leaving from the front, then the jam is eroding away. If the jam is small enough, or if the empty space is large enough, then the jam can be annihilated entirely by a single car, as I had done with traffic waves.

Now I remember something from years farther back. When leaving one of those “rubbernecker slowdowns”, I always tried to accelerate like mad. I figured that if everyone did this, then the slowdown would evaporate. Yet this did little good, because the car ahead of me would not accelerate. I could not force the cars ahead of me to stomp on the gas, so I could do nothing to aid the “evaporation” of the traffic stoppage.

Aha! I can control the people behind me by slowing down, but I cannot control the people in front of me by speeding up. Therefore, I can destroy a small traffic stoppage by slowing down long before I approach it, but I can do nothing to aid the “evaporation” at the other end of the jam. Accelerating out of the jam does nothing unless EVERYONE would do the same, and there is no way to change everyone’s behavior.

But just one single car, if it decelerates while approaching, can change the behavior of everyone behind it. It can bite a chunk out of the region of stopped traffic. If that driver gradually builds up some empty space before encountering the slowdown, the slowdown can be “eaten” just as the traffic waves were “eaten.”

 


On my evening commute on I-5 southbound from Everett, there is always a right-lane traffic jam at one of the Lynnwood off-ramps. Close-packed cars must crawl along at 2mph for a very long time. Therefore I intentionally approached that jam in the right lane, and started letting a REALLY huge empty space open up ahead of me. By the time I hit the jam, there was maybe 1000ft of empty road ahead of me.

Sure enough, my big empty space stopped traffic from feeding it from behind, while the front of the jam kept dissolving as usual. By the time I arrived, the jam was about half the size it had been. Amazing. This wasn’t any little traffic wave, yet just a single driver was able to take a huge bite out of it.

Obviously, my actions did more than just reduce the size of the jam. In order to create the empty space, I was temporarily driving about 10 mph below the speed of the heavy traffic. I did this for several minutes, and therefore I was causing a slight slowdown behind me.

After I arrived at the jam, the jam was smaller. When all was said and done, part of the traffic jam had been removed. However, it was changed into a mild slowdown, and it was spread backwards upstream over several miles of traffic. Rather than driving at 50mph only to crawl along through a traffic jam for several minutes, everybody was now driving at 40mph for a few minutes before the jam, but then having a much smaller traffic jam to endure.

The nasty, frustrating part of the 2-mph jam was changed into a large “fuzzy” area of reduced speed. If I had done it correctly, I could have erased the whole jam, transforming it into many minutes of slightly-slow driving for everyone behind me. (If I could have started 30mi upstream of the jam, maybe I would have only needed to drive 3mph slower than traffic.)

“Anti-Traffic”
Here’s a general principle I take from the above. (I guess it’s obvious in hindsight!) ANTITRAFFIC DESTROYS TRAFFIC. Empty spaces can eat a traffic jam. While I was slightly slowing down to allow a space to gradually open up before me, I was creating a pulse of “antitraffic”.

When my antitraffic-pulse finally collided with the dense “traffic” of the jam, the two annihilated each other like a positron meeting an electron. It’s nonlinear soliton physics. The soliton waves destroy each other, leaving only a slight smudge behind.

My next thought: if I took several friends along on my experiment, we could have spaced our cars out over many miles. Each of us could have allowed a big blob of anti-traffic to appear, and then the successive impacts of the antitraffic could have completely erased the traffic jam at the Lynnwood exit.

When traffic is sparse, we cannot keep a large space ahead of us, since it’s easy for cars to pass a slightly-slow driver. But a number of separate drivers could bring smaller spaces along with them, and any traffic jam would succumb to the barrage of “antitraffic.”

Another lesson I learned: plan ahead. Plan WAY ahead. When stuck in traffic jams, I discovered that I cannot to affect them by “peeling out” after I’d made my way through the jam. I hoped to make the far end of the jam dissolve faster.

It never worked because I couldn’t get rid of the slow guy ahead of me. But if I’d planned way ahead and brought an empty space with me into the jam, I could use that space to manipulate the jam. Once I get myself packed in with everyone else, I can do nothing. In order to have an effect, I must behave differently BEFORE the jam, not while trapped inside it.

Ooops! Damn!
Traffic Jams 2
While doing all of the above, I once caught myself behaving normally and creating a huge traffic wave. What a hypocrite! Bad habits die hard.

Traffic was heavy and I was in the left lane. I had to merge across several lanes in order to get to my exit. I merged right once, but the next lane was packed solid (but moving, not jammed.) Nobody would let me in.

I drove like this for a long while, then started driving fairly slow in order to drift backwards along the lane. I found a slot and got in, but now I had to merge right once more. Many minutes had passed, and my exit was coming up. The right lane was packed solid, NOBODY WAS LETTING ME IN. I drove slower and slower, and in a panic I finally forced my way into a small gap, making the guy behind me jam on brakes.

After awhile I realized that I had just created a huge traffic wave with my behavior. Just like any rubbernecker I had suddenly slowed way down. But I had an excuse, I had to get to my exit! To make matters worse, I had nearly come to a stop, and brought two lanes of traffic to a near halt too. I probably left a long-term traffic wave at that spot on the highway. But it wasn’t my fault! Yeah, suuuure.

In stewing about this I realized that EVERYONE has this same problem: an inability to merge in dense traffic. Others were probably doing the same thing that I did, and this would make the “wave” worse and worse. The simple cure is to give up, not merge, and miss the exit. I shouldn’t have forced the issue, I should have let my exit go past. But there is a bigger issue here. People SHOULD be able to merge.

Why was traffic packed so tightly? One obvious reason: to punish the idiots who will jump into any little space. I had always done the same myself. I never allow a space to appear ahead of me or some other driver will immediately fill it in their quest to get a couple of feet of headway. But this sort of driving would also prevent any necessary merges at off ramps (and at on ramps too, of course.) By eliminating the space ahead of me, I become part of the impenetrable wall which creates the “waves” and screws up the traffic at highway ramps.

So, if I keep a few car-lengths of space ahead of me, not only can I use it to help vaporize waves and jams, but it also eliminates one of the major causes of waves and jams. It eliminates the “solid wall” of traffic at merge areas, and lets people merge without slowing down and creating traffic waves. Take a look at this animation.

Ideally, a merge area will act like gear teeth. But if everyone is defending themselves against opportunistic drivers by eliminating all gaps in traffic, then the valid merges cannot take place either. A traffic jam is created. Sometimes the jam is the fault of people like me who panic at missing their exit and come to a complete stop.

Sometimes the jam is the fault of the huge blinking arrow which blocks one entire lane of traffic during construction. But the traffic jam is ALWAYS the fault of those who refuse to let anyone merge ahead of them. “Just merge behind me.” No, that doesn’t work, since everyone in the whole lane is saying the same thing!

Delusions Of Grandeur
Traffic Jams 3
Seattle suffers from many separate rush-hour traffic jams. Why stop with the Lynnwood I-5 jam? With enough people (maybe with cellphones and GPS units), we could intentionally smooth out ALL the traffic jams on all the main Seattle highways!

This is all fantasy at this point. It’s probably illegal for several people to “conspire” to mess with traffic patterns (would we be arrested under a drag-racing law?) And while it is possible for a single driver to have huge effects on traffic patterns, some things can’t be done by a few people.

For example, suppose I want to eat the I-5 traffic jam south of the city. I would have to go all the way to Tacoma, then drive north. But if I tried driving slightly slow, a space would not open up ahead of me because nothing stops other drivers from passing me. In my experiments, I could make “antitraffic” spaces only because traffic was very heavy, and because only a very few people had the ambition to leave their lane and move into the empty space.

Rolling Barriers Made Of State Troopers
OK, so here’s how to dissolve a major interstate traffic jam. Start many miles upstream from the jam. Put a row of State Trooper vehicles across the road and have them drive towards the jam. They drive perhaps at 55 or 50 rather than 70 as everyone else had been driving.

Nobody can get by them, and so all the traffic behind the State Troopers is moving at 55 or so. In front of them, a vast space opens up. After many minutes, the traffic which had been feeding into the traffic jam simply stops arriving. The jam trickles away. Just as the last of it is gone, the row of State troopers arrives, and the jam has been transformed into miles and miles of slightly slow traffic upstream from the old location of the jam.

The situation is not so simple if extra traffic is entering from numerous on-ramps. The “rolling barrier” can’t affect these extra inputs, and if the major portion of the traffic is from on-ramps, then the “rolling barrier” idea would be worthless. Ah, but what about “rubbernecker slowdowns”? A rolling barrier could let the slowdown evaporate, and change it into a wide area of slightly-slow traffic a few miles upstream from the accident.

Would the slowdown re-form? Would rubberneckers hit the brakes and re-create the “traffic standing wave”? I don’t know. Sometimes “rubbernecker slowdowns” persist for hours after the accident has been cleared. This suggests that the slowdown is self-perpetuating. If so, then “erasing” the slowdown might be worthwhile, because once it’s erased, it will only re-form very slowly (or not at all).

If the slowdown normally persists for several hours, yet it only takes half of an hour to erase it, why not erase it? True, the slowdown is not “gone,” since it has become a wide area of slightly slow traffic. However, over many months of slowdown-erasure, this could prevent lots of fender-benders and road-rage incidents, and eliminate thousands of man-years of anger and frustration.

Also, the average speed and traffic throughput on the highway MIGHT actually improve if the region of stopped traffic could be removed. “Removing” the jam just spreads it out and does not immediately alter the average speed.

But the resulting improvements in speed might be more than you’d expect. After all, things are not “linear” in traffic flow, since those who sit at 0 mph for many minutes in a jam cannot compensate by driving at twice the speed limit afterwards. And once a jam is gone, the remaining region of slightly-slowed traffic might disperse fairly rapidly, whereas a traffic jam/stoppage is a different animal and can self-perpetuate once it has formed. And there’s another thing that happens when we spread out a “jam”…

MAKING A REAL DIFFERENCE
Traffic Jams 7
During a year of practicing the “wave-smoothing” driving habits, I kept looking for places where I could make a big difference in traffic flow. Yes, I could always use an empty space to move a piece of the traffic jam to another location. With a big empty space, I could even spread the cars apart as I moved them, the way I did it with the jammed sections in the “traffic wave.” Finally, I saw that there was one common situation where I could do some real good.

If you drive in heavy highway traffic, you’ve probably seen a traffic wave develop at a construction site where one lane is blocked. You crawl and crawl at 3 mph until you get to the bottleneck, then you take your turn merging as the two lanes slooooowly come together. Then you race off at 60 mph! The merging lanes formed a terrible bottleneck. A “traffic wave” develops at (and behind) the merge zone. After the bottleneck, it’s clear sailing.

Why?
WHY must a bottleneck develop at a merge zone? Well, because everyone must take turns. Wrong! Under low-traffic conditions, everyone still takes turns, yet everyone merges at high speed. A bottleneck never appears.

Traffic jams develop at a merge zone whenever the cars get so close together that there are no gaps between them. Without gaps, nobody can merge, and so the traffic comes to a near halt. But whenever traffic comes to a near halt, people always pack themselves together.

Huh. This is screwy. At the place where the lanes merge together, close-packed cars cause the bottleneck, but the bottleneck is the cause of the close-packed cars. But, but… do traffic jams CAUSE THEMSELVES? After thinking about this even more, I realized that the answer is yes. It goes like this:

Traffic is going slow
Everyone packs together and closes up the gaps
Fast merging becomes impossible
Incoming cars create a huge back-up
Cars must slooooowly take turns merging
This makes traffic go slow
Go back to the top of the loop.
This is absolutely fascinating, since this self-caused situation has a counterpart:

No. 2

Traffic flows along rapidly
Nobody closes the gaps (they follow the 2-second rule?)
Merging is easy
Streams of traffic flow together like a zipper
This allows traffic to go fast
Go back to the top of the loop.
At a merge zone, fast traffic causes traffic to be fast, while slow traffic causes a jam. Weird! The difference between these two situations is enormous, yet EITHER ONE can arise on the exact same highway under the exact same amount of traffic.

In the first one, the speed might be 2 mph, while in the second one it could be 40 mph. And here’s the important part: because the situations create themselves once they are established, they can sometimes switch from one to the other. Or somebody can switch them intentionally.

Suppose the traffic at a merge zone was flowing fast as in number 2 above. Suppose I wanted to wreck everything. I could slow way down and make all the cars pack together behind me. This would keep the other lane from merging into the close-packed lane. Cars in the merge-lane would pile up too. Then I drive off laughing evilly, because I have just caused a MASSIVE LONG-TERM TRAFFIC JAM!

Or, I could do the opposite. Suppose everything is jammed up at the merge zone. Suppose I accumulate a huge space ahead of me and bring it into the jam. When the huge space gets there, the other lane can suddenly change lanes, spread out, and start flowing fast.

Next, I speed up and merge with it, and so do the cars behind me. The “zipper-like” flow has begun. The switch has flipped. I have just ERASED a long-term bottleneck. As they say in Seattle, pretty cool, eh?"
13,"Will Robocars Kick Humans Off City Streets?
Self-driving cars could encourage policies that end public access to America’s roads.

By Ian Bogost

Reuters/Edgar Su
JUNE 23, 2016
SHARE
Whenever people go from one place to another, they don’t think much about the roads and sidewalks that pass beneath them. But this infrastructure, known as the public right-of-way, doesn’t work by magic. It is managed and regulated by specific laws. People don’t own the roads they travel on, but streets and sidewalks provide an easement—a right of use or passage separate from that of ownership.

MAKE YOUR INBOX MORE INTERESTING

Each weekday evening, get an overview of the day’s biggest news, along with fascinating ideas, images, and people.

Email Address (required)
Enter your email
Sign Up
THANKS FOR SIGNING UP!

For example: a single-family homeowner usually owns the sidewalk that flanks a property, particularly if that sidewalk falls behind a tree-planting strip that separates it from the street. By local standard or writ, the homeowner grants an easement to the general public to use the sidewalk, utility companies to use the curb where utility lines run, and so forth. Similarly, cities, towns, and the federal government own the roads, but they grant easement to the public to make use of them for transport. The relationships between private and public landowners is managed by local land use and planning agencies. If a builder wants to construct a new driveway from an undeveloped private property to the public right of way, it needs to get permission to do so from the appropriate public agent. That same agent is responsible for maintaining the roads so that they are usable. Some roads, like freeways, don’t offer any private access and restrict public easement to vehicles.


Unfortunately, America’s roads have seen better days. After a massive investment in new infrastructure since the mid-20th century, streets, roads, and freeways have ossified. New roads are tough to build in established cities, and existing ones are increasingly difficult to improve. When roads do get built, they are usually constructed for new development in suburbs or subdivisions. Fear, local resident entrenchment, and lack of funding has hampered adequate upgrades of roads, too. America’s car cities—Los Angeles, Houston, Atlanta, and Dallas, for example—have endured increased congestion when more and more cars travel on roads built for far fewer.

Even when the roads are clear, they are often in poor condition. The economic crisis of 2008 reduced the tax base that supports public service in the near term, but it also inaugurated a prolonged program of social austerity justified by that supposed crisis. And the longer maintenance and support gets deferred, the more complex and costly it becomes to catch up.

The state of public roads might have a surprising and substantial impact on the role of self-driving cars. Cars need roads to drive on, so the technology companies at the center of the robocar revolution might become increasingly invested in them. But roads themselves might become inaccessible to citizens if driving becomes fully autonomous.

*  *  *

If one thing’s clear about cities of the future, it’s that they will become more and more intertwined with the technology business. Public-private partnerships—mergers of private and public funding and support—are helping make costly, unpopular urban development projects more palatable, including tech-enabled transit. Altamonte Springs, a city of 42,000 residents north of Orlando, Florida, for instance, is set to begin offering Uber public subsidies to service its residents. The initiative, which is worth up to half a million dollars a year to the ride-hailing company for starters, effectively turns it into a private provider of public transit.

Eventually, other, larger cities might choose such arrangements to spare themselves the cost and trouble of maintaining public transit services. Such an outcome wouldn’t be entirely new, either. Before the car took over after World War II, public transit was mostly privately owned and operated in America. A return to privatized transit in the form of car-hailing services would make a poetic victory for the automobile over the train and the bus.

A welcome one, too. Citizens are becoming more and more accustomed to the conveniences of app-based services like Uber and Lyft. Google’s parent company, Alphabet, reportedly has 100 people working on future urban design projects at a new entity, Sidewalk Labs.

The roads themselves might become inaccessible to citizens if driving becomes fully autonomous.
It’s not hard to imagine a near future in which municipalities like Altamonte Springs might use the cost savings (or direct investment) of companies like Uber, Lyft, or Google to invest in much-needed maintenance and updates. Once cars become autonomous, the benefit of investments like these will increase even further, since the cost and liability of human drivers can be averted. Better roads mean lower maintenance costs.

RECOMMENDED READING
Animation of a pac-man gobbling up dots and bank-robbers
Why Are Gamers So Much Better Than Scientists at Catching Fraud?
STUART RITCHIE
An upside-down MAGA hat
Meet the Anti-MAGA Trolls
KAITLYN TIFFANY
Computer with screen glitching out
The Internet Is Rotting
JONATHAN ZITTRAIN
Substantial, new infrastructure development might even be impossible without private-sector investment. For example, engineers have been researching future electric motorways that could charge electric vehicles while they drive. These roads would be very expensive in the near-term, but by reducing the weight and complexity of batteries in vehicles, it could pay off in the long term—but only with the leverage of scale. It’s hard to imagine many governments investing in such infrastructure right now, but if tech or automotive companies end up building new fleets of autonomous, electric car-service vehicles in the future, the opportunity to outsource power to the roadways might become appealing enough to “wire” the roads like Google is currently wiring cities for fiber broadband.

Public-private partnerships for roads might begin the erosion of the public right of way. But it’s also possible that autonomous vehicles will all but require limited access to public roads to operate effectively.

Today’s self-driving cars have to be designed and programmed to interact with messy circumstances. Pedestrians, dogs, bicycles, human-driven vehicles, and other obstacles all pose challenges to robocars, and if autonomous vehicles are even modestly successful, avoiding collisions with fallible human drivers will prove a temporary problem.

That transition could take years, and for the moment it is subject to state-by-state rather than federal regulation. But massive private wealth and widespread public approval among tech companies, along with overall regulatory decline in general and explicit federal support for self-driving cars in particular, make it just as likely that robocars will become widespread sooner than anyone expects.

The more self-driving cars there are on the roads, the less complex and more predictable the overall behavior of traffic becomes. One technological model for both semi- and fully-autonomous cars is vehicle-to-vehicle communication. Once installed with GPS, networking, and sensor arrays, cars will be able to communicate with one another directly, allowing for better routing options and increased safety response, such as adaptive accident avoidance. Drivers could use the relief. Thanks in part to smartphone-caused distracted driving—a circumstance created by the same technology companies that might soon offer autonomous vehicles as respite— traffic deaths last year rose at the fastest rate since 1965. Some proponents have predicted that self-driving cars will prevent so many traffic deaths, human-driven cars will be illegal within the next two decades on safety grounds alone.

If cars can get smart, why can’t roads?
The rise of automobiles in the 20th century pushed out pedestrian access in cities almost everywhere in America. Future connected vehicles could perform more sophisticated feats than just replacing human fallibility, ones that might exclude human drivers even from operating vehicles alongside robocars. And with that exclusion, traditional vehicles might meet the same fate in this century that pedestrians did in the last.

Consider intersections. Stop signs, traffic lights, and other common flow control tools give human drivers a common set of conventions for piloting their vehicles. But future autonomous cars would have no real need for them. Networked, computer-controlled vehicles can coordinate with another, evaluating traffic conditions and altering their course in real time to avoid collisions. Like the Lexus-branded pods in Steven Spielberg’s Minority Report, future self-driving vehicles might drive at high speed, coordinating with and around one another in a highly choreographed way. The result might look like the labyrinthine traffic in a chaotic city like Delhi, but without the risk of calamity.

If electric motorways become a reality, it’s even possible that they could be built in such a way that roadways could physically or electronically prohibit traditional vehicles from traversing them. If cars can get smart, why can’t roads? And if roads get smart, it’s likely that they’ll express some of that supposed wisdom through new controls and restrictions.

*   *   *

Once autonomous cars reach a tipping point, cities and the private companies that operate robocar services might voluntarily exclude human drivers from the roads. Not only because doing so could save hundreds of thousands of lives per year, but also because ceding the roads to robots might become a more convenient and economical alternative to sharing the roads with them. Given the opportunity, autonomous car services will likely be willing to invest heavily in redeveloping urban infrastructure that would give them an economic advantage in serving the populations that live there. Especially if it would mean a monopoly on local transit. The results could change city life entirely.

Some consequences of widespread robocar transit are clear already. Parking lots and structures are commonly cited future casualties of the future rise of autonomous cars. As Clive Thompson wrote in Mother Jones earlier this year, “many urbanists predict that fleets of robocars could become so reliable that many, many people would choose not to own automobiles, causing the amount of parking needed to drop through the floor.” Nobody likes parking lots, and it’s possible that robocars could inaugurate an era of redevelopment for these urban blights.

But if parking lots are the organs of the urban creature, roads are its circulatory system. They are the primary means by which people move from place to place—whether they do so by car, bus, foot—or even streetcar or light rail. No matter how hypothetical, the potential eviction of human access to roads ought to raise far more eyebrows than the philosophical gimmickry of robocar Trolley Problems.

Surely not all roads would be made robot-only at once. Perhaps freeways and major thoroughfares would do so first. But a city is a network of roads, and the shift from shared to computerized streets would spread. Roads connect to other roads, such that the transition from hybrid passageways to fully autonomous ones would imply an alteration to the urban landscape not yet conceived. It might be easier just to ban people from roads entirely—even secondary roads and residential side-streets. After all, that’s also how the car took over from the train and the sidewalk in the United States.

The possibility of being locked out of public rights of way is a problem somewhat unique to America. Dense European cities like Paris and London are already criss-crossed by public rail transit and relatively small in physical size. It’s easy and even pleasant to walk across Paris or Barcelona. Crossing Houston or Atlanta by foot or rail isn’t just unpleasant; it’s all but impossible.

Unless the nascent renaissance of pedestrian-friendly urbanism expands significantly and rapidly, the self-reliance cars have provided for decades could reverse course. Instead of offering freedom to maneuver, cars would yoke citizens to the services of oligopolistic technology transit companies.

*   *   *

There are reasons beyond pride and tradition to worry about future autonomous car service providers’ impact on urban life. Social justice, for one. There’s evidence that gig economy companies like Uber and AirBnB make it easier to discriminate against racial minorities, disabled people, and other disadvantaged populations.

American streets have long been a site for racial strife, whether through the federal endorsement of housing prejudice known as redlining, via urban planning that further segregated minority neighborhoods from economic centers, or from the white flight and dissent that stymied the development or expansion of public transit networks in the civil rights era. Once wealthy, non-local technology companies have a direct impact on the financing and planning of urban spaces, the negative impact on less white, less wealthy communities could be severe.

The past offers lessons for the future in this regard. In Atlanta, where I live, urban planners of the mid-20th century effectively zoned the city into distinctly separate black and white areas. As public transit and economic development-driven investments like stadia entered the picture, black communities were physically cut off from white ones. Streets were dead-ended and renamed, hampering navigational continuity. The effects of these choices continue today, invisibly. They would become even more entrenched if the  publicly-passable thoroughfares that already segregate the city were made impassible except by robocar. Historically, thoroughfares have already helped isolate neighborhoods by race and class. If those arterials were impassible like freeways, their divisive effect would only intensify.

There are also economic concerns. We already know that Uber and Lyft are overwhelmingly used by the wealthy and well-educated. Those services are still relatively expensive compared to public transit, even if they are often more convenient. With the exception of private property that explicitly limits public easement, roads and sidewalks currently can be accessed free-as-in-beer as much as free-as-in-speech. But in a hypothetical future in which local municipalities enter into arrangements with future versions of Google or Uber, that access might come at a price. It’s easy to imagine that cross-town transit might soon require acceptance of non-negotiable terms of service that would allow your robocar provider to aggregate and sell where you go, when, with whom, and for what purpose.

Other, stranger realities are possible. Imagine if walking across the street required a microtransaction to insure safe passage. Violations might be subject to tickets or fines—although more likely, your local transit vendor would already know where you are thanks to your smartphone, and just debit your metered service plan accordingly. (Cities like Los Angeles and Honolulu, which impose steep fines for jaywalking, offer a prototype. The car industry has a long history lobbying against jaywalking.) And imagine if Google or Apple, which already provide most folks with mapping services, also got into the business of conveying those citizens through cities. Maps might become accessible only by the good graces of car services. Perhaps you could buy a subscription for unlimited access to your block or neighborhood. The materially fortunate might spring for an all-access plan—the only way to see a whole-city street view.

When Silicon Valley runs our automobiles, they’ll do so according to the business practices of the technology industry.
If this sounds preposterous, consider the fact that tech companies tend to spurn negotiation with local communities if they don’t get their way. Just as Uber and Lyft pulled out of Austin to protest an undesirable regulatory hurdle, Google or Apple might restrict access to their mapping services in areas that don’t adopt political positions convenient to their corporate interests.

They might even elect to alter the physical environment accordingly. In America, street signs are yoked to signage built for human drivers, mounted atop traffic signals and stop signs. Once those devices aren’t needed, their attached markings might also disappear. Perhaps tech giants could persuade municipalities to remove street signs and markers to realize cleaner, less distracting urban conveyance via the synergy of app-street-and-car transit networks.

No matter the scenario, one thing’s for sure: When Silicon Valley runs our automobiles, they’ll do so according to the business practices of the technology industry, not according to the principles of local urban planning and civic life.

*   *   *

The communities most resilient to whatever changes autonomous cars might bring to the public right of way are likely to be those that wean themselves from cars in the meantime. Here, American urban planners and citizens alike can learn from recent trends in Europe, where the relative powerlessness of the automobile has allowed cities to suppress and even reverse its advance. As my colleague Adrienne LaFrance reported last year, Oslo is trying to ban cars entirely from its downtown by 2019. And Barcelona hopes to combat pollution and noise with nine-block “superblocks,” neighborhoods that are bounded by car-accessible streets, but which reserve internal passage for citizen use.


With a few notable exceptions, American cities will never be dense and small enough to adopt European approaches to urban planning. But efforts to make sprawling U.S. cities denser, more walkable, and less reliant on automobiles are more popular than they have been since the car and the suburb took over for the train and the city-center after World War II. Increasing safe access to major roads for bicycles and walkers, for example, might offer an unexpected benefit should the robocarpocalypse arrive: Neighborhoods with deliberately-built, well-maintained paths for non-automotive transit will retain the freedom of entrance and egress that the car currently insures everywhere. And a renewed interest in denser, mixed-use zoning and development has created more opportunities for superblock-like communities in car-centric cities like Atlanta and Houston. Even so, the automobile still rules these towns, and especially in the cheaper, more sprawling suburbs and exurbs where a majority of metropolitan-area inhabitants still reside. Avoiding the roboway entirely might prove impossible.

When imagining a future with self-driving cars, the impact on the immediate future seems most urgent. For the moment, safety and liability issues concern the public most. But even if imperfect, autonomous vehicles are already much safer than human-driven ones. When it comes to robots on the roadways, city dwellers would be better off worrying about their future access to the roads in the first place. Eventually, those citizens might enjoy a new kind of freedom, that of avoiding owning and operating automobiles altogether. But in exchange, they might also give up freedom to move around the city without the help of a few, large global technology companies. Then the suburbs might reverse their fortune and become desirable once more. Not as a place to live, but as a place to escape and move about freely, without the intervention of Big Robocar. That is, if you can still get there in the first place.

Ian Bogost is a contributing writer at The Atlantic and the Ivan Allen College Distinguished Chair in Media Studies at the Georgia Institute of Technology. His latest book is Play Anything."
14,
15,"Authenticity in the Age of the Fake
As science blurs the real and unreal, we are learning to distinguish them in new ways.
BY PHILIP BALL
NOVEMBER 24, 2016
 ADD A COMMENT FACEBOOK TWITTER EMAIL SHARING REDDIT STUMBLEUPON TUMBLR POCKET
The announcement of synthetic diamonds in 1955 was met with the same kind of alarm and skepticism that greeted claims to have made alchemical gold in the Middle Ages. Could these “fake” gems, created by a team at the General Electric research laboratories in Schenectady, New York really match the genuine article? One anonymous critic from California captured a widespread suspicion in blunt terms when he wrote to GE, saying:

You can’t make real diamonds for they are nature grown. You can’t make gold; no one can. They dig gold out of the ground and also diamonds. But no one can make them with a machine. That is just a lot of bull.

Yet what if it were true that diamonds really can be manufactured? When GE revealed the discovery, the stock of the De Beers diamond cartel in South Africa, which dominated the global market, plummeted. It seemed like a rare and precious commodity was about to be supplanted by an artificial form that could be fabricated by the ton, mirroring a millennia-old concern about the devastating power of fakes. Concerns over the devaluation of gold currency led the Roman emperor Diocletian to ban alchemy in the third century, and worries about counterfeiting and debased coinage also lay behind the condemnations of the art by Pope John XXII in 1317 and of King Henry IV of England in 1403.

This, though, was no alchemy: The GE diamonds were perfect chemical replicas of the real thing. Was it the end of a billion-dollar market?

Ball_BR_synth-diamond2
ALL THAT GLITTERS: A rose-cut synthetic diamond created using a chemical vapor deposition process.Wikipedia
The answer was no. “Fake” diamonds are cheaper, and for industrial uses they have utterly eclipsed their natural counterparts. But at the luxury end of the market—gemstones for jewelry—artificial diamonds account for only 2 percent of global sales. How come?

When it comes to luxury and exotic materials, the competition between fake and real is partly a technical, chemical affair: how to create a good imitation, and how to spot it. But, as artificial gold and diamonds show, there is a deeper level to it, which is about something very human and socially constructed: the concept and value of authenticity.

Sapolsky_TH-F1
ALSO IN CHEMISTRY  
The Dawn of Life in a Toaster Oven
By Johnny Bontemps

God might just as well have begun with a toaster oven. A few years ago at a yard sale, Nicholas Hud spotted a good candidate: A vintage General Electric model, chrome-plated with wood-grain panels, nestled in an old yellowed box,...READ MORE



There are few better examples of the social implications of chemical “fakery” than the synthetic dyes of the 19th century. Medieval and Renaissance cultures were virtually color-coded hierarchies. Crimson and scarlet garments were for cardinals, bishops, popes, and monarchs, echoing the ruby-purple of the emperor’s robes in classical Rome. Clothing displaying other rich colors was a mark of wealth; black in particular came to signify the conspicuous consumption of the affluent merchants, who could afford cloth dyed in several expensive dyes until it took on this somber shade. Cheap substitutes would fool no one—they faded fast in sunlight or washed quickly from the threads.

Purple, with its associations of antique nobility, was especially resonant. But how challenging it was to make! When the wife of Napoleon III, the glamorous Empress Eugénie, visited England in 1858 dressed in a crinoline made from swathes of glorious mauve-hued cloth, the English went crazy to imitate this display of continental haute couture. Even Queen Victoria rather dowdily followed suit. The timing could not have been better for the young chemist William Perkin, who discovered in 1856 how to make a strong and permanent purple dye from an extract of the abundant coal tar, the residue from the extraction of gas from coal. Tempted at first to call it Tyrian purple to capitalize on the exclusive associations of antiquity, Perkin soon realized that nowadays French fashion counted for more, and it became “mauve,” after the French word for mallow, a plant with purple flowers.

Ball_BR_mauve-dress
PURPLE FOR THE PEOPLE: This silk dress is dyed with Sir William Henry Perkin’s original mauve aniline dye. The purple, even though it has faded, has a richness unachievable by natural dyes.SSPL/Getty Images
This too seemed a kind of alchemy, as Charles Dickens’ journal All The Year Round acknowledged in 1859:

Alchemists of old spent their days and nights searching for gold, and never found the magic Proteus, though they chased him through all gases and all metals. If they had, indeed, we doubt much if the discovery had been as useful as this of Perkins’s [sic] purple.

But the disdain expressed toward the garish colors that Perkin’s purple, and related dyes such as magenta, made accessible to the general public from the 1860s had within it a clear distaste for the arriviste. Now that simply anyone can wear what once marked you out as a person of consequence meant that new ways were needed to arbitrate social distinction. “Never trust a woman who wears mauve, whatever her age may be,” wrote Oscar Wilde in The Picture of Dorian Gray. “It always means they have a history.”

With chemistry in full bloom in the later 19th century, all the material signifiers of class and culture were under threat. Expensive ivory, allegedly in short supply in the mid-century (although the rumor was false), could be faked with nitrocellulose, a plastic discovered in 1845 by the Swiss-German chemist Christian Schönbein. The British inventor and metallurgist Alexander Parkes turned this sticky mass into a hard substance that he patented as Parkesine in 1862, selling it as an ivory substitute for combs, buttons, and cutlery handles. Parkes’ product was poor—it cracked and caught fire—and his business folded in just a few years. But at much the same time, the American brothers John and Isaiah Hyatt found that nitrocellulose could be made malleable and workable by adding castor oil or camphor, which they marketed with more success as celluloid.

Ball_BR_billiards
EXPLODING BILLIARDS: This billiard ball, a gift of the Celanese Corporation, is made of cellulose nitrate, or celluloid. Billiard balls made of celluloid would occasionally detonate on contact.National Museum of American History, Smithsonian Institution
It wasn’t the perfect substitute by any means, not least because it is so flammable: nitrocellulose was also used as a fluffy explosive called “gun cotton.” There are tales of celluloid billiard balls, masquerading as ivory, producing a sharp detonation on contact: not enough to do anyone harm, although John Hyatt attested that the loud cracks might lead to guns being drawn in crowded billiard bar-rooms. The use of unstable nitrocellulose as an enamel substitute in dentures was an even riskier venture.

Celluloid was semi-natural, a plastic made from plant cellulose. The era of truly synthetic plastic began with the invention of Bakelite by Leo Baekeland around 1907. This dark resin was a polymer made from phenol (another coal-tar extract) and formaldehyde. It could be easily molded, but once hardened it retained its shape even when heated. This property, and its electrically insulating character, made it well suited for making components in the growing electronics industry. But the dark sheen resembled amber and mahogany, and so it was soon to be found imitating these expensive materials in jewelry, handles, and household devices such as radio sets. Still today buyers on eBay fret over how to distinguish real amber from Bakelite imitations.

Fakes can now be nearly as complex as the objects they copy.

The initial promise of plastics, then, was to provide ‘luxury for all’: materials resembling those only the wealthy had previously been able to afford. They were egalitarian materials: as cultural critic and philosopher Roland Barthes put it, “their use is historically bourgeois in origin … they aimed at reproducing cheaply the rarest substances, diamonds, silk, feathers, furs, silver, all the luxurious brilliance of the world.”

But if individuals of low status can pass themselves off as high-status—richly garbed, wearing faked gold and jewels—what then can survive of the social order? We might have moved beyond the hierarchy of princes, lords, and bishops, but not beyond the need for a pecking order and a narrative to back it up. It’s to that narrative that we returned in the age of the fake, both to create new emblems of status and to defend old ones.



Mixed up with the human code of privilege and power is an ancient belief in the moral authority of nature’s divine handywork. “Making” is always a compromised, ambiguous affair; beyond even the rhyming coincidence with faking, we have a word in English that can connote either activity: forging. It seemed entirely consistent with the superiority of God’s creations when the 17th-century pioneer of microscopy Robert Hooke found natural objects (such as insects) to be far more intricately and delicately formed at the tiniest scales than the crude artifacts of humans.

Modern science has shifted this balance. Fakes can now be nearly as complex as the objects they copy (or in the case of fake diamonds, essentially identical). One response to this intrusion is to move the boundary deeper into nature, looking for subtler marks of authenticity: Real and fake diamonds tend to have different spectroscopic signatures of light absorption, while X-ray crystallography, chromatography, and mass spectrometry are the high-tech methods used for detective work on pigments to authenticate artworks.

Another response to the increasing perfection of the fake, though, is to change the narratives behind authenticity and desirability—to assert them on different terms. Sometimes the mere passage of time is enough to elevate the fake itself to an object of desire. The “retro” value of genuine Bakelite products, for example, now makes them sought-after, and liable to be forged from more recent, cheaper plastics. The fake, invested with its own unique history, has become the real deal, and fetches a premium price to match. Items of “vintage bakelite” jewelry are listed on the online auction site eBay for thousands of dollars, despite the fact that they are essentially plastic trinkets.

Ball_BR_ebay.
THE NEW AUTHENTIC: A bakelite bracelet selling for $2400 on eBay.
Alternatively, the fake might itself stake a claim to moral authority, as in the case of fake fur, or of fake rhino horn, created using 3-D printing technology in an effort to ease poaching of the endangered animals.

The diamond industry, aware of the flexible nature of how we assign authenticity, is trying to re-invent the narrative surrounding its product. Concerned that synthetic diamonds are starting to enter the high-end jewelry market (sometimes surreptitiously), a cartel of natural diamond mining companies, including De Beers, called the Diamond Producers Association has launched a campaign to promote its products. The rhetoric—“Real is Rare”—speaks volumes about today’s distinctions between “real” and “fake”: If you really love someone, the campaign videos insist, you’ll settle for giving them nothing but a “real” gem. A fake gem becomes a sign of a faked affection. That the artificial gems are probably at this point still more rare than natural ones, and are as “real” a diamond as any other in chemical terms, is beside the point here. The “genuine article” is being protected by a narrative of natural authenticity that—ironically, given the pollution and questionable human-rights records of past diamond mining—draws from that associated with green movements.


REAL IS RARE: An association of diamond mining companies is producing ads promoting the virtues of real diamonds, with the tagline “real is rare.”Real is Rare, Real is a Diamond.
Manufacturing such large diamonds is not easy in any case. As well as GE’s high-pressure, high-temperature (HPHT) method that squeezes carbon to diamond, it’s now possible to grow diamond by letting carbon atoms settle from a vapor onto a growing diamond surface, a technique called chemical vapor deposition (CVD). Initially HPHT synthesis could produce only very small diamonds—fine for making abrasive grit, not so much for necklaces. And CVD was best suited to making diamond coatings, which again was fine for cutting tools but not jewelry. Today it’s feasible to grow larger, gem-quality diamonds artificially, but at a price scarcely much lower than that of natural diamonds. And it’s not easy to make these artificially gems perfectly colorless—impurities can give them a yellow, brown, or gray tint. Both for these reasons and our apparent continued belief in the value of a “real” diamond, natural diamond retains almost complete control over the gem market.

The superiority of the “nature grown” substance is of course a strong selling point for health foods and “natural” cosmetic and pharmaceutical products. That isn’t to suggest such claims are totally fictitious: Some of the artificial food colorings that stemmed from William Perkin’s coal-tar dyes really were toxic. But the narrative often insinuates an almost moral authority of the “real” over the “fake.”

Witness the arguments over natural versus synthetic vitamins. “Many vitamin and mineral supplements are manufactured synthetically with chemicals and do not come straight from their natural sources,” one web site, www.GlobalHealingCenter.com, warns:

They are made to mimic the way natural vitamins act in our bodies. Natural vitamins are derived directly from plant material containing the vitamin, not produced in a test tube … Evolution has dictated that we eat the food we can gather from the earth, not the food we create in a lab.

Other sites determined to defend the superiority of natural provenance warn that, even if the molecules are identical, they are synthesized by a different pathway, using different reagents, than those in nature.

It’s true that some claim the natural vitamins—but not the synthetic variants—come with a tranche of enzymes, ions, minerals, and other ingredients that control the way the body recognizes, metabolizes, and uses them. Others say that a synthetic variant might contain impurities that the natural compounds do not. But if these things are truly important for the physiological effects, they can be addressed in principle. The rhetoric, however, sets out to persuade us that a synthetic substance can never be equivalent to a “natural” one, however much they might look identical. And it’s a story that works: Whole-food supplements, which surely qualify as luxury items too, have been one of the most quickly growing segments in the industry.

This distrust of the “fake” and reverence for the “real” and “authentic” seems to be deeply ingrained in human nature. We will find a rationalization of it, one way or another, and construct narratives to keep the distinction alive. For in the end that distinction seems to look not outward to the material world but inward to the self. Philosophers from Jean-Jacques Rousseau to Jean-Paul Sartre have interpreted the idea of authenticity in personal, individualistic terms as a freely chosen response to circumstances, something like Polonius’ dictum in Hamlet: “To thine own self be true.” Psychological studies suggest that sense of personal authenticity inflects how we function in the world: our sense of autonomy and self-esteem, what goals we pursue, how we relate to others. We reject the fake smile, the insincere action; we disdain artifice and pretense. It’s entirely to be expected, then, that the policing of fakery is socially constructed. What matters is not so much how the object is constituted, what chemistry created it, what imperfections it has. What matters is what it says about us.



Philip Ball is a writer based in London. His latest book is The Water Kingdom: A Secret History of China."
16,
17,"The New Abolitionism
Averting planetary disaster will mean forcing fossil fuel companies to give up at least $10 trillion in wealth.
By Chris HayesTwitter. Before the cannons fired at Fort Sumter, the Confederates announced their rebellion with lofty rhetoric about “violations of the Constitution of the United States” and “encroachments upon the reserved rights of the States.” But the brute, bloody fact beneath those words was money. So much goddamn money.

The leaders of slave power were fighting a movement of dispossession. The abolitionists told them that the property they owned must be forfeited, that all the wealth stored in the limbs and wombs of their property would be taken from them. Zeroed out. Imagine a modern-day political movement that contended that mutual funds and 401(k)s, stocks and college savings accounts were evil institutions that must be eliminated completely, more or less overnight. This was the fear that approximately 400,000 Southern slaveholders faced on the eve of the Civil War.

Today, we rightly recoil at the thought of tabulating slaves as property. It was precisely this ontological question—property or persons?—that the war was fought over. But suspend that moral revulsion for a moment and look at the numbers: Just how much money were the South’s slaves worth then? A commonly cited figure is $75 billion, which comes from multiplying the average sale price of slaves in 1860 by the number of slaves and then using the Consumer Price Index to adjust for inflation. But as economists Samuel H. Williamson and Louis P. Cain argue, using CPI-adjusted prices over such a long period doesn’t really tell us much: “In the 19th century,” they note, “there were no national surveys to figure out what the average consumer bought.” In fact, the first such survey, in Massachusetts, wasn’t conducted until 1875.

Anti-Asian Racism and the 2020 Olympic Games
In order to get a true sense of how much wealth the South held in bondage, it makes far more sense to look at slavery in terms of the percentage of total economic value it represented at the time. And by that metric, it was colossal. In 1860, slaves represented about 16 percent of the total household assets—that is, all the wealth—in the entire country, which in today’s terms is a stunning $10 trillion.

Ten trillion dollars is already a number much too large to comprehend, but remember that wealth was intensely geographically focused. According to calculations made by economic historian Gavin Wright, slaves represented nearly half the total wealth of the South on the eve of secession. “In 1860, slaves as property were worth more than all the banks, factories and railroads in the country put together,” civil war historian Eric Foner tells me. “Think what would happen if you liquidated the banks, factories and railroads with no compensation.”

* * *

In 2012, the writer and activist Bill McKibben published a heart-stopping essay in Rolling Stone titled “Global Warming’s Terrifying New Math.” I’ve read hundreds of thousands of words about climate change over the last decade, but that essay haunts me the most.

The piece walks through a fairly straightforward bit of arithmetic that goes as follows. The scientific consensus is that human civilization cannot survive in any recognizable form a temperature increase this century more than 2 degrees Celsius (3.6 degrees Fahrenheit). Given that we’ve already warmed the earth about 0.8 degrees Celsius, that means we have 1.2 degrees left—and some of that warming is already in motion. Given the relationship between carbon emissions and global average temperatures, that means we can release about 565 gigatons of carbon into the atmosphere by mid-century. Total. That’s all we get to emit if we hope to keep inhabiting the planet in a manner that resembles current conditions.

Now here’s the terrifying part. The Carbon Tracker Initiative, a consortium of financial analysts and environmentalists, set out to tally the amount of carbon contained in the proven fossil fuel reserves of the world’s energy companies and major fossil fuel–producing countries. That is, the total amount of carbon we know is in the ground that we can, with present technology, extract, burn and put into the atmosphere. The number that the Carbon Tracker Initiative came up with is… 2,795 gigatons. Which means the total amount of known, proven extractable fossil fuel in the ground at this very moment is almost five times the amount we can safely burn.

Proceeding from this fact, McKibben leads us inexorably to the staggering conclusion that the work of the climate movement is to find a way to force the powers that be, from the government of Saudi Arabia to the board and shareholders of ExxonMobil, to leave 80 percent of the carbon they have claims on in the ground. That stuff you own, that property you’re counting on and pricing into your stocks? You can’t have it.

Given the fluctuations of fuel prices, it’s a bit tricky to put an exact price tag on how much money all that unexcavated carbon would be worth, but one financial analyst puts the price at somewhere in the ballpark of $20 trillion. So in order to preserve a roughly habitable planet, we somehow need to convince or coerce the world’s most profitable corporations and the nations that partner with them to walk away from $20 trillion of wealth. Since all of these numbers are fairly complex estimates, let’s just say, for the sake of argument, that we’ve overestimated the total amount of carbon and attendant cost by a factor of 2. Let’s say that it’s just $10 trillion.

The last time in American history that some powerful set of interests relinquished its claim on $10 trillion of wealth was in 1865—and then only after four years and more than 600,000 lives lost in the bloodiest, most horrific war we’ve ever fought.

It is almost always foolish to compare a modern political issue to slavery, because there’s nothing in American history that is slavery’s proper analogue. So before anyone misunderstands my point, let me be clear and state the obvious: there is absolutely no conceivable moral comparison between the enslavement of Africans and African-Americans and the burning of carbon to power our devices. Humans are humans; molecules are molecules. The comparison I’m making is a comparison between the political economy of slavery and the political economy of fossil fuel.

More acutely, when you consider the math that McKibben, the Carbon Tracker Initiative and the Intergovernmental Panel on Climate Change (IPCC) all lay out, you must confront the fact that the climate justice movement is demanding that an existing set of political and economic interests be forced to say goodbye to trillions of dollars of wealth. It is impossible to point to any precedent other than abolition.

The connection between slavery and fossil fuels is more than metaphorical.

* * *

The connection between slavery and fossil fuels, however, is more than metaphorical. Before the widespread use of fossil fuels, slaves were one of the main sources of energy (if not the main source) for societies stretching back millennia. Prior to the Industrial Revolution, nearly all energy to power societies flowed from the natural ecological cascade of sun and food: the farmhands in the fields, the animals under saddle, the burning of wood or grinding of a mill. A life of ceaseless exertion.

Before fossil fuels, the only way out of this drudgery was by getting other human beings to do the bulk of the work that the solar regime required of its participants. This could be done by using accrued money to pay for labor, but more often than not—particularly in societies like the Roman Empire that achieved density and scale—it was achieved through slavery. Slavery opened up for the slave owners vast new vistas of possibility. The grueling mundane exertions demanded of everyone under a solar regime could be cast off, pushed down on the shoulders of the slave.

In this respect, the basic infrastructure of energy distribution and exploitation in the plantation South was not so different from feudal Europe or ancient Egypt. During the first half of the nineteenth century, coal, whale oil, pneumatic power and all manner of mechanization penetrated the more urbanized North, while the South remained largely mired in the pre-industrial age. In 1850, only 14 percent of the nation’s canal mileage and 26 percent of its railroad mileage ran through slave states, and the industrial output of the entire region was only one-third that of Massachusetts alone.

Not only that, but as time marched forward, the South lagged further and further behind. In Battle Cry of Freedom, James McPherson notes that while in 1850 slave states had 42 percent of the population, they “possessed only 18 percent of the country’s manufacturing capacity, a decline from the 20 percent of 1840.” The same holds true for the South’s percentage of railroad miles, which was declining as the war approached. In 1852, James D.B. DeBow, a vociferous advocate of diversifying the Southern economy, lamented that “the North grows rich, and powerful, and great, whilst we, at best, are stationary.” (This underdevelopment would haunt the South well into the twentieth century: in 1930, only 38 percent of residents of the former Confederate states had electricity, compared with about 85 percent in states that had been free.)

Slavery was a kind of crutch, giving the South its own version of what development economists now call a ""resource curse.""

This lagging wasn’t just happenstance: many historians argue that it was, in fact, the availability of the cheap, plentiful energy resource of slavery that meant the South faced less pressure to urbanize, electrify or industrialize. Slavery, and the energy it provided, was a kind of crutch giving the antebellum South its own version of what modern-development economists now call, in a very different context, a “resource curse”—that is, an overreliance on a resource (in this case, enslaved human beings) that stunts economic diversification and development.

Crucially, as slavery became more profitable to the planter class and ever more central to the economic health of the South, the ideas about slavery grew increasingly aggressive, expansionist and reactionary. “Very few people at the time of the Revolution and the Constitution publicly affirmed the desirability of slavery,” Foner observes. “They generally said, ‘We’re stuck with it; there’s nothing we can do.’”

Even in much of the South, slavery was at first seen as a necessary evil, a shameful feature of the American experience that would necessarily be phased out over time. Many slave-owning founders shared in this consensus. Slave owner and Virginian Patrick Henry referred to slavery in a private letter as an “abominable practice…a species of violence and tyranny” that was “repugnant to humanity.” His fellow Virginian Richard Henry Lee called the slave trade an “iniquitous and disgraceful traffic” in 1759 while introducing a bill to try to end it. Thomas Jefferson, at times an ardent defender of slavery and the white supremacy that undergirded it, confessed in 1779 that “the whole commerce between master and slave is a perpetual exercise of the most boisterous passions, the most unremitting despotism on the one part, and degrading submissions on the other.”

When Jefferson wrote those words, slavery had nowhere near the economic grip on the South that it would have during the cotton boom in the first half of the nineteenth century. Between 1805 and 1860, the price per slave grew from about $300 to $750, and the total number of slaves increased from 1 million to 4 million—which meant that the total value of slaves grew a whopping 900 percent in the half-century before the war.

This increase in the price of slaves was due largely to two factors. In 1808, the Act Prohibiting Importation of Slaves took effect, permanently constraining supply. From then on, all new slaves came as the offspring of existing slaves. And then there was cotton. It’s hard to overestimate the impact that cotton had on the South during the decades leading up to the war. No place on earth produced more cotton, and the world’s demand was insatiable. Economic historian Roger L. Ransom writes that “by the mid-1830s, cotton shipments accounted for more than half the value of all exports from the United States.” So lucrative was the crop that the planter class rushed into it, leaving behind everything else. As McPherson notes, per capita production of the South’s principal food crops actually declined during this period.

All of this led to a heady kind of triumphalism. In 1858, Senator James Henry Hammond, a South Carolina plantation owner, took to the floor of the Senate to inquire mockingly:

What would happen if no cotton was furnished for three years? I will not stop to depict what every one can imagine, but this is certain: England would topple headlong and carry the whole civilized world with her, save the South. No, you dare not make war on cotton. No power on earth dares to make war upon it. Cotton is king.

It is perhaps not surprising that under conditions of stupendous profit and accumulation, the rhetoric of the South’s politicians and planter class changed to a florid celebration of the peculiar institution. “By the 1830s, [John C.] Calhoun and all these guys, some of them go so far as to say, ‘It would be better for white workers if they were slaves,’” Foner tells me. “They have a whole literature on why slavery should be expanded.” Indeed, here’s Calhoun in 1837:

I hold that in the present state of civilization, where two races of different origin, and distinguished by color, and other physical differences, as well as intellectual, are brought together, the relation now existing in the slaveholding States between the two, is, instead of an evil, a good—a positive good.

Here’s Hammond in the same “Cotton is king” speech, playing the same notes:

In all social systems there must be a class to do the menial duties, to perform the drudgery of life. That is, a class requiring but a low order of intellect and but little skill. Its requisites are vigor, docility, fidelity. Such a class you must have, or you would not have that other class which leads progress, civilization, and refinement…. Fortunately for the South, she found a race adapted to that purpose to her hand. A race inferior to her own, but eminently qualified in temper, in vigor, in docility, in capacity to stand the climate, to answer all her purposes. We use them for our purpose, and call them slaves.

“Our negroes,” according to Southern social theorist George Fitzhugh, “are not only better off as to physical comfort than free laborers, but their moral condition is better…. [They are] the happiest, and, in some sense, the freest people in the world.”

So the basic story looks like this: in the decades before the Civil War, the economic value of slavery explodes. It becomes the central economic institution and source of wealth for a region experiencing a boom that succeeded in raising per capita income and concentrating wealth ever more tightly in the hands of the Southern planter class. During this same period, the rhetoric of the planter class evolves from an ambivalence about slavery to a full-throated, aggressive celebration of it. As slavery becomes more valuable, the slave states find ever more fulsome ways of praising, justifying and celebrating it. Slavery increasingly moves from an economic institution to a cultural one; it becomes a matter of identity, of symbolism—indeed, in the hands of the most monstrously adept apologists, a thing of beauty.

And yet, at the very same time, casting a shadow over it all is the growing power of the abolition movement in the North and the dawning awareness that any day might be slavery’s last. So that, on the eve of the war, slavery had never been more lucrative or more threatened. That also happens to be true of fossil fuel extraction today.

* * *

THE NATION IS READER FUNDED. YOUR SUPPORT IS VITAL TO OUR WORK.

DONATE NOW!

America is in the grip of a fossil fuel frenzy almost without precedent. By 2015, the United States is projected to surpass Saudi Arabia as the largest producer of oil in the world. After sixty years of being a net importer of fuel, we are now a net exporter, and it’s possible that we will break our 1970 record for peak oil production. This comes thanks to both deepwater drilling and shale fields like the Bakken formation in North Dakota, whose previously inaccessible reserves have been unlocked by horizontal drilling and hydraulic fracturing technologies, also known as “fracking.”

These same technologies have also produced an unprecedented natural gas surge, as fracking wells are sunk into the soil of ranches and parks and hillsides across the country. Pennsylvania’s Marcellus Shale alone produces about 14 billion cubic feet of natural gas per day—the equivalent of more than 2.4 million barrels of oil. Shale extraction has quadrupled in the past four years and now accounts for about 40 percent of the annual natural gas yields in the United States, which recently surpassed Russia as the world’s largest natural gas producer.

At the very same time that extraction has come to play an increasingly dominant role in the US economy, we have seen a dramatic reversal in the politics of fossil fuel and climate change. Whereas high-profile Republicans once expressed ambivalence about our reliance on fossil fuels, viewing it as a kind of necessary evil that would ultimately be phased out, in the last five years the extraction of fossil fuels has become—to steal a phrase—“a positive good.”

During the 1988 vice-presidential debate, Dan Quayle argued that “the greenhouse effect is an important environmental issue. It’s important for us to get the data in, to see what alternatives we have to the fossil fuels…. We need to get on with it, and in a George Bush administration, you can bet that we will.”

That wasn’t quite the case, but in 1989, Newt Gingrich was one of twenty-five Republican co-sponsors of the Global Warming Prevention Act, which held that “the Earth’s atmosphere is being changed at an unprecedented rate by pollutants resulting from human activities, inefficient and wasteful fossil fuel use, and the effects of rapid population growth in many regions” and that “increasing the nation’s and world’s reliance on ecologically sustainable solar and renewable resources…is a significant long-term solution to reducing fossil-generated carbon dioxide and other pollutants.” In 1990, President George H.W. Bush said at an IPCC event, “We all know that human activities are changing the atmosphere in unexpected and in unprecedented ways.”

While his son did little to curb carbon emissions when he took his turn at the presidency, he did at least give it lip service. Speaking ahead of the 2005 G8 Summit, George W. Bush said, “It’s now recognized that the surface of the earth is warmer, and that an increase in greenhouse gases caused by humans is contributing to the problem.” As part of the 2007 Energy Independence and Security Act, he signed into law minimum efficiency requirements to begin to phase out the use of incandescent bulbs in 2012. (A law that would, in the Obama era, become a top conservative target, as the Tea Party rallied to support the incandescent bulb as if it were a constitutionally enshrined right.)

And in 2008, somewhat miraculously, John McCain’s platform featured support for a cap-and-trade bill that would have effectively put a price on carbon. But even by that year, you could already feel a seismic shift in the rhetoric. I sat in the Xcel Energy Center in St. Paul in 2008 and watched Sarah Palin lead thousands of people in a thunderous chant of “Drill, baby, drill!”

After Obama’s election, things moved quickly: McCain dropped support for his own legislation to regulate carbon pollution. In 2010, Bob Inglis, a conservative congressman from South Carolina, was soundly defeated by a Tea Party challenger in the Republican primary, due chiefly to Inglis’s refusal to deny the science on climate change. A year later, Gingrich called his appearance alongside Nancy Pelosi in a 2008 ad urging action on climate change the “dumbest single thing I’ve done in years,” recanting his acceptance of the science and embracing denialism. He was not alone—in fact, outright denialism is now more or less the official Republican line. In 2011, and again in January of this year, Republicans on the House Energy and Commerce Committee voted to block the EPA from regulating carbon emissions and against amendments that would acknowledge that climate change is, in fact, happening.

And it’s not just denialism: extracting and burning carbon is now roundly celebrated by conservative politicians, as if plunging holes into the earth to pull out fossilized peat is a sign of the nation’s potency. In 2012, Mitt Romney said he would build the controversial Keystone XL pipeline himself. Texas Representative Steve Stockman tweeted in March 2013 that “the best thing about the Earth is if you poke holes in it oil and gas come out.”

Remember, all of this is happening at the same time that (a) fossil fuel companies are pulling more carbon out of the ground than ever before, and (b) it’s becoming increasingly clear that those companies will have to leave 80 percent of their reserves in the ground if we are to avert a global cataclysm. In the same way that the abolition movement cast a shadow over the cotton boom, so does the movement to put a price on carbon spook the fossil fuel companies, which even at their moment of peak triumph wonder if a radical change is looming around the corner.

Let me pause here once again to be clear about what the point of this extended historical comparison is and is not. Comparisons to slavery are generally considered rhetorically out of bounds, and for good reason. We are walking on treacherous terrain. The point here is not to associate modern fossil fuel companies with the moral bankruptcy of the slaveholders of yore, or the politicians who defended slavery with those who defend fossil fuels today.

In fact, the parallel I want to highlight is between the opponents of slavery and the opponents of fossil fuels. Because the abolitionists were ultimately successful, it’s all too easy to lose sight of just how radical their demand was at the time: that some of the wealthiest people in the country would have to give up their wealth. That liquidation of private wealth is the only precedent for what today’s climate justice movement is rightly demanding: that trillions of dollars of fossil fuel stay in the ground. It is an audacious demand, and those making it should be clear-eyed about just what they’re asking. They should also recognize that, like the abolitionists of yore, their task may be as much instigation and disruption as it is persuasion. There is no way around conflict with this much money on the line, no available solution that makes everyone happy. No use trying to persuade people otherwise.

* * *

If I’ve done my job so far, you should, right about now, be feeling despair. If, indeed, what we need to save the earth is to forcibly pry trillions of dollars of wealth out of the hands of its owners, and if the only precedent for that is the liberation of the slaves—well, then you wouldn’t be crazy if you concluded that we’re doomed, since that result was achieved only through the most brutal extended war in our nation’s history.

So here is why we’re not doomed. Among many obvious differences between the slave power and the fossil fuel cabal is this definitive one. Slaves were incredibly valuable in large part because they produced huge amounts of value with relatively little capital required. Slave owners merely had to provide food, water and shelter (often wretchedly insufficient) and maintain a system of repression and surveillance to guard against the ever-present threat of rebellion or escape. Compared with many other kinds of investments, unlocking the value of slaves required very little of the plantation owners.

Such is not the case with fossil fuels. Fossil fuel extraction is one of the most capital-intensive industries in the world. While it is immensely, unfathomably profitable, it requires ungodly amounts of money to dig and drill the earth, money to pump and refine and transport the fuel so that it can go from the fossilized plant matter thousands of feet beneath the earth’s surface into your Honda. And that constant need for billions of new dollars in investment capital is the industry’s Achilles’ heel.

A variety of forces are now attacking precisely this vulnerability. The movement to stop the Keystone XL pipeline is probably the largest social movement in American history directed at stopping a piece of capital investment, which is what the pipeline is. Because without that pipeline, a lot of the dirty fuel trapped in the Alberta tar sands is too costly to be worth pulling out.

The divestment movement is pushing colleges, universities, municipalities, pension funds and others to remove their investment from fossil fuel companies. So far, eighteen foundations, twenty-seven religious institutions, twenty-two cities, and eleven colleges and universities have committed themselves to divestment. Together, they have pledged to divest hundreds of millions of dollars from the fossil fuel companies so far.

Of course, that’s a drop in the global pool of capital. But some of the largest funds in the world are sovereign wealth funds, which are subject to political pressure. The largest such fund belongs to Norway, which is seriously considering divesting from fossil fuels.

Investors, even those unmotivated by stewardship of the planet, have reason to be suspicious of the fossil fuel companies. Right now, they are seeing their investment dollars diverted from paying dividends to doing something downright insane: searching for new reserves. Globally, the industry spends $1.8 billion a day on exploration. As one longtime energy industry insider pointed out to me, fossil fuel companies are spending much more on exploring for new reserves than they are posting in profits.

Think about that for a second: to stay below a 2 degree Celsius rise, we can burn only one-fifth of the total fossil fuel that companies have in their reserves right now. And yet, fossil fuel companies are spending hundreds of billions of dollars looking for new reserves—reserves that would be sold and emitted only in some distant postapocalyptic future in which we’ve already burned enough fossil fuel to warm the planet past even the most horrific projections.

This means that fossil fuel companies are taking their investors’ money and spending it on this extremely expensive suicide mission. Every single day. If investors say, “Stop it—we want that money back as dividends rather than being spent on exploration,” then, according to this industry insider, “what that means is, literally, the oil and gas companies don’t have a viable business model. If all your investors say that, and all the analysts start saying that, they can no longer grow as businesses.”

Please support The Nation. Donate now!

In fact, in certain climate and investment circles, people have begun to talk about “stranded assets”—that is, the risk that either national or global carbon-pricing regimes will make the extraction of some of the current reserves uneconomical. Recently, shareholders pushed ExxonMobil to start reporting on its exposure to the risk of stranded assets, which was a crucial first step, though the report itself was best summarized by McKibben as saying, basically, “We plan on overheating the planet, we don’t think any government will stop us, we dare you to try.”

That is the current stance of the fossil fuel companies: “It’s our property, and we’re gonna extract, sell and burn all of it. What are you gonna do about it?”

""Power concedes nothing without a demand. It never did and it never will.”

Those people you see getting arrested outside the White House protesting Keystone XL, showing up at shareholder meetings and sitting in on campuses to get their schools to divest are doing something about it. They are attacking the one weak link in the chain of doom that is our fossil fuel economy.

As the great abolitionist Frederick Douglass said, “Power concedes nothing without a demand. It never did and it never will.” What the climate justice movement is demanding is the ultimate abolition of fossil fuels. And our fates all depend on whether they succeed."
18," John Rock's Error

What the co-inventor of the Pill didn't know about
menstruation can endanger women's health.

 	
John Rock was christened in 1890 at the Church of the Immaculate Conception in Marlborough, Massachusetts, and married by Cardinal William O'Connell, of Boston. He had five children and nineteen grandchildren. A crucifix hung above his desk, and nearly every day of his adult life he attended the 7 a.m. Mass at St. Mary's in Brookline. Rock, his friends would say, was in love with his church. He was also one of the inventors of the birth-control pill, and it was his conviction that his faith and his vocation were perfectly compatible. To anyone who disagreed he would simply repeat the words spoken to him as a child by his home-town priest: ""John, always stick to your conscience. Never let anyone else keep it for you. And I mean anyone else."" Even when Monsignor Francis W. Carney, of Cleveland, called him a ""moral rapist,"" and when Frederick Good, the longtime head of obstetrics at Boston City Hospital, went to Boston's Cardinal Richard Cushing to have Rock excommunicated, Rock was unmoved. ""You should be afraid to meet your Maker,"" one angry woman wrote to him, soon after the Pill was approved. ""My dear madam,"" Rock wrote back, ""in my faith, we are taught that the Lord is with us always. When my time comes, there will be no need for introductions.""

In the years immediately after the Pill was approved by the F.D.A., in 1960, Rock was everywhere. He appeared in interviews and documentaries on CBS and NBC, in Time, Newsweek, Life, The Saturday Evening Post. He toured the country tirelessly. He wrote a widely discussed book, ""The Time Has Come: A Catholic Doctor's Proposals to End the Battle Over Birth Control,"" which was translated into French, German, and Dutch. Rock was six feet three and rail-thin, with impeccable manners; he held doors open for his patients and addressed them as ""Mrs."" or ""Miss."" His mere association with the Pill helped make it seem respectable. ""He was a man of great dignity,"" Dr. Sheldon&nbsp;J. Segal, of the Population Council, recalls. ""Even if the occasion called for an open collar, you'd never find him without an ascot. He had the shock of white hair to go along with that. And posture, straight as an arrow, even to his last year."" At Harvard Medical School, he was a giant, teaching obstetrics for more than three decades. He was a pioneer in in-vitro fertilization and the freezing of sperm cells, and was the first to extract an intact fertilized egg. The Pill was his crowning achievement. His two collaborators, Gregory Pincus and Min- Cheuh Chang, worked out the mechanism. He shepherded the drug through its clinical trials. ""It was his name and his reputation that gave ultimate validity to the claims that the pill would protect women against unwanted pregnancy,"" Loretta McLaughlin writes in her marvellous 1982 biography of Rock. Not long before the Pill's approval, Rock travelled to Washington to testify before the F.D.A. about the drug's safety. The agency examiner, Pasquale DeFelice, was a Catholic obstetrician from Georgetown University, and at one point, the story goes, DeFelice suggested the unthinkable--that the Catholic Church would never approve of the birth-control pill. ""I can still see Rock standing there, his face composed, his eyes riveted on DeFelice,"" a colleague recalled years later, ""and then, in a voice that would congeal your soul, he said, 'Young man, don't you sell my church short.' ""

In the end, of course, John Rock's church disappointed him. In 1968, in the encyclical ""Humanae Vitae,"" Pope Paul VI outlawed oral contraceptives and all other ""artificial"" methods of birth control. The passion and urgency that animated the birth-control debates of the sixties are now a memory. John Rock still matters, though, for the simple reason that in the course of reconciling his church and his work he made an error. It was not a deliberate error. It became manifest only after his death, and through scientific advances he could not have anticipated. But because that mistake shaped the way he thought about the Pill--about what it was, and how it worked, and most of all what it meant--and because John Rock was one of those responsible for the way the Pill came into the world, his error has colored the way people have thought about contraception ever since.

John Rock believed that the Pill was a ""natural"" method of birth control. By that he didn't mean that it felt natural, because it obviously didn't for many women, particularly not in its earliest days, when the doses of hormone were many times as high as they are today. He meant that it worked by natural means. Women can get pregnant only during a certain interval each month, because after ovulation their bodies produce a surge of the hormone progesterone. Progesterone--one of a class of hormones known as progestin--prepares the uterus for implantation and stops the ovaries from releasing new eggs; it favors gestation. ""It is progesterone, in the healthy woman, that prevents ovulation and establishes the pre- and post-menstrual 'safe' period,"" Rock wrote. When a woman is pregnant, her body produces a stream of progestin in part for the same reason, so that another egg can't be released and threaten the pregnancy already under way. Progestin, in other words, is nature's contraceptive. And what was the Pill? Progestin in tablet form. When a woman was on the Pill, of course, these hormones weren't coming in a sudden surge after ovulation and weren't limited to certain times in her cycle. They were being given in a steady dose, so that ovulation was permanently shut down. They were also being given with an additional dose of estrogen, which holds the endometrium together and--as we've come to learn--helps maintain other tissues as well. But to Rock, the timing and combination of hormones wasn't the issue. The key fact was that the Pill's ingredients duplicated what could be found in the body naturally. And in that naturalness he saw enormous theological significance.

In 1951, for example, Pope Pius XII had sanctioned the rhythm method for Catholics because he deemed it a ""natural"" method of regulating procreation: it didn't kill the sperm, like a spermicide, or frustrate the normal process of procreation, like a diaphragm, or mutilate the organs, like sterilization. Rock knew all about the rhythm method. In the nineteen-thirties, at the Free Hospital for Women, in Brookline, he had started the country's first rhythm clinic for educating Catholic couples in natural contraception. But how did the rhythm method work? It worked by limiting sex to the safe period that progestin created. And how did the Pill work? It worked by using progestin to extend the safe period to the entire month. It didn't mutilate the reproductive organs, or damage any natural process. ""Indeed,"" Rock wrote, oral contraceptives ""may be characterized as a 'pill-established safe period,' and would seem to carry the same moral implications"" as the rhythm method. The Pill was, to Rock, no more than ""an adjunct to nature.""

In 1958, Pope Pius XII approved the Pill for Catholics, so long as its contraceptive effects were ""indirect""--that is, so long as it was intended only as a remedy for conditions like painful menses or ""a disease of the uterus."" That ruling emboldened Rock still further. Short-term use of the Pill, he knew, could regulate the cycle of women whose periods had previously been unpredictable. Since a regular menstrual cycle was necessary for the successful use of the rhythm method--and since the rhythm method was sanctioned by the Church--shouldn't it be permissible for women with an irregular menstrual cycle to use the Pill in order to facilitate the use of rhythm? And if that was true why not take the logic one step further? As the federal judge John T. Noonan writes in ""Contraception,"" his history of the Catholic position on birth control:

If it was lawful to suppress ovulation to achieve a regularity necessary for successfully sterile intercourse, why was it not lawful to suppress ovulation without appeal to rhythm? If pregnancy could be prevented by pill plus rhythm, why not by pill alone? In each case suppression of ovulation was used as a means. How was a moral difference made by the addition of rhythm?

These arguments, as arcane as they may seem, were central to the development of oral contraception. It was John Rock and Gregory Pincus who decided that the Pill ought to be taken over a four-week cycle--a woman would spend three weeks on the Pill and the fourth week off the drug (or on a placebo), to allow for menstruation. There was and is no medical reason for this. A typical woman of childbearing age has a menstrual cycle of around twenty- eight days, determined by the cascades of hormones released by her ovaries. As first estrogen and then a combination of estrogen and progestin flood the uterus, its lining becomes thick and swollen, preparing for the implantation of a fertilized egg. If the egg is not fertilized, hormone levels plunge and cause the lining--the endometrium--to be sloughed off in a menstrual bleed. When a woman is on the Pill, however, no egg is released, because the Pill suppresses ovulation. The fluxes of estrogen and progestin that cause the lining of the uterus to grow are dramatically reduced, because the Pill slows down the ovaries. Pincus and Rock knew that the effect of the Pill's hormones on the endometrium was so modest that women could conceivably go for months without having to menstruate. ""In view of the ability of this compound to prevent menstrual bleeding as long as it is taken,"" Pincus acknowledged in 1958, ""a cycle of any desired length could presumably be produced."" But he and Rock decided to cut the hormones off after three weeks and trigger a menstrual period because they believed that women would find the continuation of their monthly bleeding reassuring. More to the point, if Rock wanted to demonstrate that the Pill was no more than a natural variant of the rhythm method, he couldn't very well do away with the monthly menses. Rhythm required ""regularity,"" and so the Pill had to produce regularity as well.

It has often been said of the Pill that no other drug has ever been so instantly recognizable by its packaging: that small, round plastic dial pack. But what was the dial pack if not the physical embodiment of the twenty-eight-day cycle? It was, in the words of its inventor, meant to fit into a case ""indistinguishable"" from a woman's cosmetics compact, so that it might be carried ""without giving a visual clue as to matters which are of no concern to others."" Today, the Pill is still often sold in dial packs and taken in twenty-eight-day cycles. It remains, in other words, a drug shaped by the dictates of the Catholic Church--by John Rock's desire to make this new method of birth control seem as natural as possible. This was John Rock's error. He was consumed by the idea of the natural. But what he thought was natural wasn't so natural after all, and the Pill he ushered into the world turned out to be something other than what he thought it was. In John Rock's mind the dictates of religion and the principles of science got mixed up, and only now are we beginning to untangle them.

In 1986, a young scientist named Beverly Strassmann travelled to Africa to live with the Dogon tribe of Mali. Her research site was the village of Sangui in the Sahel, about a hundred and twenty miles south of Timbuktu. The Sahel is thorn savannah, green in the rainy season and semi-arid the rest of the year. The Dogon grow millet, sorghum, and onions, raise livestock, and live in adobe houses on the Bandiagara escarpment. They use no contraception. Many of them have held on to their ancestral customs and religious beliefs. Dogon farmers, in many respects, live much as people of that region have lived since antiquity. Strassmann wanted to construct a precise reproductive profile of the women in the tribe, in order to understand what female biology might have been like in the millennia that preceded the modern age. In a way, Strassmann was trying to answer the same question about female biology that John Rock and the Catholic Church had struggled with in the early sixties: what is natural? Only, her sense of ""natural"" was not theological but evolutionary. In the era during which natural selection established the basic patterns of human biology--the natural history of our species--how often did women have children? How often did they menstruate? When did they reach puberty and menopause? What impact did breast-feeding have on ovulation? These questions had been studied before, but never so thoroughly that anthropologists felt they knew the answers with any certainty.

Strassmann, who teaches at the University of Michigan at Ann Arbor, is a slender, soft-spoken woman with red hair, and she recalls her time in Mali with a certain wry humor. The house she stayed in while in Sangui had been used as a shelter for sheep before she came and was turned into a pigsty after she left. A small brown snake lived in her latrine, and would curl up in a camouflaged coil on the seat she sat on while bathing. The villagers, she says, were of two minds: was it a deadly snake--Kere me jongolo, literally, ""My bite cannot be healed""--or a harmless mouse snake? (It turned out to be the latter.) Once, one of her neighbors and best friends in the tribe roasted her a rat as a special treat. ""I told him that white people aren't allowed to eat rat because rat is our totem,"" Strassmann says. ""I can still see it. Bloated and charred. Stretched by its paws. Whiskers singed. To say nothing of the tail."" Strassmann meant to live in Sangui for eighteen months, but her experiences there were so profound and exhilarating that she stayed for two and a half years. ""I felt incredibly privileged,"" she says. ""I just couldn't tear myself away.""

Part of Strassmann's work focussed on the Dogon's practice of segregating menstruating women in special huts on the fringes of the village. In Sangui, there were two menstrual huts--dark, cramped, one-room adobe structures, with boards for beds. Each accommodated three women, and when the rooms were full, latecomers were forced to stay outside on the rocks. ""It's not a place where people kick back and enjoy themselves,"" Strassmann says. ""It's simply a nighttime hangout. They get there at dusk, and get up early in the morning and draw their water."" Strassmann took urine samples from the women using the hut, to confirm that they were menstruating. Then she made a list of all the women in the village, and for her entire time in Mali--seven hundred and thirty- six consecutive nights--she kept track of everyone who visited the hut. Among the Dogon, she found, a woman, on average, has her first period at the age of sixteen and gives birth eight or nine times. From menarche, the onset of menstruation, to the age of twenty, she averages seven periods a year. Over the next decade and a half, from the age of twenty to the age of thirty-four, she spends so much time either pregnant or breast-feeding (which, among the Dogon, suppresses ovulation for an average of twenty months) that she averages only slightly more than one period per year. Then, from the age of thirty-five until menopause, at around fifty, as her fertility rapidly declines, she averages four menses a year. All told, Dogon women menstruate about a hundred times in their lives. (Those who survive early childhood typically live into their seventh or eighth decade.) By contrast, the average for contemporary Western women is somewhere between three hundred and fifty and four hundred times.

Strassmann's office is in the basement of a converted stable next to the Natural History Museum on the University of Michigan campus. Behind her desk is a row of battered filing cabinets, and as she was talking she turned and pulled out a series of yellowed charts. Each page listed, on the left, the first names and identification numbers of the Sangui women. Across the top was a time line, broken into thirty-day blocks. Every menses of every woman was marked with an X. In the village, Strassmann explained, there were two women who were sterile, and, because they couldn't get pregnant, they were regulars at the menstrual hut. She flipped through the pages until she found them. ""Look, she had twenty-nine menses over two years, and the other had twenty- three."" Next to each of their names was a solid line of x's. ""Here's a woman approaching menopause,"" Strassmann went on, running her finger down the page. ""She's cycling but is a little bit erratic. Here's another woman of prime childbearing age. Two periods. Then pregnant. I never saw her again at the menstrual hut. This woman here didn't go to the menstrual hut for twenty months after giving birth, because she was breast-feeding. Two periods. Got pregnant. Then she miscarried, had a few periods, then got pregnant again. This woman had three menses in the study period."" There weren't a lot of x's on Strassmann's sheets. Most of the boxes were blank. She flipped back through her sheets to the two anomalous women who were menstruating every month. ""If this were a menstrual chart of undergraduates here at the University of Michigan, all the rows would be like this.""

Strassmann does not claim that her statistics apply to every preindustrial society. But she believes--and other anthropological work backs her up--that the number of lifetime menses isn't greatly affected by differences in diet or climate or method of subsistence (foraging versus agriculture, say). The more significant factors, Strassmann says, are things like the prevalence of wet-nursing or sterility. But over all she believes that the basic pattern of late menarche, many pregnancies, and long menstrual-free stretches caused by intensive breast-feeding was virtually universal up until the ""demographic transition"" of a hundred years ago from high to low fertility. In other words, what we think of as normal--frequent menses--is in evolutionary terms abnormal. ""It's a pity that gynecologists think that women have to menstruate every month,""Strassmann went on. ""They just don't understand the real biology of menstruation.""

To Strassmann and others in the field of evolutionary medicine, this shift from a hundred to four hundred lifetime menses is enormously significant. It means that women's bodies are being subjected to changes and stresses that they were not necessarily designed by evolution to handle. In a brilliant and provocative book, ""Is Menstruation Obsolete?,"" Drs. Elsimar Coutinho and Sheldon S. Segal, two of the world's most prominent contraceptive researchers, argue that this recent move to what they call ""incessant ovulation"" has become a serious problem for women's health. It doesn't mean that women are always better off the less they menstruate. There are times--particularly in the context of certain medical conditions--when women ought to be concerned if they aren't menstruating: In obese women, a failure to menstruate can signal an increased risk of uterine cancer. In female athletes, a failure to menstruate can signal an increased risk of osteoporosis. But for most women, Coutinho and Segal say, incessant ovulation serves no purpose except to increase the occurence of abdominal pain, mood shifts, migraines, endometriosis, fibroids, and anemia--the last of which, they point out, is ""one of the most serious health problems in the world.""

Most serious of all is the greatly increased risk of some cancers. Cancer, after all, occurs because as cells divide and reproduce they sometimes make mistakes that cripple the cells' defenses against runaway growth. That's one of the reasons that our risk of cancer generally increases as we age: our cells have more time to make mistakes. But this also means that any change promoting cell division has the potential to increase cancer risk, and ovulation appears to be one of those changes. Whenever a woman ovulates, an egg literally bursts through the walls of her ovaries. To heal that puncture, the cells of the ovary wall have to divide and reproduce. Every time a woman gets pregnant and bears a child, her lifetime risk of ovarian cancer drops ten per cent. Why? Possibly because, between nine months of pregnancy and the suppression of ovulation associated with breast-feeding, she stops ovulating for twelve months--and saves her ovarian walls from twelve bouts of cell division. The argument is similar for endometrial cancer. When a woman is menstruating, the estrogen that flows through her uterus stimulates the growth of the uterine lining, causing a flurry of potentially dangerous cell division. Women who do not menstruate frequently spare the endometrium that risk. Ovarian and endometrial cancer are characteristically modern diseases, consequences, in part, of a century in which women have come to menstruate four hundred times in a lifetime.

In this sense, the Pill really does have a ""natural""effect. By blocking the release of new eggs, the progestin in oral contraceptives reduces the rounds of ovarian cell division. Progestin also counters the surges of estrogen in the endometrium, restraining cell division there. A woman who takes the Pill for ten years cuts her ovarian-cancer risk by around seventy per cent and her endometrial-cancer risk by around sixty per cent. But here ""natural"" means something different from what Rock meant. He assumed that the Pill was natural because it was an unobtrusive variant of the body's own processes. In fact, as more recent research suggests, the Pill is really only natural in so far as it's radical--rescuing the ovaries and endometrium from modernity. That Rock insisted on a twenty-eight-day cycle for his pill is evidence of just how deep his misunderstanding was: the real promise of the Pill was not that it could preserve the menstrual rhythms of the twentieth century but that it could disrupt them.

Today, a growing movement of reproductive specialists has begun to campaign loudly against the standard twenty-eight-day pill regimen. The drug company Organon has come out with a new oral contraceptive, called Mircette, that cuts the seven-day placebo interval to two days. Patricia Sulak, a medical researcher at Texas A.& M. University, has shown that most women can probably stay on the Pill, straight through, for six to twelve weeks before they experience breakthrough bleeding or spotting. More recently, Sulak has documented precisely what the cost of the Pill's monthly ""off"" week is. In a paper in the February issue of the journal Obstetrics and Gynecology, she and her colleagues documented something that will come as no surprise to most women on the Pill: during the placebo week, the number of users experiencing pelvic pain, bloating, and swelling more than triples, breast tenderness more than doubles, and headaches increase by almost fifty per cent. In other words, some women on the Pill continue to experience the kinds of side effects associated with normal menstruation. Sulak's paper is a short, dry, academic work, of the sort intended for a narrow professional audience. But it is impossible to read it without being struck by the consequences of John Rock's desire to please his church. In the past forty years, millions of women around the world have been given the Pill in such a way as to maximize their pain and suffering. And to what end? To pretend that the Pill was no more than a pharmaceutical version of the rhythm method?

In 1980 and 1981, Malcolm Pike, a medical statistician at the University of Southern California, travelled to Japan for six months to study at the Atomic Bomb Casualties Commission. Pike wasn't interested in the effects of the bomb. He wanted to examine the medical records that the commission had been painstakingly assembling on the survivors of Hiroshima and Nagasaki. He was investigating a question that would ultimately do as much to complicate our understanding of the Pill as Strassmann's research would a decade later: why did Japanese women have breast-cancer rates six times lower than American women?

In the late forties, the World Health Organization began to collect and publish comparative health statistics from around the world, and the breast-cancer disparity between Japan and America had come to obsess cancer specialists. The obvious answer--that Japanese women were somehow genetically protected against breast cancer--didn't make sense, because once Japanese women moved to the United States they began to get breast cancer almost as often as American women did. As a result, many experts at the time assumed that the culprit had to be some unknown toxic chemical or virus unique to the West. Brian Henderson, a colleague of Pike's at U.S.C. and his regular collaborator, says that when he entered the field, in 1970, ""the whole viral- and chemical- carcinogenesis idea was huge--it dominated the literature."" As he recalls, ""Breast cancer fell into this large, unknown box that said it was something to do with the environment--and that word 'environment' meant a lot of different things to a lot of different people. They might be talking about diet or smoking or pesticides.""

Henderson and Pike, however, became fascinated by a number of statistical pecularities. For one thing, the rate of increase in breast-cancer risk rises sharply throughout women's thirties and forties and then, at menopause, it starts to slow down. If a cancer is caused by some toxic outside agent, you'd expect that rate to rise steadily with each advancing year, as the number of mutations and genetic mistakes steadily accumulates. Breast cancer, by contrast, looked as if it were being driven by something specific to a woman's reproductive years. What was more, younger women who had had their ovaries removed had a markedly lower risk of breast cancer; when their bodies weren't producing estrogen and progestin every month, they got far fewer tumors. Pike and Henderson became convinced that breast cancer was linked to a process of cell division similar to that of ovarian and endometrial cancer. The female breast, after all, is just as sensitive to the level of hormones in a woman's body as the reproductive system. When the breast is exposed to estrogen, the cells of the terminal-duct lobular unit--where most breast cancer arises--undergo a flurry of division. And during the mid-to-late stage of the menstrual cycle, when the ovaries start producing large amounts of progestin, the pace of cell division in that region doubles.

It made intuitive sense, then, that a woman's risk of breast cancer would be linked to the amount of estrogen and progestin her breasts have been exposed to during her lifetime. How old a woman is at menarche should make a big difference, because the beginning of puberty results in a hormonal surge through a woman's body, and the breast cells of an adolescent appear to be highly susceptible to the errors that result in cancer. (For more complicated reasons, bearing children turns out to be protective against breast cancer, perhaps because in the last two trimesters of pregnancy the cells of the breast mature and become much more resistant to mutations.) How old a woman is at menopause should matter, and so should how much estrogen and progestin her ovaries actually produce, and even how much she weighs after menopause, because fat cells turn other hormones into estrogen.

Pike went to Hiroshima to test the cell-division theory. With other researchers at the medical archive, he looked first at the age when Japanese women got their period. A Japanese woman born at the turn of the century had her first period at sixteen and a half. American women born at the same time had their first period at fourteen. That difference alone, by their calculation, was sufficient to explain forty per cent of the gap between American and Japanese breast-cancer rates. ""They had collected amazing records from the women of that area,"" Pike said. ""You could follow precisely the change in age of menarche over the century. You could even see the effects of the Second World War. The age of menarche of Japanese girls went up right at that point because of poor nutrition and other hardships. And then it started to go back down after the war. That's what convinced me that the data were wonderful.""

Pike, Henderson, and their colleagues then folded in the other risk factors. Age at menopause, age at first pregnancy, and number of children weren't sufficiently different between the two countries to matter. But weight was. The average post- menopausal Japanese woman weighed a hundred pounds; the average American woman weighed a hundred and forty-five pounds. That fact explained another twenty-five per cent of the difference. Finally, the researchers analyzed blood samples from women in rural Japan and China, and found that their ovaries-- possibly because of their extremely low-fat diet--were producing about seventy-five per cent the amount of estrogen that American women were producing. Those three factors, added together, seemed to explain the breast-cancer gap. They also appeared to explain why the rates of breast cancer among Asian women began to increase when they came to America: on an American diet, they started to menstruate earlier, gained more weight, and produced more estrogen. The talk of chemicals and toxins and power lines and smog was set aside. ""When people say that what we understand about breast cancer explains only a small amount of the problem, that it is somehow a mystery, it's absolute nonsense,"" Pike says flatly. He is a South African in his sixties, with graying hair and a salt-and-pepper beard. Along with Henderson, he is an eminent figure in cancer research, but no one would ever accuse him of being tentative in his pronouncements. ""We understand breast cancer extraordinarily well. We understand it as well as we understand cigarettes and lung cancer.""

What Pike discovered in Japan led him to think about the Pill, because a tablet that suppressed ovulation--and the monthly tides of estrogen and progestin that come with it--obviously had the potential to be a powerful anti-breast-cancer drug. But the breast was a little different from the reproductive organs. Progestin prevented ovarian cancer because it suppressed ovulation. It was good for preventing endometrial cancer because it countered the stimulating effects of estrogen. But in breast cells, Pike believed, progestin wasn't the solution; it was one of the hormones that caused cell division. This is one explanation for why, after years of studying the Pill, researchers have concluded that it has no effect one way or the other on breast cancer: whatever beneficial effect results from what the Pill does is cancelled out by how it does it. John Rock touted the fact that the Pill used progestin, because progestin was the body's own contraceptive. But Pike saw nothing ""natural""about subjecting the breast to that heavy a dose of proges- tin. In his view, the amount of progestin and estrogen needed to make an effective contraceptive was much greater than the amount needed to keep the reproductive system healthy--and that excess was unnecessarily raising the risk of breast cancer. A truly natural Pill might be one that found a way to suppress ovulation without using progestin. Throughout the nineteen-eighties, Pike recalls, this was his obsession. ""We were all trying to work out how the hell we could fix the Pill. We thought about it day and night.""

Pike's proposed solution is a class of drugs known as GnRHAs, which has been around for many years. GnRHAs disrupt the signals that the pituitary gland sends when it is attempting to order the manufacture of sex hormones. It's a circuit breaker. ""We've got substantial experience with this drug,"" Pike says. Men suffering from prostate cancer are sometimes given a GnRHA to temporarily halt the production of testosterone, which can exacerbate their tumors. Girls suffering from what's called precocious puberty--puberty at seven or eight, or even younger--are sometimes given the drug to forestall sexual maturity. If you give GnRHA to women of childbearing age, it stops their ovaries from producing estrogen and progestin. If the conventional Pill works by convincing the body that it is, well, a little bit pregnant, Pike's pill would work by convincing the body that it was menopausal.

In the form Pike wants to use it, GnRHA will come in a clear glass bottle the size of a saltshaker, with a white plastic mister on top. It will be inhaled nasally. "
19,

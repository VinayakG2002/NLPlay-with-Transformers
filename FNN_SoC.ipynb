{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FNN SoC.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMhfkmxTdy9axbXuFuRFPwF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VinayakG2002/README.md/blob/main/FNN_SoC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NNRm64KH-dDA"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "import random"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2aiahluaBVJg"
      },
      "source": [
        "url='https://raw.githubusercontent.com/VinayakG2002/NLPlay-with-Transformers/main/IMDB%20Dataset.csv?token=ATKA2LIXPHWLGQT64HBC2KLA6XEPW'\n",
        "df = pd.read_csv(url)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 189
        },
        "id": "KFZueZg7De5J",
        "outputId": "45b2f20e-35dd-41b4-eeb5-02645d275ff0"
      },
      "source": [
        "print(df.columns)\n",
        "df.describe()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Index(['review', 'sentiment'], dtype='object')\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>50000</td>\n",
              "      <td>50000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>unique</th>\n",
              "      <td>49582</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>top</th>\n",
              "      <td>Loved today's show!!! It was a variety and not...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>freq</th>\n",
              "      <td>5</td>\n",
              "      <td>25000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   review sentiment\n",
              "count                                               50000     50000\n",
              "unique                                              49582         2\n",
              "top     Loved today's show!!! It was a variety and not...  negative\n",
              "freq                                                    5     25000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qOPdJwmlD0g8",
        "outputId": "e9b0e6a9-5dfb-49fc-f656-7c4b42776123"
      },
      "source": [
        "nltk.download('stopwords')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "id": "EMQGLHvt6WmP",
        "outputId": "78990d61-2f87-4770-dd50-8e162d7e09c3"
      },
      "source": [
        "df['review'][0]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. They are right, as this is exactly what happened with me.<br /><br />The first thing that struck me about Oz was its brutality and unflinching scenes of violence, which set in right from the word GO. Trust me, this is not a show for the faint hearted or timid. This show pulls no punches with regards to drugs, sex or violence. Its is hardcore, in the classic use of the word.<br /><br />It is called OZ as that is the nickname given to the Oswald Maximum Security State Penitentary. It focuses mainly on Emerald City, an experimental section of the prison where all the cells have glass fronts and face inwards, so privacy is not high on the agenda. Em City is home to many..Aryans, Muslims, gangstas, Latinos, Christians, Italians, Irish and more....so scuffles, death stares, dodgy dealings and shady agreements are never far away.<br /><br />I would say the main appeal of the show is due to the fact that it goes where other shows wouldn't dare. Forget pretty pictures painted for mainstream audiences, forget charm, forget romance...OZ doesn't mess around. The first episode I ever saw struck me as so nasty it was surreal, I couldn't say I was ready for it, but as I watched more, I developed a taste for Oz, and got accustomed to the high levels of graphic violence. Not just violence, but injustice (crooked guards who'll be sold out for a nickel, inmates who'll kill on order and get away with it, well mannered, middle class inmates being turned into prison bitches due to their lack of street skills or prison experience) Watching Oz, you may become comfortable with what is uncomfortable viewing....thats if you can get in touch with your darker side.\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2vaU3ClAE8JQ"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "stop_words=stopwords.words('english')\n",
        "df['without stopwords']=df['review'].apply(lambda x:' '.join([word for word in x.split() if word not in (stop_words)]))\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "id": "Vnnw2f-sKCHh",
        "outputId": "d4c3fdd1-b812-454e-f9aa-fd1c56b3cdab"
      },
      "source": [
        "df['without stopwords'][0]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"One reviewers mentioned watching 1 Oz episode hooked. They right, exactly happened me.<br /><br />The first thing struck Oz brutality unflinching scenes violence, set right word GO. Trust me, show faint hearted timid. This show pulls punches regards drugs, sex violence. Its hardcore, classic use word.<br /><br />It called OZ nickname given Oswald Maximum Security State Penitentary. It focuses mainly Emerald City, experimental section prison cells glass fronts face inwards, privacy high agenda. Em City home many..Aryans, Muslims, gangstas, Latinos, Christians, Italians, Irish more....so scuffles, death stares, dodgy dealings shady agreements never far away.<br /><br />I would say main appeal show due fact goes shows dare. Forget pretty pictures painted mainstream audiences, forget charm, forget romance...OZ mess around. The first episode I ever saw struck nasty surreal, I say I ready it, I watched more, I developed taste Oz, got accustomed high levels graphic violence. Not violence, injustice (crooked guards who'll sold nickel, inmates who'll kill order get away it, well mannered, middle class inmates turned prison bitches due lack street skills prison experience) Watching Oz, may become comfortable uncomfortable viewing....thats get touch darker side.\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BCn3MpffKSNI"
      },
      "source": [
        "def remove_punctuation(text):\n",
        "    final = \"\".join(u for u in text if u not in (\"?\", \".\", \";\", \":\", \"!\", '\"', ','))\n",
        "    return final\n",
        "\n",
        "df['without stopwords'] = df['without stopwords'].apply(remove_punctuation)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lPOOcjhHKjgT"
      },
      "source": [
        "def remove_tag(text):\n",
        "  final=\"\"\n",
        "  x=True\n",
        "  for letter in text:\n",
        "    if letter=='<':\n",
        "      x=False\n",
        "    if x==True:\n",
        "      final=final + letter\n",
        "    if letter=='>':\n",
        "      x=True\n",
        "      final= final+ \" \"\n",
        "  return final\n",
        "df['without stopwords'] = df['without stopwords'].apply(remove_tag)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "id": "mjUD_HxHLigc",
        "outputId": "759b9255-e649-4be7-f617-5864a49d3285"
      },
      "source": [
        "df['without stopwords'][0]"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"One reviewers mentioned watching 1 Oz episode hooked They right exactly happened me  The first thing struck Oz brutality unflinching scenes violence set right word GO Trust me show faint hearted timid This show pulls punches regards drugs sex violence Its hardcore classic use word  It called OZ nickname given Oswald Maximum Security State Penitentary It focuses mainly Emerald City experimental section prison cells glass fronts face inwards privacy high agenda Em City home manyAryans Muslims gangstas Latinos Christians Italians Irish moreso scuffles death stares dodgy dealings shady agreements never far away  I would say main appeal show due fact goes shows dare Forget pretty pictures painted mainstream audiences forget charm forget romanceOZ mess around The first episode I ever saw struck nasty surreal I say I ready it I watched more I developed taste Oz got accustomed high levels graphic violence Not violence injustice (crooked guards who'll sold nickel inmates who'll kill order get away it well mannered middle class inmates turned prison bitches due lack street skills prison experience) Watching Oz may become comfortable uncomfortable viewingthats get touch darker side\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LCiUeFqWLoqX",
        "outputId": "83e38ee2-6c47-4dfe-a179-94749ee7b96d"
      },
      "source": [
        "nltk.download('punkt')\n",
        "df['tokenized']=df['without stopwords'].apply(nltk.word_tokenize)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I9K3-9mTMinF"
      },
      "source": [
        "def stemmer(input):\n",
        "  output= [nltk.stem.PorterStemmer().stem(word) for word in input]\n",
        "  return output\n",
        "df['stemmed tokens']=df['tokenized'].apply(stemmer)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmy7WLcgPZRi"
      },
      "source": [
        "df['label']=[2*(sentiment=='positive')-1 for sentiment in df['sentiment']]"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "id": "E2xx5eHPREz_",
        "outputId": "64a4ba0b-9701-4c01-9fdd-75164568c9bd"
      },
      "source": [
        "df.head(10)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>without stopwords</th>\n",
              "      <th>tokenized</th>\n",
              "      <th>stemmed tokens</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>One of the other reviewers has mentioned that ...</td>\n",
              "      <td>positive</td>\n",
              "      <td>One reviewers mentioned watching 1 Oz episode ...</td>\n",
              "      <td>[One, reviewers, mentioned, watching, 1, Oz, e...</td>\n",
              "      <td>[one, review, mention, watch, 1, Oz, episod, h...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
              "      <td>positive</td>\n",
              "      <td>A wonderful little production   The filming te...</td>\n",
              "      <td>[A, wonderful, little, production, The, filmin...</td>\n",
              "      <td>[A, wonder, littl, product, the, film, techniq...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I thought this was a wonderful way to spend ti...</td>\n",
              "      <td>positive</td>\n",
              "      <td>I thought wonderful way spend time hot summer ...</td>\n",
              "      <td>[I, thought, wonderful, way, spend, time, hot,...</td>\n",
              "      <td>[I, thought, wonder, way, spend, time, hot, su...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Basically there's a family where a little boy ...</td>\n",
              "      <td>negative</td>\n",
              "      <td>Basically there's family little boy (Jake) thi...</td>\n",
              "      <td>[Basically, there, 's, family, little, boy, (,...</td>\n",
              "      <td>[basic, there, 's, famili, littl, boy, (, jake...</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
              "      <td>positive</td>\n",
              "      <td>Petter Mattei's Love Time Money visually stunn...</td>\n",
              "      <td>[Petter, Mattei, 's, Love, Time, Money, visual...</td>\n",
              "      <td>[petter, mattei, 's, love, time, money, visual...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Probably my all-time favorite movie, a story o...</td>\n",
              "      <td>positive</td>\n",
              "      <td>Probably all-time favorite movie story selfles...</td>\n",
              "      <td>[Probably, all-time, favorite, movie, story, s...</td>\n",
              "      <td>[probabl, all-tim, favorit, movi, stori, selfl...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>I sure would like to see a resurrection of a u...</td>\n",
              "      <td>positive</td>\n",
              "      <td>I sure would like see resurrection dated Seahu...</td>\n",
              "      <td>[I, sure, would, like, see, resurrection, date...</td>\n",
              "      <td>[I, sure, would, like, see, resurrect, date, s...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>This show was an amazing, fresh &amp; innovative i...</td>\n",
              "      <td>negative</td>\n",
              "      <td>This show amazing fresh &amp; innovative idea 70's...</td>\n",
              "      <td>[This, show, amazing, fresh, &amp;, innovative, id...</td>\n",
              "      <td>[thi, show, amaz, fresh, &amp;, innov, idea, 70, '...</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Encouraged by the positive comments about this...</td>\n",
              "      <td>negative</td>\n",
              "      <td>Encouraged positive comments film I looking fo...</td>\n",
              "      <td>[Encouraged, positive, comments, film, I, look...</td>\n",
              "      <td>[encourag, posit, comment, film, I, look, forw...</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>If you like original gut wrenching laughter yo...</td>\n",
              "      <td>positive</td>\n",
              "      <td>If like original gut wrenching laughter like m...</td>\n",
              "      <td>[If, like, original, gut, wrenching, laughter,...</td>\n",
              "      <td>[If, like, origin, gut, wrench, laughter, like...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              review  ... label\n",
              "0  One of the other reviewers has mentioned that ...  ...     1\n",
              "1  A wonderful little production. <br /><br />The...  ...     1\n",
              "2  I thought this was a wonderful way to spend ti...  ...     1\n",
              "3  Basically there's a family where a little boy ...  ...    -1\n",
              "4  Petter Mattei's \"Love in the Time of Money\" is...  ...     1\n",
              "5  Probably my all-time favorite movie, a story o...  ...     1\n",
              "6  I sure would like to see a resurrection of a u...  ...     1\n",
              "7  This show was an amazing, fresh & innovative i...  ...    -1\n",
              "8  Encouraged by the positive comments about this...  ...    -1\n",
              "9  If you like original gut wrenching laughter yo...  ...     1\n",
              "\n",
              "[10 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6MdUCC02CPK"
      },
      "source": [
        "\n",
        "from gensim import corpora\n",
        "\n",
        "review_dict = corpora.Dictionary(df['stemmed tokens'])"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Se0ePheN2FjP",
        "outputId": "11433984-ee7a-44ba-c471-eb8e8a1829d1"
      },
      "source": [
        "print(review_dict)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dictionary(133824 unique tokens: [\"'ll\", '(', ')', '1', 'Em']...)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MbVkEMT3RTWD",
        "outputId": "bce0109f-7ea5-4bba-c363-363acd8e6117"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "# Train Test Split Function\n",
        "\n",
        "def split_train_test(top_data_df_small, test_size=0.3, shuffle_state=True):\n",
        "    X_train, X_test, Y_train, Y_test = train_test_split(top_data_df_small[['review','sentiment','without stopwords','tokenized','stemmed tokens','label']], \n",
        "                                                        top_data_df_small['label'], \n",
        "                                                        shuffle=shuffle_state,\n",
        "                                                        test_size=test_size, \n",
        "                                                        random_state=15)\n",
        "    print(\"Value counts for Train sentiments\")\n",
        "    print(Y_train.value_counts())\n",
        "    print(\"Value counts for Test sentiments\")\n",
        "    print(Y_test.value_counts())\n",
        "    print(type(X_train))\n",
        "    print(type(Y_train))\n",
        "    X_train = X_train.reset_index()\n",
        "    X_test = X_test.reset_index()\n",
        "    Y_train = Y_train.to_frame()\n",
        "    Y_train = Y_train.reset_index()\n",
        "    Y_test = Y_test.to_frame()\n",
        "    Y_test = Y_test.reset_index()\n",
        "    print(X_train.head())\n",
        "    return X_train, X_test, Y_train, Y_test\n",
        "\n",
        "# Call the train_test_split\n",
        "X_train, X_test, Y_train, Y_test = split_train_test(df)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Value counts for Train sentiments\n",
            "-1    17512\n",
            " 1    17488\n",
            "Name: label, dtype: int64\n",
            "Value counts for Test sentiments\n",
            " 1    7512\n",
            "-1    7488\n",
            "Name: label, dtype: int64\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "<class 'pandas.core.series.Series'>\n",
            "   index  ... label\n",
            "0  36322  ...     1\n",
            "1   4638  ...     1\n",
            "2  46808  ...    -1\n",
            "3  38099  ...    -1\n",
            "4  31461  ...     1\n",
            "\n",
            "[5 rows x 7 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 585
        },
        "id": "OMniYRhRukIU",
        "outputId": "34852339-2db0-4d6f-ef8e-c1892c8661ba"
      },
      "source": [
        "X_train\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>without stopwords</th>\n",
              "      <th>tokenized</th>\n",
              "      <th>stemmed tokens</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>36322</td>\n",
              "      <td>The sequel is exactly what you will expect it ...</td>\n",
              "      <td>positive</td>\n",
              "      <td>The sequel exactly expect be And good enough e...</td>\n",
              "      <td>[The, sequel, exactly, expect, be, And, good, ...</td>\n",
              "      <td>[the, sequel, exactli, expect, be, and, good, ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4638</td>\n",
              "      <td>This is a pretty well known one so i won't get...</td>\n",
              "      <td>positive</td>\n",
              "      <td>This pretty well known one get deep it The bas...</td>\n",
              "      <td>[This, pretty, well, known, one, get, deep, it...</td>\n",
              "      <td>[thi, pretti, well, known, one, get, deep, it,...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>46808</td>\n",
              "      <td>I made the mistake of buying this since I coll...</td>\n",
              "      <td>negative</td>\n",
              "      <td>I made mistake buying since I collect comic bo...</td>\n",
              "      <td>[I, made, mistake, buying, since, I, collect, ...</td>\n",
              "      <td>[I, made, mistak, buy, sinc, I, collect, comic...</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>38099</td>\n",
              "      <td>This movie is the proverbial 80s flick that sh...</td>\n",
              "      <td>negative</td>\n",
              "      <td>This movie proverbial 80s flick shows viewer l...</td>\n",
              "      <td>[This, movie, proverbial, 80s, flick, shows, v...</td>\n",
              "      <td>[thi, movi, proverbi, 80, flick, show, viewer,...</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>31461</td>\n",
              "      <td>I absolutely LOVED this movie as a child. I ca...</td>\n",
              "      <td>positive</td>\n",
              "      <td>I absolutely LOVED movie child I can't seem fi...</td>\n",
              "      <td>[I, absolutely, LOVED, movie, child, I, ca, n'...</td>\n",
              "      <td>[I, absolut, love, movi, child, I, ca, n't, se...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34995</th>\n",
              "      <td>39296</td>\n",
              "      <td>When i started watching \"Surface\"for the first...</td>\n",
              "      <td>positive</td>\n",
              "      <td>When started watching Surfacefor first time ho...</td>\n",
              "      <td>[When, started, watching, Surfacefor, first, t...</td>\n",
              "      <td>[when, start, watch, surfacefor, first, time, ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34996</th>\n",
              "      <td>49015</td>\n",
              "      <td>Are you kidding me? This is quite possibly the...</td>\n",
              "      <td>negative</td>\n",
              "      <td>Are kidding me This quite possibly worst amate...</td>\n",
              "      <td>[Are, kidding, me, This, quite, possibly, wors...</td>\n",
              "      <td>[are, kid, me, thi, quit, possibl, worst, amat...</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34997</th>\n",
              "      <td>2693</td>\n",
              "      <td>One would make you believe that this game is a...</td>\n",
              "      <td>negative</td>\n",
              "      <td>One would make believe game man obsessed numbe...</td>\n",
              "      <td>[One, would, make, believe, game, man, obsesse...</td>\n",
              "      <td>[one, would, make, believ, game, man, obsess, ...</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34998</th>\n",
              "      <td>8076</td>\n",
              "      <td>I'D BUY THAT FOR A DOLLAR!!!&lt;br /&gt;&lt;br /&gt;I did ...</td>\n",
              "      <td>positive</td>\n",
              "      <td>I'D BUY THAT FOR A DOLLAR  I buy film dollar I...</td>\n",
              "      <td>[I, 'D, BUY, THAT, FOR, A, DOLLAR, I, buy, fil...</td>\n",
              "      <td>[I, 'D, buy, that, for, A, dollar, I, buy, fil...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34999</th>\n",
              "      <td>7624</td>\n",
              "      <td>Seems to have been made as a vehicle for W.C. ...</td>\n",
              "      <td>positive</td>\n",
              "      <td>Seems made vehicle WC Fields Carol Dempster do...</td>\n",
              "      <td>[Seems, made, vehicle, WC, Fields, Carol, Demp...</td>\n",
              "      <td>[seem, made, vehicl, WC, field, carol, dempste...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>35000 rows Ã— 7 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       index  ... label\n",
              "0      36322  ...     1\n",
              "1       4638  ...     1\n",
              "2      46808  ...    -1\n",
              "3      38099  ...    -1\n",
              "4      31461  ...     1\n",
              "...      ...  ...   ...\n",
              "34995  39296  ...     1\n",
              "34996  49015  ...    -1\n",
              "34997   2693  ...    -1\n",
              "34998   8076  ...     1\n",
              "34999   7624  ...     1\n",
              "\n",
              "[35000 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YPjnwScJT08l",
        "outputId": "51563e88-85d7-43cf-931f-ab76b8be892b"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device available for running: \")\n",
        "print(device)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device available for running: \n",
            "cpu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0khqdiuCeEns"
      },
      "source": [
        "# define FeedforwardNeuralNetwork\n",
        "class FeedForwardNN(nn.Module):\n",
        "    def __init__(self,input_size,hidden_size,output_size):\n",
        "        super(FeedForwardNN,self).__init__()\n",
        "        \n",
        "        # Linear fnction 1\n",
        "        self.LF1 = nn.Linear(input_size,hidden_size)\n",
        "        # Nonlinear fnction 1\n",
        "        self.NLF1 = nn.ReLU()\n",
        "        \n",
        "        # Linear function 2\n",
        "        self.LF2 = nn.Linear(hidden_size,hidden_size)\n",
        "        # Nonlinear fnction 2\n",
        "        self.NLF2 = nn.ReLU()\n",
        "        \n",
        "        #Linear function 2\n",
        "        self.LF3 = nn.Linear(hidden_size,output_size)\n",
        "        \n",
        "    def forward(self,x):\n",
        "        # Layer 1\n",
        "        out = self.LF1(x)\n",
        "        out = self.NLF1(out)\n",
        "        \n",
        "        #Layer 2 \n",
        "        out = self.LF2(out)\n",
        "        out = self.NLF2(out)\n",
        "        \n",
        "        #Layer 3\n",
        "        out = self.LF3(out)\n",
        "        \n",
        "        return F.softmax(out,dim=1)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lEDW_6_8fU1K"
      },
      "source": [
        "\n",
        "VOCAB_SIZE = len(review_dict)\n",
        "NUM_LABELS = 2\n",
        "\n",
        "def make_bow_vector(review_dict, sentence):\n",
        "    vec = torch.zeros(VOCAB_SIZE, dtype=torch.float64, device=device)\n",
        "    for word in sentence:\n",
        "        vec[review_dict.token2id[word]] += 1\n",
        "    return vec.view(1, -1).float()"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xad47tRPuD8c"
      },
      "source": [
        "\n",
        "def make_pred(label):\n",
        "    if label == -1:\n",
        "        return torch.tensor([0], dtype = torch.long, device = device)\n",
        "    elif label == 1:\n",
        "        return torch.tensor([1], dtype = torch.long, device = device)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nC8hiu3jr1Sg"
      },
      "source": [
        "def train(num_epochs,batch_size,optimizer,loss_function = nn.CrossEntropyLoss()):\n",
        "  losses = []\n",
        "  task_len = num_epochs *len(X_train)\n",
        "  ittr = 0\n",
        "  for epoch in range(num_epochs):\n",
        "      train_loss = torch.tensor([0.],device = device)\n",
        "      for index,row in X_train.iterrows():\n",
        "          ittr+=1\n",
        "          if ittr%(task_len//100) == 0:\n",
        "            print(ittr*100//(task_len),'% complemeted')\n",
        "          # make bag of word vector \n",
        "          bow_vec = make_bow_vector(review_dict,row['stemmed tokens'])\n",
        "        \n",
        "          # Forward pass\n",
        "          preds = FFNN(bow_vec)\n",
        "        \n",
        "          # get target label\n",
        "          target = make_pred(Y_train['label'][index])\n",
        "\n",
        "          loss = loss_function(preds, target)\n",
        "  \n",
        "          train_loss +=loss\n",
        "        \n",
        "          if ittr%batch_size ==0:\n",
        "            # clear gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # compute gradientts\n",
        "            train_loss.backward()\n",
        "\n",
        "            # update parameters\n",
        "            optimizer.step()\n",
        "            \n",
        "            losses.append(train_loss.item())\n",
        "            train_loss = 0.\n",
        "  return losses"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_FLiFhFstLRr"
      },
      "source": [
        "def accuracy():\n",
        "  correct_preds = 0\n",
        "  for index,row in X_test.iterrows():\n",
        "    bow_vec = make_bow_vector(review_dict,row['stemmed tokens'])\n",
        "    # Forward pass\n",
        "    preds = list(FFNN(bow_vec)[0])\n",
        "    if preds[0]>preds[1]:\n",
        "      out = -1\n",
        "    else :\n",
        "      out = +1\n",
        "    if out == row['label']:\n",
        "      correct_preds+=1\n",
        "  \n",
        "  return correct_preds*100/len(X_test)\n",
        "\n",
        "def plot_loss(losses):\n",
        "  x = [i for i in range(0,len(losses))]\n",
        "  plt.plot(x,losses)\n",
        "  plt.title('Loss fnction')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.xlabel('batch index')"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWRVt0R9vQFD"
      },
      "source": [
        "VOCAB_SIZE = len(review_dict)\n",
        "\n",
        "input_size = VOCAB_SIZE\n",
        "hidden_dim = 500\n",
        "output_size = 2\n",
        "num_epochs = 2\n",
        "batch_size = 200\n",
        "\n",
        "FFNN = FeedForwardNN(input_size,hidden_dim,output_size)\n",
        "FFNN.to(device)\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(FFNN.parameters(), lr = 0.005)\n"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        },
        "id": "jSOPR2dytQie",
        "outputId": "6b59ba43-440c-40d9-c4cb-dc534121fca9"
      },
      "source": [
        "losses = train(num_epochs,200,optimizer)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-ee7714e598c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-23-bb928000f6ab>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(num_epochs, batch_size, optimizer, loss_function)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0;31m# compute gradientts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0mtrain_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0;31m# update parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    147\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tgK8orVwxadz"
      },
      "source": [
        "accuracy()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LC4I5iPAxcVZ"
      },
      "source": [
        "plot_loss(losses)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1FxkecAxgU7"
      },
      "source": [
        "optimizer = optim.SGD(FFNN.parameters(), lr = 0.001)\n",
        "num_epochs = 2\n",
        "batch_size = 100\n",
        "losses = train(num_epochs,batch_size,optimizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5-RSC6kxjIm"
      },
      "source": [
        "\n",
        "print(accuracy())\n",
        "\n",
        "plot_loss(losses)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GL4alfZHxmHo"
      },
      "source": [
        "\n",
        "optimizer = optim.SGD(FFNN.parameters(), lr = 0.0001)\n",
        "num_epochs = 3\n",
        "batch_size = 1000\n",
        "losses = train(num_epochs,batch_size,optimizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "njWj8eO6xof2"
      },
      "source": [
        "print(accuracy())\n",
        "\n",
        "plot_loss(losses)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFz1AEdqxxBf"
      },
      "source": [
        "optimizer = optim.SGD(FFNN.parameters(), lr = 1e-6)\n",
        "num_epochs = 5\n",
        "batch_size = 5000\n",
        "losses = train(num_epochs,batch_size,optimizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WW4mfGRjxqz7"
      },
      "source": [
        "print(accuracy())\n",
        "\n",
        "plot_loss(losses)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t6Lz5BlHxzZZ"
      },
      "source": [
        "optimizer = optim.SGD(FFNN.parameters(), lr = 1e-4)\n",
        "num_epochs = 3\n",
        "batch_size = 10\n",
        "losses = train(num_epochs,batch_size,optimizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U4RiwIJAxrUP"
      },
      "source": [
        "print(accuracy())\n",
        "\n",
        "plot_loss(losses)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}